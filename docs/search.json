[
  {
    "objectID": "research/volley/volley.html",
    "href": "research/volley/volley.html",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "We extend and adapt the modelling frameworks typically used for the analysis of football data and propose a novel Bayesian hierarchical modelling framework for the analysis and prediction of volleyball results in regular seasons. Three different sub-models or “modules” form our framework: (1) The module of the observed number of points scored by the two opposing teams in a match (\\(y_h\\) and \\(y_a\\)); (2) the module of the binary indicator for the number of sets played (\\(d^s\\)); (3) the module of the binary indicator for the winner of the match (\\(d^m\\)). These three modules are jointly modelled using a flexible Bayesian parametric approach, which allows to fully propagate the uncertainty for each unobserved quantity and to assess the predictive performance of the model in a relatively easy way. In the following, we describe the notation and the model used in each of the three modules.\n\n\nIn the first module of the framework, we model the number of points scored by the home and away team in the \\(i\\)-th match of the season \\(\\boldsymbol y=(y_{hi},y_{ai})\\) using two independent Poisson distributions, as shown in Equation 1 and Equation 2:\n\\[\ny_{hi} \\sim Poisson(\\theta_{hi}),\n\\tag{1}\\]\n\\[\ny_{ai} \\sim Poisson(\\theta_{ai}),\n\\tag{2}\\]\nconditionally on the set of parameters \\(\\boldsymbol \\theta=(\\theta_{hi},\\theta_{ai})\\), representing the scoring intensity in the \\(i\\)-th match for the home and away team, respectively. These parameters are then modelled using the log-linear regressions, as shown in Equation 3 and Equation 4:\n\\[\nlog(\\theta_{hi}) =\\mu + \\lambda + att_{h(i)} + def_{a(i)},\n\\tag{3}\\]\n\\[\nlog(\\theta_{ai}) =\\mu + att_{a(i)} + def_{h(i)},\n\\tag{4}\\]\nwhich corresponds to a Poisson log-linear model. Within these formulae, \\(\\mu\\) is a constant, while \\(\\lambda\\) can be identified as the home effect and represents the advantage for the team hosting the game which is typically assumed to be constant for all the teams and throughout the season. The overall offensive and defensive performances of the \\(k\\)-th team is captured by the parameters \\(att\\) and \\(def\\), whose nested indexes \\(h(i), a(i)=1,\\ldots,K\\) identify the home and away team in the \\(i\\)-th game of the season, where \\(K\\) denotes the total number of the teams.\nWe then expand the modelling framework to incorporate match-specific statistics related to the offensive and defensive performances of the home and away teams. More specifically, Equation 5 and Equation 6 show the effects associated with the attack intensity of the home teams and the defence effect of the away teams:\n\\[\natt_{h(i)} =\\alpha_{0h(i)} + \\alpha_{1h(i)}att^{eff}_{hi} + \\alpha_{2h(i)}ser^{eff}_{hi},\n\\tag{5}\\]\n\\[\ndef_{a(i)} =\\beta_{0a(i)} + \\beta_{1a(i)}def^{eff}_{ai} + \\beta_{2a(i)}blo^{eff}_{ai}.\n\\tag{6}\\]\nWe omit the index \\(i\\) from the terms to the left-hand side of the above formulae to ease notation, i.e. \\(att_{h(i)}=att_{h(i)i}\\) and \\(def_{a(i)}=def_{a(i)i}\\). The overall offensive effect of the home teams is a function of a baseline team specific parameter \\(\\alpha_{0h(i)}\\), and the attack and serve efficiencies of the home team, whose impact is captured by the parameters \\(\\alpha_{1h(i)}\\) and \\(\\alpha_{2h(i)}\\). The overall defensive effect of the away team is a function of a baseline team-specific parameter \\(\\beta_{0a(i)}\\), and the defence and block efficiencies of the away team, whose impact is captured by the parameters \\(\\beta_{1a(i)}\\) and \\(\\beta_{2a(i)}\\), respectively. Similarly, Equation 7 and Equation 8 show the effects associated with the attack intensity of the away teams and the defence effect of the home teams:\n\\[\natt_{a(i)} =\\alpha_{0a(i)} + \\alpha_{1a(i)}att^{eff}_{ai}+ \\alpha_{2a(i)}ser^{eff}_{ai},\n\\tag{7}\\]\n\\[\ndef_{h(i)} =\\beta_{0h(i)} + \\beta_{1h(i)}def^{eff}_{hi}+ \\beta_{2h(i)}blo^{eff}_{hi},\n\\tag{8}\\]\nTo achieve identifiability of the model, a set of parametric constraints needs to be imposed. We impose sum-to-zero constraints on the team-specific parameters, i.e. we set \\(\\sum_{k=1}^{K}\\alpha_{jk}=0\\) and \\(\\sum_{k=1}^{K}\\beta_{jk}=0\\), for \\(k=1,\\ldots,K\\) and \\(j=(0,1,2)\\). Under this set of constraints, the overall offensive and defensive effects of the teams are expressed as departures from a team of average offensive and defensive performance. Within a Bayesian framework, prior distributions need to be specified for all random parameters in the model. Weakly informative Normal distributions centred at \\(0\\) with a relatively large variances are specified for the fixed effect parameters.\n\n\n\nIn the second module, we explicitly model the chance of playing \\(5\\) sets in the \\(i\\)-th match of the season, i.e. the sum of the sets won by the home (\\(s_{hi}\\)) and away (\\(s_{ai}\\)) team is equal to \\(5\\). This is necessary when generating predictions in order to correctly assign the points to the winning/losing teams throughout the season and evaluate the rankings of the teams at the end of the season. We model the indicator variable \\(d^s_{i}\\), taking value \\(1\\) if \\(5\\) sets were played in the \\(i-\\)th match and \\(0\\) otherwise, as shown in Equation 9 and Equation 10, using a Bernoulli distribution\n\\[\nd^s_{i}:=\\mathbb{I}(s_{hi}+s_{ai}=5)\\sim\\mbox{Bernoulli}(\\pi^s_{i}),\n\\tag{9}\\]\nwhere\n\\[\nlogit(\\pi^s_{i})= \\gamma_0 + \\gamma_1y_{hi} + \\gamma_2y_{ai}.  \n\\tag{10}\\]\n\n\n\nThe last module deals with the chance of the home team to win the \\(i\\)-th match, i.e. the total number of sets won by the home team (\\(s_{hi}\\)) is larger than that of the away team (\\(s_{ai}\\)) – we note that we could have also equivalently decided to model the chance of the away team to win the \\(i\\)-th match. This part of the model is again necessary when predicting the results for future matches, since the team associated with the higher number of points scored in the \\(i\\)-th match may not correspond to the winning team. We model the indicator variable \\(d^m_{i}\\), taking value \\(1\\) if the home team won the \\(i-\\)th match and \\(0\\) otherwise, as shown in Equation 11 and Equation 12, using another Bernoulli distribution\n\\[\nd^m_{i}:=\\mathbb{I}(s_{hi}&gt;s_{ai}) \\sim\\mbox{Bernoulli}(\\pi^m_{i}),\n\\tag{11}\\]\nwhere\n\\[\nlogit(\\pi^m_{i})= \\eta_0 + \\eta_1y_{hi} + \\eta_2y_{ai} + \\eta_3 d^s_i.\n\\tag{12}\\]\nFigure 1 shows a graphical representation of the modelling framework proposed.\n\n\n\n\n\n\nFigure 1: Graphical representation of the modelling framework.\n\n\n\nThe framework corresponds to a joint distribution for all the observed quantities which are explicitly modelled. This is factored into the product of the marginal distribution of the total number of points scored by the two teams in each match, Module 1 – \\(p(\\boldsymbol y)\\), the conditional distribution of the probability of playing \\(5\\) sets in a match given \\(\\boldsymbol y\\), Module 2 – \\(p(d^s_i \\mid \\boldsymbol y)\\), and the conditional probability of winning the match given \\(\\boldsymbol y\\) and \\(d^s_i\\), Module 3 – \\(p(d^m_i\\mid \\boldsymbol y, d^s_i)\\). Module 1 also includes the different in-game statistics as covariates in the model. These are related to the either the offensive (serve and attack efficiency) or defensive (defence and block efficiency) effects of the home and away teams in each match of the season, and are respectively denoted in the graph as \\(\\boldsymbol x^{att}_{ti}=(ser^{eff}_{ti}, att^{eff}_{ti})\\) and \\(\\boldsymbol x^{def}_{ti}=(def^{eff}_{ti}, blo^{eff}_{ti})\\) to ease notation, for \\(t=(h,a)\\).\n\n\n\nAlthough the individual-level correlation between the observable variables \\(y_{hi}\\) and \\(y_{ai}\\) is taken into account through the hierarchical structure of the framework, a potential limitation of the model is that it ignores the possible multilevel correlation between the team-specific offensive \\(\\alpha_{jk}\\) and defensive \\(\\beta_{jk}\\) coefficients, for \\(j=(0,1,2)\\) and \\(k=1,\\ldots,K\\). In an alternative analysis, we account for the multilevel correlation using Inverse-Wishart distributions on the covariance matrix of the team specific parameters $ {}$ and $ {}$, which are scaled in order to facilitate the specification of the priors."
  },
  {
    "objectID": "research/volley/volley.html#module-1-modelling-the-scoring-intensity",
    "href": "research/volley/volley.html#module-1-modelling-the-scoring-intensity",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "In the first module of the framework, we model the number of points scored by the home and away team in the \\(i\\)-th match of the season \\(\\boldsymbol y=(y_{hi},y_{ai})\\) using two independent Poisson distributions, as shown in Equation 1 and Equation 2:\n\\[\ny_{hi} \\sim Poisson(\\theta_{hi}),\n\\tag{1}\\]\n\\[\ny_{ai} \\sim Poisson(\\theta_{ai}),\n\\tag{2}\\]\nconditionally on the set of parameters \\(\\boldsymbol \\theta=(\\theta_{hi},\\theta_{ai})\\), representing the scoring intensity in the \\(i\\)-th match for the home and away team, respectively. These parameters are then modelled using the log-linear regressions, as shown in Equation 3 and Equation 4:\n\\[\nlog(\\theta_{hi}) =\\mu + \\lambda + att_{h(i)} + def_{a(i)},\n\\tag{3}\\]\n\\[\nlog(\\theta_{ai}) =\\mu + att_{a(i)} + def_{h(i)},\n\\tag{4}\\]\nwhich corresponds to a Poisson log-linear model. Within these formulae, \\(\\mu\\) is a constant, while \\(\\lambda\\) can be identified as the home effect and represents the advantage for the team hosting the game which is typically assumed to be constant for all the teams and throughout the season. The overall offensive and defensive performances of the \\(k\\)-th team is captured by the parameters \\(att\\) and \\(def\\), whose nested indexes \\(h(i), a(i)=1,\\ldots,K\\) identify the home and away team in the \\(i\\)-th game of the season, where \\(K\\) denotes the total number of the teams.\nWe then expand the modelling framework to incorporate match-specific statistics related to the offensive and defensive performances of the home and away teams. More specifically, Equation 5 and Equation 6 show the effects associated with the attack intensity of the home teams and the defence effect of the away teams:\n\\[\natt_{h(i)} =\\alpha_{0h(i)} + \\alpha_{1h(i)}att^{eff}_{hi} + \\alpha_{2h(i)}ser^{eff}_{hi},\n\\tag{5}\\]\n\\[\ndef_{a(i)} =\\beta_{0a(i)} + \\beta_{1a(i)}def^{eff}_{ai} + \\beta_{2a(i)}blo^{eff}_{ai}.\n\\tag{6}\\]\nWe omit the index \\(i\\) from the terms to the left-hand side of the above formulae to ease notation, i.e. \\(att_{h(i)}=att_{h(i)i}\\) and \\(def_{a(i)}=def_{a(i)i}\\). The overall offensive effect of the home teams is a function of a baseline team specific parameter \\(\\alpha_{0h(i)}\\), and the attack and serve efficiencies of the home team, whose impact is captured by the parameters \\(\\alpha_{1h(i)}\\) and \\(\\alpha_{2h(i)}\\). The overall defensive effect of the away team is a function of a baseline team-specific parameter \\(\\beta_{0a(i)}\\), and the defence and block efficiencies of the away team, whose impact is captured by the parameters \\(\\beta_{1a(i)}\\) and \\(\\beta_{2a(i)}\\), respectively. Similarly, Equation 7 and Equation 8 show the effects associated with the attack intensity of the away teams and the defence effect of the home teams:\n\\[\natt_{a(i)} =\\alpha_{0a(i)} + \\alpha_{1a(i)}att^{eff}_{ai}+ \\alpha_{2a(i)}ser^{eff}_{ai},\n\\tag{7}\\]\n\\[\ndef_{h(i)} =\\beta_{0h(i)} + \\beta_{1h(i)}def^{eff}_{hi}+ \\beta_{2h(i)}blo^{eff}_{hi},\n\\tag{8}\\]\nTo achieve identifiability of the model, a set of parametric constraints needs to be imposed. We impose sum-to-zero constraints on the team-specific parameters, i.e. we set \\(\\sum_{k=1}^{K}\\alpha_{jk}=0\\) and \\(\\sum_{k=1}^{K}\\beta_{jk}=0\\), for \\(k=1,\\ldots,K\\) and \\(j=(0,1,2)\\). Under this set of constraints, the overall offensive and defensive effects of the teams are expressed as departures from a team of average offensive and defensive performance. Within a Bayesian framework, prior distributions need to be specified for all random parameters in the model. Weakly informative Normal distributions centred at \\(0\\) with a relatively large variances are specified for the fixed effect parameters."
  },
  {
    "objectID": "research/volley/volley.html#module-2-modelling-the-probability-of-playing-5-sets",
    "href": "research/volley/volley.html#module-2-modelling-the-probability-of-playing-5-sets",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "In the second module, we explicitly model the chance of playing \\(5\\) sets in the \\(i\\)-th match of the season, i.e. the sum of the sets won by the home (\\(s_{hi}\\)) and away (\\(s_{ai}\\)) team is equal to \\(5\\). This is necessary when generating predictions in order to correctly assign the points to the winning/losing teams throughout the season and evaluate the rankings of the teams at the end of the season. We model the indicator variable \\(d^s_{i}\\), taking value \\(1\\) if \\(5\\) sets were played in the \\(i-\\)th match and \\(0\\) otherwise, as shown in Equation 9 and Equation 10, using a Bernoulli distribution\n\\[\nd^s_{i}:=\\mathbb{I}(s_{hi}+s_{ai}=5)\\sim\\mbox{Bernoulli}(\\pi^s_{i}),\n\\tag{9}\\]\nwhere\n\\[\nlogit(\\pi^s_{i})= \\gamma_0 + \\gamma_1y_{hi} + \\gamma_2y_{ai}.  \n\\tag{10}\\]"
  },
  {
    "objectID": "research/volley/volley.html#module-3-modelling-the-probability-of-winning-the-match",
    "href": "research/volley/volley.html#module-3-modelling-the-probability-of-winning-the-match",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "The last module deals with the chance of the home team to win the \\(i\\)-th match, i.e. the total number of sets won by the home team (\\(s_{hi}\\)) is larger than that of the away team (\\(s_{ai}\\)) – we note that we could have also equivalently decided to model the chance of the away team to win the \\(i\\)-th match. This part of the model is again necessary when predicting the results for future matches, since the team associated with the higher number of points scored in the \\(i\\)-th match may not correspond to the winning team. We model the indicator variable \\(d^m_{i}\\), taking value \\(1\\) if the home team won the \\(i-\\)th match and \\(0\\) otherwise, as shown in Equation 11 and Equation 12, using another Bernoulli distribution\n\\[\nd^m_{i}:=\\mathbb{I}(s_{hi}&gt;s_{ai}) \\sim\\mbox{Bernoulli}(\\pi^m_{i}),\n\\tag{11}\\]\nwhere\n\\[\nlogit(\\pi^m_{i})= \\eta_0 + \\eta_1y_{hi} + \\eta_2y_{ai} + \\eta_3 d^s_i.\n\\tag{12}\\]\nFigure 1 shows a graphical representation of the modelling framework proposed.\n\n\n\n\n\n\nFigure 1: Graphical representation of the modelling framework.\n\n\n\nThe framework corresponds to a joint distribution for all the observed quantities which are explicitly modelled. This is factored into the product of the marginal distribution of the total number of points scored by the two teams in each match, Module 1 – \\(p(\\boldsymbol y)\\), the conditional distribution of the probability of playing \\(5\\) sets in a match given \\(\\boldsymbol y\\), Module 2 – \\(p(d^s_i \\mid \\boldsymbol y)\\), and the conditional probability of winning the match given \\(\\boldsymbol y\\) and \\(d^s_i\\), Module 3 – \\(p(d^m_i\\mid \\boldsymbol y, d^s_i)\\). Module 1 also includes the different in-game statistics as covariates in the model. These are related to the either the offensive (serve and attack efficiency) or defensive (defence and block efficiency) effects of the home and away teams in each match of the season, and are respectively denoted in the graph as \\(\\boldsymbol x^{att}_{ti}=(ser^{eff}_{ti}, att^{eff}_{ti})\\) and \\(\\boldsymbol x^{def}_{ti}=(def^{eff}_{ti}, blo^{eff}_{ti})\\) to ease notation, for \\(t=(h,a)\\)."
  },
  {
    "objectID": "research/volley/volley.html#accounting-for-the-multilevel-correlation",
    "href": "research/volley/volley.html#accounting-for-the-multilevel-correlation",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "Although the individual-level correlation between the observable variables \\(y_{hi}\\) and \\(y_{ai}\\) is taken into account through the hierarchical structure of the framework, a potential limitation of the model is that it ignores the possible multilevel correlation between the team-specific offensive \\(\\alpha_{jk}\\) and defensive \\(\\beta_{jk}\\) coefficients, for \\(j=(0,1,2)\\) and \\(k=1,\\ldots,K\\). In an alternative analysis, we account for the multilevel correlation using Inverse-Wishart distributions on the covariance matrix of the team specific parameters $ {}$ and $ {}$, which are scaled in order to facilitate the specification of the priors."
  },
  {
    "objectID": "research/reviewNL/reviewNL.html",
    "href": "research/reviewNL/reviewNL.html",
    "title": "A review of heath economic evaluation practice in the Netherlands: are we moving forward?",
    "section": "",
    "text": "Introduction\nIn the Netherlands, the Dutch National Health Care Institute (Zorginstituut Nederland or ZIN) is the body in charge of issuing recommendations and guidance on good practice in health economic evaluations, not just for pharmaceutical products, but also in relation to other fields of application such as medical devices, long-term care and forensics. In 2016, ZIN issued an update on the guidance for health economic evaluations, which aggregated into a single document and revised three separately published guidelines for pharmacoeconomics evaluation, outcomes research and costing manual. The novel aspects and future policy direction introduced by these guidelines have already been object of discussion, particularly with respect to the potential impact and concerns associated with their implementation in standard health economics practice in the Netherlands. Given the importance covered by these guidelines, an assessment of their impact on economic evaluation practice is desirable.\nThe objective of this paper was to review the evolution of health economic evaluation practice in the Netherlands before and after the introduction of the ZIN’s 2016 guidelines. Based on some key components within the health economics framework addressed by the new guidelines, we specifically focus on reviewing the statistical methods, missing data methods and software implemented by health economists. Given the intrinsic complexity of analysing health economics data, the choice of the analytical approaches to deal with these problems as well as transparent information on their implementation is crucial in determining the degree of confidence that decision-makers should have towards cost-effectiveness results obtained from these studies\n\n\nThe ZIN 2016 guidelines\nThe main objective of the guidelines is to ensure the comparability and quality of health economic evaluations in the Netherlands, therefore facilitating the task of the decision-maker regarding the reimbursement of new health care interventions. Following the example of guidelines issued by decision-making bodies in other countries, including the National Institute for Health and Care Excellence (NICE) in the UK, the recommended features for economic evaluations are summarised in a typical scenario referred to as ‘reference case’, although deviations from it are allowed when properly justified.\nBased on the structure of the reference case, four essential components of a health economic evaluation are identified: framework, analytic approach, input data and reporting. For the purpose of the review, we only focus on these components in the reference case as the main elements upon which evaluating health economics practice.\n\n\nMethods\nWe performed a bibliographic search in June 2021 using search engines of two online full-text journal repositories: (1) PubMed and (2) Zorginstituut. These sources were chosen to maximise the number of studies that could be accessed given the scoping nature of the review and the lack of a search strategy based on a pre-defined and rigid approach typical of systematic reviews. Articles were considered eligible for the review only if they were cost-effectiveness or cost-utility analyses targeting a Dutch population. To allow the inclusion of a reasonable amount of studies, the key words used in the search strategy were (cost-effectiveness OR cost-utility OR economic evaluation), and we targeted studies published between January 2016 and April 2021.\n\n\nAnalytic approaches\nAlmost all reviewed empirical analyses used bootstrapping (95%), although the number of replications varied largely across the studies, with the most popular choices being 5000 (55%) followed by 2000 (29%). Studies showed even more variability in the choice of the methods used in combination with bootstrapping. Seven general classes of statistical approaches were identified, among which unadjusted methods were the most popular choice across both time periods. A clear change in the type of statistical methods used between the two periods is denoted by a strong decrease (from 64 to 39) in the number of unadjusted analyses in 2016–2020 compared to the earlier period, which is compensated by a rise in the number of adjusted analyses using either SUR or linear mixed effects model (LMM) methods.\nFrom Figure 1 we can look at the different type and combination of software programs used as an indication of the implementation preferences of analysts for health economic evaluations. Although in principle the choice of software should have no impact on the quality of the statistical methods implemented, it has been highlighted how use of simpler software (e.g. spreadsheet calculators such as Excel) may become increasingly cumbersome for matching more realistic and therefore complex modelling requirements.\n\n\n\n\n\n\nFigure 1: Heatmap of the combination of software programs used\n\n\n\nThe most popular software was SPSS, chosen by 87 (52%) of the studies, either in the base-case (33%) or secondary (19%) analyses, often used in combination with Excel or by itself. When either STATA (26%) or R (13%) was used in the base-case analysis, SPSS was still the most popular choice in secondary analyses. Other combinations of software were less frequently chosen, even though 38 (23%) of the studies were unclear about the software implemented.\n\n\nMissing data methods\nAcross both periods limited changes are observed in terms of order of choice for missing data methods, with MI being the most popular base-case analysis, followed by complete case analysis (CCA), as the most popular SA choice. However, two noticeable variations in the frequency of these methods are observed between the two periods. First, the proportion of studies using MI in the base-case analysis has considerably increased over time (from 28 to 39%), which is compensated by a decrease in the proportion of less advanced methods such as CCA (from 14 to 5%) and single imputation (SI) (from 21 to 16%). Second, the number of studies not clearly reporting the methods has also considerably decreased (from 12 to 5%). The observed trend between the two periods may be the result of the specific recommendations from the 2016 guidelines in regards to the ‘optimal’ missing data strategy, resulting in a more frequent adoption of MI techniques and, at the same time, a less frequent use of CCA in the base-case analysis. However, in contrast to these guidelines, a large number of studies still does not perform any SA to missing data assumptions (about 65% in 2010–2015 and 63% in 2016–2020).\nMost of the studies lie in the middle and lower parts of the plot, and are associated with a limited or sufficient quality of information. However, only a few of these studies rely on very strong and unjustified missing data assumptions, while the majority provides either adequate justifications or uses methods associated with weak assumptions. Only 11 (14%) studies are associated with both high-quality scores and less restrictive missingness assumptions. No study was associated with either full information or adequate justifications for the assumptions explored in base-case and sensitivity analysis.\n\n\nDiscussion\nDescriptive information extracted from the reviewed studies provides some first insights about changes in practice in the years following the publication of the guidelines. First, a clear trend is observed towards an increase in the adoption of a societal and health care perspective and of CUA as the reference base-case analysis approach. Second, a similar increment is observed in the use of recommended instruments for the collection and valuation of health economic outcomes, such as EQ-5D-5L for QALYs and friction method for costs. Most of these changes are in accordance with the 2016 guidelines, which are likely to have played a role in influencing analysts and practitioners towards a clearer and more standardised way to report health economic results.\nWhen looking at the type of statistical methods used to perform the analysis, an important shift occurs between the two periods towards the use of methods that allow for regression adjustment, with a considerable uptake in the use of SURs and LMMs in the context of empirical analyses. These techniques are strongly supported by the 2016 guidelines in that they allow us to correct for potential bias due to confounding effects, deal with clustered data and formally take into account the correlation between costs and effects. Bootstrapping remains the most popular methods to quantify uncertainty around parameter estimates across both periods. However, the health economic analysis framework requires that the level of complexity of the analysis model is reflected in the way uncertainty surrounding the estimates is generated.\nThe transition between the two time periods reveals an increase in the use of MI techniques in the base-case analysis together with a decrease in the overall use of CCA. This change is in line with the 2016 guidelines which warns about the inherent limitations and potential bias of simple methods (e.g. CCA) when compared to MI as the potential reference method to handle missing values. Nevertheless, improvements are still needed given that many studies (more than 6%) performed the analysis under a single missing data assumption. This is not ideal since by definition missing data assumptions can never be checked, making the results obtained under a specific method (i.e. assumption) potentially biased.\n\n\nConclusions\nGiven the complexity of the health economics framework, the implementation of simple but likely inadequate analytic approaches may lead to imprecise cost-effectiveness results. This is a potentially serious issue for bodies such as ZIN in the Netherlands that use these evaluations in their decision making, thus possibly leading to incorrect policy decisions about the cost-effectiveness of new health care interventions. Our review shows, over time, a change in common practice with respect to different analysis components in accordance with the recent ZIN’s 2016 guidelines. This is an encouraging movement towards the standardised use of more suitable and robust analytic methods in terms of both statistical, uncertainty and missing data analysis. Improvements are however still needed, particularly in the choice of adequate statistical techniques to deal with the complexity of the data analysed and in the assessment of the impact of alternative missing data assumptions on the results in SA."
  },
  {
    "objectID": "research/mnarHTA/mnarHTA.html",
    "href": "research/mnarHTA/mnarHTA.html",
    "title": "Nonignorable Missingness Models in Health Technology Assessment",
    "section": "",
    "text": "Economic evaluation alongside Randomised Clinical Trials (RCTs) is an important and increasingly popular component of the process of technology appraisal. The typical analysis of individual level data involves the comparison of two interventions for which suitable measures of clinical benefits and costs are observed on each patient enrolled in the trial at different time points throughout the follow up. Individual level data from RCTs are almost invariably affected by missingness. The recorded outcome process is often incomplete due to individuals who drop out or are observed intermittently throughout the study, causing some observations to be missing. In most applications, the economic evaluation is performed on the cross-sectional variables, computed using only the data from the individuals who are observed at each time point in the trial (completers), with at most limited sensitivity analysis to missingness assumptions. This, however, is an extremely inefficient approach as the information from the responses of all partially observed subjects is completely lost and it is also likely biased unless the completers are a random sample of the subjects on each arm. The problem of missingness is often embedded within a more complex framework, which makes the modelling task in economic evaluations particularly challenging. Specifically, the effectiveness and cost data typically present a series of complexities that need to be simultaneously addressed to avoid biased results.\nUsing a recent randomised trial as our motivating example, we present a Bayesian parametric model for conducting inference on a bivariate health economic longitudinal response. We specify our model to account for the different types of complexities affecting the data while accommodating a sensitivity analysis to explore the impact of alternative missingness assumptions on the inferences and on the decision-making process for health technology assessment."
  },
  {
    "objectID": "research/mnarHTA/mnarHTA.html#modelling-framework",
    "href": "research/mnarHTA/mnarHTA.html#modelling-framework",
    "title": "Nonignorable Missingness Models in Health Technology Assessment",
    "section": "Modelling framework",
    "text": "Modelling framework\nThe distribution of the observed responses \\(\\boldsymbol y_{ijt}=(u_{ijv},c_{ijt})\\) is specified in terms of a model for the utility and cost variables at time \\(j=\\{0,1,2\\}\\), which are jointly modelled without using a multilevel approach and separately by treatment group. In particular, the joint distribution for \\(\\boldsymbol y_{ijt}\\) is specified as a series of conditional distributions that capture the dependence between utilities and costs as well as the time dependence.\nFollowing the recommendations from the published literature, we account for the skewness using Beta and Log-Normal distributions for the utilities and costs, respectively. Since the Beta distribution does not allow for negative values, we scaled the utilities on \\([0,1]\\) through the transformation \\(u^{\\star}_{ij}=\\frac{u_{ij}-\\text{min}(\\boldsymbol u_{j})}{\\text{max}(\\boldsymbol u_{j})-\\text{min}(\\boldsymbol u_{j})}\\), and fit the model to these transformed variables. To account for the structural values \\(u_{ij} = 1\\) and \\(c_{ij} = 0\\) we use a hurdle approach by including in the model the indicator variables \\(d^u_{ij}:=\\mathbb{I}(u_{ij}=1)\\) and \\(d^c_{ij}:=\\mathbb{I}(c_{ij}=0)\\), which take value \\(1\\) if subject \\(i\\) is associated with a structural value at time \\(j\\) and 0 otherwise. The probabilities of observing these values, as well as the mean of each variable, are then modelled conditionally on other variables via linear regressions defined on the logit or log scale. Specifically, at time \\(j=1,2\\), the probability of observing a zero and the mean costs are modelled conditionally on the utilities and costs at the previous times, while the probability of observing a one and the mean utilities are modelled conditionally on the current costs (also at \\(j=0\\)) and the utilities at the previous times (only at \\(j=\\{1,2\\}\\)). The model is summarised by Figure 1:\n\n\n\n\n\n\nFigure 1: Longitudinal model for missingness.\n\n\n\nWe use partial identifying restrictions to link the observed data distribution \\(p(\\boldsymbol y_{obs},\\boldsymbol r)\\) to the extrapolation distribution \\(p(\\boldsymbol y_{mis} \\mid \\boldsymbol y_{obs},\\boldsymbol r)\\) and consider interpretable deviations from a benchmark scenario to assess how inferences are driven by our assumptions. Specifically, we identify the marginal mean of the missing responses in each pattern \\(\\boldsymbol y^{\\boldsymbol r}_{mis}\\) by averaging across the corresponding components that are observed and add the sensitivity parameters \\(\\boldsymbol \\Delta_j\\).\nWe define \\(\\boldsymbol \\Delta_j=(\\Delta_{c_{j}},\\Delta_{u_{j}})\\) to be time-specific location shifts at the marginal mean in each pattern and set \\(\\boldsymbol \\Delta_j = \\boldsymbol 0\\) as the benchmark scenario. We then explore departures from this benchmark using alternative priors on \\(\\boldsymbol \\Delta_j\\), which are calibrated using the observed standard deviations for costs and utilities at each time \\(j\\) to define the amplitude of the departures from \\(\\boldsymbol \\Delta_j=\\boldsymbol 0\\)."
  },
  {
    "objectID": "research/lmmHTA/lmmHTA.html",
    "href": "research/lmmHTA/lmmHTA.html",
    "title": "Linear mixed models to handle missing at random data in trial based economic evaluations",
    "section": "",
    "text": "Introduction\nCost‐effectiveness analyses (CEAs) conducted alongside randomised controlled trials are an important source of information for decision-makers in the process of technology appraisal (Ramsey et al., 2015). The analysis is based on healthcare outcome data and health service use, typically collected at multiple time points and then combined into overall measures of effectiveness and cost. A popular approach to handle missingness is to discard the participants with incomplete observations (complete case analysis or CCA), allowing for derivation of the overall measures based on the completers alone. We note that slightly different definitions of CCA are possible, depending on the form of the model of interest, the type of missingness and the inclusion of observed covariates. This approach, although appealing by its simplicity, has well-recognised limitations including loss of efficiency and an increased risk of bias. We propose the use of linear mixed effects models (LMMs) as an alternative approach under MAR. LMMs are commonly used for the modelling of dependent data (e.g. repeated-measures) and belong to the general class of likelihood-based methods. LMMs appear surprisingly uncommon for the analysis of repeated measures in trial-based CEA, perhaps because of a lack of awareness or familiarity with fitting LMMs.\n\n\nMethods\nLinear mixed model extends the usual linear model framework by the addition of “random effect” terms, which can take into account the dependence between observations.\n\\[\nY_{ij}=\\beta_1+\\beta_2 X_{i1}+\\ldots+\\beta_(P+1) X_{iP}+\\omega_i+\\epsilon_{ij},\n\\tag{1}\\]\nwhere \\(Y_{ij}\\) denotes the outcome repeatedly collected for each individual \\(i=1,\\ldots,N\\) at multiple times \\(j=1,\\ldots,J\\). The model parameters commonly referred to as fixed effects include an intercept \\(\\beta_1\\) and the coefficients \\((\\beta_2,\\ldots,\\beta_{(P+1)})\\) associated with the predictors \\(X_{i1},\\ldots,X_{iP}\\), while \\(\\omega_i\\) and \\(\\epsilon_{ij}\\) are two random terms: \\(\\epsilon_{ij}\\) is the usual error term and \\(\\omega_i\\) is a random intercept which captures variation in outcomes between individuals. The models can be extended to deal with more complex structures, for example by allowing the effect of the covariates to vary across individuals (random slope) or a different covariance structure of the errors. LMMs can be fitted even if some outcome data are missing and provide correct inferences under MAR.\nA particular type of LMMs commonly used in the analysis of repeated measures in clinical trials is referred to as Mixed Model for Repeated Measurement (MMRM). The model includes a categorical effect for time, an interaction between time and treatment arm, and allows errors to have different variance and correlation over time (i.e. unstructured covariance structure). Figure 1 shows some examples of possible covariance structures that may be explored for LMMs.\n\n\n\n\n\n\nFigure 1: Some examples of covariance structures in LMM\n\n\n\nIncremental (between-group) or marginal (within-group) estimates for aggregated outcomes over the trial period, such as quality-adjusted life years (QALYs) or total costs can be retrieved as linear combinations of the parameter estimates from Equation 1. For example, the mean difference in total cost is obtained by summing up the estimated differences at each follow-up point, while differences on a QALY scale can be obtained as weighted linear combinations of the coefficient estimates of the utility model.\n\n\nConclusions\nWe believe LMMs represent an alternative approach which can overcome some of these limitations.\n\nFirst, practitioners may be more comfortable with the standard regression framework.\nSecond, LMMs can be tailored to address other data features (e.g. cluster-randomised trials or non-normal distribution) while also easily combined with bootstrapping.\nThird, LMMs do not rely on imputation, and results are therefore deterministic and easily reproducible, whereas the Monte Carlo error associated with multiple imputation may cause results to vary from one imputation to another, unless the number of imputations is sufficiently large.\n\nAlthough the methodology illustrated is already known, particularly in the area of statistical analyses, to our knowledge LMMs have rarely been applied to health economic data collected alongside randomised trials. We believe the proposed methods is preferable to a complete-case analysis when CEA data are incomplete, and that it can offer an interesting alternative to imputation methods."
  },
  {
    "objectID": "research/hurdleHTA/hurdleHTA.html",
    "href": "research/hurdleHTA/hurdleHTA.html",
    "title": "Bayesian Modelling for Health Economic Evaluations",
    "section": "",
    "text": "Modelling Framework\nWe propose a unified Bayesian framework that jointly accounts for the typical complexities of the data (e.g. correlation, skewness, spikes at the boundaries and missingness), and that can be implemented in a relatively easy way.\nConsider the usual cross-sectional bivariate outcome formed by the QALYs and total cost variables \\((e_{it}, c_{it})\\) calculated for the \\(i-\\)th person in group \\(t\\) of the trial. To simplify the notation, unless necessary, we suppress the treatment indicator \\(t\\). Equation 1 specifies the joint distribution \\(p(e_i,c_i)\\) as\n\\[\np(e_i,c_i) = p(c_i)p(e_i\\mid c_i) = p(e_i)p(c_i\\mid e_i)\n\\tag{1}\\]\nwhere, for example, \\(p(e_i)\\) is the marginal distribution of the QALYs and \\(p(c_i\\mid e_i)\\) is the conditional distribution of the costs given the QALYs. Note that, although the two factorisations are mathematically equivalent, the choice of which to use has different practical implications. From a statistical point of view, the factorisations require the specifications of different statistical models, e.g. \\(p(e_i)\\) or \\(p(e_i\\mid c_i)\\), which may have different approximation errors. From a clinical point of view, the two versions make different assumptions about the casual relationships between the outcomes, i.e. either \\(e_i\\) determines \\(c_i\\) or vice versa. We describe our analysis under the assumption that the costs are determined by the effectiveness measures and therefore we specify the joint distribution \\(p(e_i,c_i)\\) in terms of a marginal distribution for the QALYs and a conditional distribution for the costs.\nFor each individual we consider a marginal distribution \\(p(e_i \\mid \\boldsymbol \\theta_e)\\) indexed by a set of parameters \\(\\boldsymbol \\theta_e\\) comprising a location \\(\\boldsymbol \\phi_{ie}\\) and a set of ancillary parameters \\(\\boldsymbol\\psi_e\\) typically including some measure of marginal variance \\(\\sigma^2_e\\). Equation 2 models the location parameter using a generalised linear structure\n\\[\ng_e(\\phi_{ie})= \\alpha_0 \\,\\,[+ \\ldots]\n\\tag{2}\\]\nwhere \\(\\alpha\\_0\\) is the intercept and the notation \\([+\\ldots]\\) indicates that other terms (e.g. quantifying the effect of relevant covariates) may or may not be included. In the absence of covariates or assuming that a centered version \\(x_i^{\\star} = (x_i - \\bar{x})\\) is used, the parameter \\(\\mu_e = g_e^{-1}(\\alpha_0)\\) represents the population average QALYs. For the costs, we consider a conditional model \\(p(c_i\\mid e_i,\\boldsymbol\\theta_c)\\), which explicitly depends on the QALYs, as well as on a set of quantities \\(\\boldsymbol\\theta_c\\), again comprising a location \\(\\phi_{ic}\\) and ancillary parameters \\(\\boldsymbol \\psi_{c}\\). For example, when normal distributions are assumed for both \\(p(e_i \\mid \\boldsymbol \\theta\\_e)\\) and \\(p(c_i \\mid e_i, \\boldsymbol \\theta_c)\\), i.e. bivariate normal on both outcomes, the ancillary parameters \\(\\boldsymbol\\psi_c\\) include a conditional variance \\(\\tau^2_c\\), which can be expressed as a function of the marginal variance \\(\\sigma^2_c\\). More specifically, the conditional variance of \\(p(c_i \\mid e_i, \\boldsymbol \\theta_c)\\) is a function of the marginal effectiveness and cost variances and has the closed form \\(\\tau^2_c=\\sigma^2_c - \\sigma^2_e \\beta^2\\), where \\(\\beta=\\rho \\frac{\\sigma_c}{\\sigma_e}\\) and \\(\\rho\\) is the parameter capturing the correlation between the variables.\nEquation 3 models the location as a function of the QALYs as\n\\[\ng\\_c(\\phi\\_{ic}) = \\beta\\_{0} + \\beta\\_{1}(e\\_{i}-\\mu\\_{e})\\,\\,[+\\ldots]\n\\tag{3}\\]\nHere, \\((e_i-\\mu_e)\\) is the centered version of the QALYs, while \\(\\beta_{1}\\) quantifies the correlation between costs and QALYs. Assuming other covariates are either also centered or absent, \\(\\mu_c = g_c^{-1}(\\beta_{0})\\) is the estimated population average cost. The Figure 1 shows a graphical representation of the general modelling framework.\n\n\n\n\n\n\nFigure 1: Modelling framework.\n\n\n\nThe QALYs and cost distributions are represented in terms of combined modules, the blue and the red boxes, in which the random quantities are linked through logical relationships. This ensures the full characterisation of the uncertainty for each variable in the model. Notably, this is general enough to be extended to any suitable distributional assumption, as well as to handle covariates in either or both the modules.\nThe proposed framework allows jointly tackling of the different complexities that affect the data in a relatively easy way by means of its modular structure and flexible choice for the distributions of the QALYs and cost variables. Using the MenSS trial as motivating example, we start from the original analysis and expand the model using alternative specifications that progressively account for an increasing number of complexities in the outcomes. We specifically focus on appropriately modelling spikes at the boundary and missingness, as they have substantial implications in terms of inferences and, crucially, cost-effectiveness results.\n\n\nExample\nThree model specifications are considered and applied to QALY data from a RCT case study: 1) Normal marginal for the QALYs and Normal conditional for the costs (which is identical to a Bivariate Normal distribution for the two outcomes); 2) Beta marginal for the QALYs and Gamma conditional for the costs; and 3) Hurdle Model. Figure 2 shows the observed QALYs in both treatment groups (indicated with black crosses) as well as summaries of the posterior distributions for the imputed values, obtained from each model. Imputations are distinguished based on whether the corresponding baseline utility value is observed or missing (blue or red lines and dots, respectively) and are summarised in terms of posterior mean and \\(90\\%\\) HPD intervals.\n\n\n\n\n\n\nFigure 2: Imputed QALYs under alternative model specifications.\n\n\n\nThere are clear differences in the imputed values and corresponding credible intervals between the three models in both treatment groups. Neither the Bivariate Normal nor the Beta-Gamma models produce imputed values that capture the structural one component in the data. In addition, as to be expected, the Bivariate Normal fails to respect the natural support for the observed QALYs, with many of the imputations exceeding the unit threshold bound. These unrealistic imputed values highlight the inadequacy of the Normal distribution for the data and may lead to distorted inferences. Conversely, imputations under the Hurdle Model are more realistic, as they can replicate values in the whole range of the observed data, including the structural ones. Imputed unit QALYs with no discernible interval are only observed in the intervention group due to the original data composition, i.e. individuals associated with a unit baseline utility and missing QALYs are almost exclusively present in the intervention group.\n\n\nConclusions\nWe have presented a flexible Bayesian framework that can handle the typical complexities affecting outcome data in CEA, while also being relatively easy to implement using freely available Bayesian software. This is a key advantage that can encourage practitioners to move away from likely biased methods and promote the use of our framework in routine analyses. In conclusion, the proposed framework can:\n\nJointly model costs and QALYs;\nAccount for skewness and structural values;\nAssess the robustness of the results under a set of differing missingness assumptions.\n\nThe original contribution of this work consists in the joint implementation of methods that account for the complexities of the data within a unique and flexible framework that is relatively easy to apply. In the next chapter we will take a step forward in the analysis and present a longitudinal model that can use all observed utility and cost data in the analysis, explore alternative nonignorable missing data assumptions, while simultaneously handling the complexities that affect the data."
  },
  {
    "objectID": "posts/2020-12-20-my-blog-post/index.html",
    "href": "posts/2020-12-20-my-blog-post/index.html",
    "title": "It is Xmas again Yeah",
    "section": "",
    "text": "It is Xmas again! Wow, how quickly time flies. The situation here in the Netherlands is not ideal as the number of infected is on the rise again and the government has declared a full lockdown until January 19th. Starting from next month I will start teaching in the department but everything will still be online due to the uncertainty of the pandemic. It is unfortunate but there is nothing we can do about it and let us just hope the situation will improve in the next few weeks/months.\nAside from sad stuff, I have some good news about my research. My paper about longitudinal models in trial-based CEA, whose pre-print is available on my ArXiv account, has been officially accepted for publication in Value in Health, after a very long time (more than 2 years of peer-review process I believe!). I am really happy as this is a paper for which I spent a lot of time and effort, so it is nice to see some ouput out of it. I am also glad to announce that a new paper I am co-authoring with Baptiste (LSHTM) and Catrin (Bangor University) which is a tutorial on the use of mixed effects models for trial-based CEAs. The main idea is to make available and spread awareness in the HE community about the possibility to use these models to derive CEA results, rather than relying on the standard linear regression models. The methods are nothing special but we believe health economists are not well aware of the fact that sometimes they can be implemented in a much easier way compared to standard approaches. We tried to summarise the main advantages and disadvantages of the methods while also providing software code (in STATA and R) to show how they can be fitted. The paper has been accepted as a contributed presentation at the next HESG meeting. Unfortunately, I will not be able to attend the meeting as I will be quite busy with the teaching that week but Catrin has kindly agreed to present in behalf of all authors. I look forward to receive the feedback from the people attending the conference!\nThe past month I have been quite busy with some teaching and consultancy work here at UM, mostly for medical students about basic statistical concepts and techniques but nonetheless very interesting for me in order to get acquainted with the new job and duties. A big thank is due to my new colleagues who have been incredibly nice to me and have helped me a lot to get into the system. I was actually planning to post my news earlier this month but with incredible disappointment I found out that the most recent updated of Hugo (the system used by blogdown for building this site) completely messed up my previous version of the website which was broken. I had to rebuild the website again copying and pasting all my previous material. Nothing crazy, but certainly very annoying to do. I just hope the next update will not force me to do it again! So, if you see something different compared to before, you know why (especially in the sections for the tutorials on using BUGS/JAGS/STAN). Related to this, I must acknowledge Mr. Kim who recently contacted me with regard to an incorrect specification for one of the STAN models in the tutorial on generalised linear mixed models. I would like to thank him again very much for noticing this mistake which I fixed in the new version of the website now online (apologies for the long time it took for me to make the changes!). I am happy to see that my code can be of help to anyone who might be interested in doing some nice modelling.\nFinally, a quick note about somehting I found curious. A medical statistician who I deeply respect and admire, Dr.Tim Morris from MRCCTU, posted the following provocative tweet\n\nwhere he asks about the interpretation of a Bayesian credible interval while also saying that quick and simple answers will not be considered reliable. Well, I have much to say on this but I feel like twitter is not the best location to try and argument a proper discussion. I hope I will be able to find some time to post on my website a more constructive answer. The topic is not very quick to grasp, especially if someone is used to think in frequentist terms and theory which, by definition, are not very useful to think at statistics from a Bayesian point of view. For the moment, as a quick answer, I would just say that in my opinion statistics is not a uniquely defined discipline but there are different ways it can be approached, and the rules and theory of one approach do not necessarily apply to others!\nNext time, perhaps, I will follow this with a proper argument, stay tuned!"
  },
  {
    "objectID": "posts/2020-10-10-my-blog-post/index.html",
    "href": "posts/2020-10-10-my-blog-post/index.html",
    "title": "Why health economists do not care about statistical significance?",
    "section": "",
    "text": "Hello dear readers!\nI have finally come back from my lethargy with a new exciting posts about why the job of health economists, although inevitably involving some statistics, can be very different from what standard statisticians typically do. To make this point, I will abuse and give my own opinion of the notorious (and now quite old) paper published by Dr. Claxton (Claxton (1999)).\nBefore starting my discussion I would like to point out a very nice post made by Sam Watson on the Academic Health Economists’ Blog, who nicely provided his own opinion on the matter and discussed the pros and cons of the paper. I really recommend people interested in knowing more about health economics to check out this blog which is one of the most popular in the field and which has contributions from many many health economists on different topics.\nIn this post, in contrast to what Sam did in his own, I will not focus on the technical details about the health economics decision-making problem of minimising the expected loss / maximising the utility function in terms of societal preferences (i.e. social welfare in terms of effectiveness and costs). Instead, here I would like to address the much simpler and basic question why health economists do not use statistical significance in their analyses?. In my own experience, especially in trial-based analyses, I had to work closely with both health economists and statisticians who often are unaware of what the other “people” are doing. Since cost-effectiveness analyses have become more and more important over the last decade, I believe it is imperative that even statisticians should learn at least the basics of what their colleagues are doing and most importantly why. Of course, the same applies for health economists who should know a bit of statistics in order to do their job. However, in general, I have the feeling statisticians are reluctant to care about health economics, perhaps due to different statistical methodology required for these analyses compared with standard methods used for the analysis of clinical outcomes.\nApologies for the very long premise. So, let’s start with the most basic question of all. what is the objective of health economics ? To inform decision-makers about the cost-effectiveness of a given treatment with respect to alternatives. Unlike the standard clinical analysis which focuses merely on detecting whether there is a “clinically relevant” difference between treatments in terms of some pre-defined outcome measure, the health economic analysis does not qualify as a mere yes/no problem due to the existence of opportunity costs. These are the losses, either in terms of forgone benefits or extra costs, that the society/health care provider would incur by funding a treatment which is not cost-effective compared to others. Precisely, the decision-making nature of the problem comes from the fact that only a limited amount of resources are available to decision-makers, who need to define some sort of rule to prioritise the distribution of the funds across a pool of possible treatments. Thus, if the objective is to maximise the health benefits for a given budget, then treatments should be selected based on a target quantity, which summarises effectiveness, costs and the preferences of decision-makers, while also taking into account the uncertainty about our conclusions. This quantity takes the name of incremental net benefit and is given by\n\\[\n\\text{INB} = K \\Delta_e - \\Delta_c,\n\\]\nwhere \\(\\Delta_e\\) and \\(\\Delta_c\\) are the mean differences between two competing interventions in terms of some effectiveness and cost measures, while \\(K\\) is the acceptance threshold representing the budget the decision-maker is willing to spend to obtain an increment of one unit of effectiveness (cost per unit of effectiveness gained). If we were to evaluate an hypothesis test about cost-effectiveness of treatment 2 vs treatment 1, we would then test: \\(\\text{INB} \\leq 0\\) (\\(H_0\\)) vs \\(\\text{INB} &gt; 0\\) (\\(H_1\\)).\nA natural test statistic for this hypothesis is:\n\\[\nt = \\frac{\\sqrt{n}Kd_e-d_c}{\\sqrt{K^2 s^2_e -2Ks_{ce}+s^2_c}},\n\\]\nwhere \\(d_e\\) and \\(d_c\\) are the in-sample estimates of the corresponding population values, while \\(s\\) are the sample standard deviations. Under the null, this test statistic has a t-student distribution with \\(n-1\\) degrees of freedom. However, we can see how not rejecting the null hypothesis when a new treatment has a positive but statistically insignificant mean incremental net benefit imposes unnecessary costs which can be valued in either monetary or effectiveness terms. Decisions should be based only on the mean irrespective of whether any differences in this quantity are regarded as statistically significant. This is because one of the mutually exclusive alternatives must be chosen and this decision cannot be deferred. The opportunity costs of failing to make the correct decision based on a single test statistic are symmetrical while the definition of which of the alternatives is regarded as current practice is completely irrelevant."
  },
  {
    "objectID": "posts/2020-10-10-my-blog-post/index.html#so-how-do-you-make-the-decision-if-there-no-hypothesis-test",
    "href": "posts/2020-10-10-my-blog-post/index.html#so-how-do-you-make-the-decision-if-there-no-hypothesis-test",
    "title": "Why health economists do not care about statistical significance?",
    "section": "So how do you make the decision if there no hypothesis test ?",
    "text": "So how do you make the decision if there no hypothesis test ?\nAlthough the ordinary rules of statistical analyses do not apply, we still need to summarise the uncertainty associated with our results based on the mean INB given that decision-making problems must take into account and quantify the impact that sampling uncertainty has on our conclusions. This is typically achieved through the use of re-sampling methods, such as bootstrapping, (frequentist approach) or Bayesian methods. The two approaches are very different from a theoretical perspective but I will not address this point to save me of few weeks of time. Here, I am most interested in what type of results we will obtain once applied these methods. Well, typically we will look at something like this:\n\n\n\nCost-Effectiveness Plane"
  },
  {
    "objectID": "posts/2020-08-07-my-blog-post/index.html",
    "href": "posts/2020-08-07-my-blog-post/index.html",
    "title": "What is Bayesian inference?",
    "section": "",
    "text": "What is probability? The answer to this question is generally acknowledged to be the one that respects the so called Kolmogorov axioms which can be brutally simplified to:\nOne of the ways in which Bayesian statistics differs from classical statistics is in the interpretation of probability. Differences in interpretation continue to be controversial, are critical to the distinction between Bayesian and non-Bayesian statistics.\nIn classical statistics probability is often understood as a property of the phenomenon being studied: for instance, the probability that a tossed coin will come up heads is a characteristic of the coin. Thus, by tossing the coin many times under more or less identical conditions, and noting the result of each toss, we can estimate the probability of a head, with the precision of the estimate monotonically increasing with the number of tosses. In this view, probability is the limit of a long-run, relative frequency; i.e. if \\(A\\) is an event of interest (e.g. the coin lands heads up) then\n\\[\n\\text{Pr}(A) = \\lim_{n\\rightarrow\\infty}\\frac{m}{n}\n\\]\nis the probability of \\(A\\), where \\(m\\) is the number of times we observe the event \\(A\\) and \\(n\\) is the number of repetitions. Given this definition of probability, we can understand why classicial statistics is sometimes referred to as frequentist and objectivist. However, historians of science stress that at least two notions of probability were under development from the late \\(1600\\)s onwards: the objectivist view described above, and a subjectivist view. With regard to the latter, we can consider different ‘degrees’ of belief to interpret probability, ‘from the very neighbourhourhood of certainty and demonstration, quite down to improbability and unlikeliness, even to the confines of impossibility’. For Locke, ‘Probability is likeliness to be true’, a definition in which (repeated) games of chance play no part. For Bernoulli, ‘Probability is degree of certainty and differs from absolute certainty as the part differs from the whole’, it being unequivocal that the ‘certainty’ referred to is a state of mind, but, critically, (1) varied from person to person (depending on one’s knowledge and experience) and (2) was quantifiable. Ramsey and de Finetti, working independently, showed that subjective probability is not just any set of subjective beliefs, but beliefs that conform to the axioms of probability. The Ramsey-de Finetti Theorem states that if \\(p_1, p_2, \\ldots\\) are a set of betting quotients on hypotheses \\(h_1, h_2,\\ldots\\) , then if the \\(p_j\\) do not satisfy the probability axioms, there exists a betting strategy and a set of stakes such that whoever follows this betting strategy will lose a finite sum whatever the truth values of the hypotheses turn out to be. In de Finetti’s terminology, subjective probabilities that fail to conform to the axioms of probability are incoherent or inconsistent. Thus, subjective probabilities are whatever a particular person believes, provided they satisfy the axioms of probability. Thus, if I do not update my subjective beliefs in light of new information (data) in a manner consistent with the probability axioms, and you can convince me to gamble with you, you have the opportunity to take advantage of my irrationality, and are guaranteed to profit at my expense. That is, while probability may be subjective, Bayes Rule governs how rational people should update subjective beliefs."
  },
  {
    "objectID": "posts/2020-08-07-my-blog-post/index.html#subjective-probability",
    "href": "posts/2020-08-07-my-blog-post/index.html#subjective-probability",
    "title": "What is Bayesian inference?",
    "section": "Subjective probability",
    "text": "Subjective probability\nBayesian probability statements are thus about states of mind over states of the world, and not about states of the world per se. Indeed, whatever one believes about determinism or chance in social processes, the meaningful uncertainty is that which resides in our brains, upon which we will base decisions and actions. This is why, in one of the more memorable and strongest statements of the subjectivist position, de Finetti writes probability does not exist: “The abandonment of superstitious beliefs about \\(\\ldots\\) Fairies and Witches was an essential step along the road to scientific thinking. Probability, too, if regarded as something endowed with some kind of objective existence, is not less a misleading misconception, an illusory attempt to exteriorize or materialize our true probabilistic beliefs. In investigating the reasonableness of our own modes of thought and behaviour under uncertainty, all we require, and all that we are reasonably entitled to, is consistency among these beliefs, and their reasonable relation to any kind of relevant objective data”.\nThe use of subjective probability also means that Bayesians can report probabilities without a “practically unlimited” sequence of observations. What is the frequentist probability of the truth of the proposition “Jackson was the eighth president”? Since there is only one relevant experiment for this problem, the frequentist probability is either zero (if Jackson was not the eighth president) or one (if Jackson was the eighth president). Non-trivial frequentist probabilities, it seems, are reserved for phenomena that are standardized and repeatable. Bayes Theorem itself is uncontroversial: it is merely an accounting identity that follows from the axioms of probability discussed above, plus the following additional definition.\n\nConditional probability. Let \\(A\\) and \\(B\\) be events with \\(P(B)&gt;0\\). Then the conditional probability of \\(A\\) given \\(B\\) is\n\n\\[\nP(A\\mid B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nThe following two useful results are also implied by the probability axioms, plus the definition of conditional probability\n\nMultiplication rule\n\n\\[\nP(A \\cap B) = P(A\\mid B)P(B) = P(B\\mid A)P(A)\n\\]\n\nLaw of total probability\n\n\\[  \nP(B) = P(A\\cap B)+ P\\overline{(A\\cap B)} = P(B\\mid A)P(A) + P(B \\mid \\overline{A})P(\\overline{A})\n\\]"
  },
  {
    "objectID": "posts/2020-08-07-my-blog-post/index.html#bayes-theorem",
    "href": "posts/2020-08-07-my-blog-post/index.html#bayes-theorem",
    "title": "What is Bayesian inference?",
    "section": "Bayes theorem",
    "text": "Bayes theorem\nBayes Theorem can now be stated, following immediately from the definition of conditional probability. If \\(A\\) and \\(B\\) are events with \\(P(B)&gt;0\\), then\n\\[\nP(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B)}\n\\]\nIf we consider the event \\(A=H\\) to be an hypothesis and the event \\(B=E\\) to be observing some evidence, then \\(Pr(H\\mid E)\\) is the probability of \\(H\\) after obtaining \\(E\\), and \\(\\text{Pr}(H)\\) is the prior probability of \\(H\\) before considering \\(E\\). The conditional probability on the left-hand side of the theorem, \\(\\text{Pr}(H\\mid E)\\), is usually referred to as the posterior probability of \\(H\\). Bayes Theorem thus supplies a solution to the general problem of inference or induction, providing a mechanism for learning about the plausibility of a hypothesis \\(H\\) from data \\(E\\).\nIn most analyses in the social sciences, we want to learn about a continuous parameter, rather than the discrete parameters considered in the discussion thus far. Examples include the mean of a continuous variable, a proportion (a continuous parameter on the unit interval), a correlation, or a regression coefficient. In general, let the unknown parameter be \\(\\theta\\) and denote the data available for analysis as \\(\\boldsymbol y = (y_1, \\ldots , y_n)\\). In the case of continuous parameters, beliefs about the parameter are represented as probability density functions or pdfs; we denote the prior pdf as \\(p(\\theta)\\) and the posterior pdf as \\(p(\\theta \\mid \\boldsymbol y)\\). Then, Bayes Theorem for a continuous parameter is as follows:\n\\[\np(\\theta \\mid y) = \\frac{p(y \\mid \\theta) p(\\theta)}{\\int p(y \\mid \\theta) p(\\theta) d\\theta},\n\\]\nwhich is often approximated by\n\\[\np(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta),\n\\]\nwhere the proportionality constant is \\(\\left[ \\int p(y \\mid \\theta) p(\\theta) d\\theta \\right]^{-1}\\) which ensures that the posterior density integrates to one, as a proper probability density. The first term on the right hand side of the Equation is the likelihood function, the probability density of the data \\(y\\), considered as a function of \\(\\theta\\). This formulation of Bayes Rule highlights a particularly elegant feature of the Bayesian approach, showing how the likelihood function \\(p(\\boldsymbol y|\\theta)\\) can be “inverted” to generate a probability statement about \\(\\theta\\), given data \\(y\\). Thus, from a Bayesian perspective, likelihood based analyses of data assume prior ignorance, although seldom is this assumption made explicit, even if it were plausible. In other cases, when working with the so-called conjugate priors in the exponential family, the mean of the posterior distribution is a precision-weighted average of the prior and the likelihood. Suppose a prior density \\(p(\\theta)\\) belongs to a class of parametric of densities, \\(F\\). More specifically, the prior density is said to be conjugate with respect to a likelihood \\(p(y \\mid \\theta)\\) if the posterior density \\(p(\\theta \\mid y )\\) is also in \\(F\\).\nBayesian statistical inference is equivalent to combining information, marrying the information in the prior with the information in the data, with the relative contributions of prior and data to the posterior being proportional to their respective precision. That is, Bayesian analysis with conjugate priors over a parameter \\(\\theta\\) is equivalent to taking a precision-weighted average of prior information about \\(\\theta\\) and the information in the data about \\(\\theta\\). Thus:\n\nThus, when prior beliefs about \\(\\theta\\) are ‘vague’, ‘diffuse’, or, in the limit, uninformative, the posterior density will be dominated by the likelihood (i.e. the data contains much more information than the prior about the parameters);\nWhen prior information is available, the posterior incorporates it, and rationally, in the sense of being consistent with the laws of probability via Bayes Theorem. In fact, when prior beliefs are quite precise relative to the data, it is possible that the likelihood is largely ignored, and the posterior distribution will look almost exactly like the prior\n\nNote also that via Bayes Rule, if a particular region of the parameter space has zero prior probability, then it also has zero posterior probability. This feature of Bayesian updating has been dubbed Cromwell’s Rule by Lindley. The point here is that posterior distributions can sometimes look quite unusual, depending on the form of the prior and the likelihood for a particular problem. The fact that a posterior distribution may have a peculiar shape is of no great concern in a Bayesian analysis: provided one is updating prior beliefs via Bayes Rule, all is well. Unusual looking posterior distributions might suggest that one’s prior distribution was poorly specified, but, as a general rule, one should be extremely wary of engaging this kind of procedure. Bayes Rule is a procedure for generating posterior distributions over parameters in light of data. Although one can always re-run a Bayesian analysis with different priors (and indeed, this is usually a good idea), Bayesian procedures should not be used to hunt for priors that generate the most pleasing looking posterior distribution given a particular data set and likelihood. Indeed, such a practice would amount to an inversion of the Bayesian approach: i.e. if the researcher has strong ideas as to what values of \\(\\theta\\) are more likely than others, aside from the information in the data, then that auxiliary information should be considered a prior, with Bayes Rule providing a procedure for rationally combining that auxiliary information with the information in the data."
  },
  {
    "objectID": "posts/2020-08-07-my-blog-post/index.html#bayesian-updating-of-information",
    "href": "posts/2020-08-07-my-blog-post/index.html#bayesian-updating-of-information",
    "title": "What is Bayesian inference?",
    "section": "Bayesian updating of information",
    "text": "Bayesian updating of information\nBayesian procedures are often equivalent to combining the information in one set of data with another set of data. In fact, if prior beliefs represent the result of a previous data analysis (or perhaps many previous data analyses), then Bayesian analysis is equivalent to pooling information. This is a particularly compelling feature of Bayesian analysis, and one that takes on special significance when working with cojugate priors. In these cases, Bayesian procedures accumulate information in the sense that the posterior distribution is more precise than either the prior distribution or the likelihood alone. Further, as the amount of data increases, say through repeated applications of the data generation process, the posterior precision will continue to increase, eventually overwhelming any non-degenerate prior; the upshot is that analysts with different (non-degenerate) prior beliefs over a parameter will eventually find their beliefs coinciding, provided they (1) see enough data and (2) update their beliefs using Bayes Theorem. In this way Bayesian analysis has been proclaimed as a model for scientific practice acknowledging that while reasonable people may differ (at least prior to seeing data), our views will tend to converge as scientific knowledge accumulates, provided we update our views rationally, consistent with the laws of probability."
  },
  {
    "objectID": "posts/2020-08-07-my-blog-post/index.html#parameters-as-random-variables",
    "href": "posts/2020-08-07-my-blog-post/index.html#parameters-as-random-variables",
    "title": "What is Bayesian inference?",
    "section": "Parameters as random variables",
    "text": "Parameters as random variables\nOne of the critical ways in which Bayesian statistical inference differs from frequentist inference is that the result of a Bayesian analysis, the posterior density \\(p(\\theta \\mid y)\\) is just that, a probability density. Given a subjectivist interpretation of probabilty that most Bayesians adopt, the ‘randomness’ summarized by the posterior density is a reflection of the researcher’s uncertainty over \\(\\theta\\), conditional on having observed data. Contrast the frequentist approach, in which \\(\\theta\\) is not random, but a fixed (but unknown) property of a population from which we randomly sample data \\(\\boldsymbol y\\). Repeated applications of the sampling process, if undertaken, would yield different y, and different sample based estimates of θ, denoted \\(\\hat{\\theta} = \\hat{\\theta}(y)\\), this notation reminding us that estimates of parameters are functions of data. In the frequentist scheme, the \\(\\hat{\\theta}(y)\\) vary randomly across data sets (or would, if repeated sampling was undertaken), while the parameter \\(\\theta\\) is a constant feature of the population from which data sets are drawn. The distribution of values of \\(\\hat{\\theta}(y)\\) that would result from repeated application of the sampling process is called the sampling distribution, and is the basis of inference in the frequentist approach; the standard deviation of the sampling distribution of \\(\\hat{\\theta}\\) is the standard error of \\(\\hat{\\theta}\\), which plays a key role in frequentist inference. The Bayesian approach does not rely on how \\(\\hat{\\theta}\\) might vary over repeated applications of random sampling. Instead, Bayesian procedures center on a simple question: “what should I believe about \\(\\theta\\) in light of the data available for analysis, \\(y\\) ?”\nThe critical point to grasp is that in the Bayesian approach, the roles of \\(\\theta\\) and \\(\\hat{\\theta}\\) are reversed relative to their roles in classical, frequentist inference: \\(\\theta\\) is random, in the sense that the researcher is uncertain about its value, while \\(\\hat{\\theta}\\) is fixed, a feature of the data at hand."
  },
  {
    "objectID": "posts/2020-08-07-my-blog-post/index.html#conclusions",
    "href": "posts/2020-08-07-my-blog-post/index.html#conclusions",
    "title": "What is Bayesian inference?",
    "section": "Conclusions",
    "text": "Conclusions\nSo, we have seen a couple of interesting things about Bayesian statistics which people may not be aware of. First, Bayesian statistics is a scientific approach in that it provides a rational way to update subjective beliefs based on the available evidence through Bayes theorem which conforms the rules of probability. This ensures the scientific credibility of the posterior results while also providing a way to solve the inductive problem of learning from the data and update our belief about a parameter/hypothesis. Second, in contrast to the classical approach, Bayesian statistics do not rely on asymptotic results of a series of repeatable events in order to hold and therefore can be used to answer questions which do not have any meaning in the context of repeated events. Finally, Bayesian statistics sees any unknown quantity (e.g. parameters) as random variables and attach to them a probability distribution expressing the uncertainty around the estimates. Since the entire posterior distribution is derived based on Bayes theorem, this ensures correct propagation of uncertainty from the data and prior and does not require the additional step of classical statistics of deriving uncertainty measures in an “artificial way” or relying on asymptotic results.\nI hope this was a bit interesting for those who would like to get more familiar with the Bayesian philosophy and its underlying implications in terms of statistical assumptions and methods. Of course, being a Bayesian, this is the best way to go for me when doing an analysis and I would love to see more people embracing the Bayesian way as a new way of thinking statistics."
  },
  {
    "objectID": "posts/2020-06-05-my-blog-post/index.html",
    "href": "posts/2020-06-05-my-blog-post/index.html",
    "title": "New tutorials for missingHE",
    "section": "",
    "text": "Nothing major to report for the past month, mostly spent at home still in lockdown. A few offices and shops have already opened in London but all the public stuff, including my office at UCL will remain close until who knows when. So, in the meantime I put some work into prpearing some tutorials about how to use the different functions from my R package missingHE. These are built directly into the package in the form of vignettes which can be easily accessed from the R terminal once the package is installed locally.\nI have worked on three main tutorials dedicated to explain the basic functions of the package and to show how to customise the different models using different combinations of input choices. The three vignettes have each a specific target of users, starting from the beginners in using R to those who would like to have a more flexible specification of the models based on different modelling assumptions. I use the built-in dataset in the package, the MenSS study, to give practical examples of how the different changes to the models may affect the results in a standard analysis.\nThe three tutorials are:\n\nIntroduction to missingHE, which is intended for those who have little familiarity with R and just want an overview of the different functions of the package, what they do and how to extract the relevant information from the fitted models.\nFitting MNAR models in missingHE, which is intended for those who already know about the main functions of the package and would like to explore more deeply how to perform sensitivity analysis to missing not at random assumtpions using the arguments of each function.\nModel Customisation in missingHE, which is intended for those who have already grasped the basic idea behind the different functions and would like to customise their models and not just stick with the default settings. Examples include how to specify random effects, different prior distributions and so on.\n\nI believe these tutorials provide a reasonable summary about the key elements for anyone who would like to use the package but is a bit uncertain about what he or she can actually do with the functions and to which extent customisation is possible. For the moment the vignettes are only available from my GitHub version of the package (1.4.1) and can be accessed by installing the package using the command\n\ndevtools::install_github(\"AnGabrio/missingHE\", build_vignettes = TRUE)\n\nand then typing\n\nutils::browseVignettes(package = \"missingHE\")\n\nNote that you need to locally install the packages devtools and utils to access their functions. As soon as I have a bit of time I will update the version on CRAN to make them available from there as well. I spent a bit of time creating these tutorials and I hope people will find them useful to understand the package. In case anything is still unclear, feel free to contact me to ask questions.\nThat is pretty much it for the moment from me. I should add new tutorials for using JAGS and STAN on the website but time is always never enough. Naaah, I am just very lazy these days."
  },
  {
    "objectID": "posts/2020-04-20-my-blog-post/index.html",
    "href": "posts/2020-04-20-my-blog-post/index.html",
    "title": "New updates for missingHE",
    "section": "",
    "text": "In spite of how incredibly busy I am at the moment, which is also weird considering the whole lockdown situation still going on, I managed to upload a new version (1.4.0) of my R package missingHE with exciting updates!\nFor those who do not know, missingHE is specifically designed to implement Bayesian models for the analysis of trial-based economic evaluations and provides different methods to handle missingness in either or both the effectiveness and cost outcomes. The cool new things in this version are the following:\n\nFirst, random effects can now be specified for each model implemented in missingHE (I know, Bayesians should not talk about “random” or “fixed” effects as we know that there are no real “fixed” effects but the terms have become quite popular and many people would prefer this way). These include selection, hurdle and pattern mixture models. The package allows a flexible implementation of either random intercept only, random slope only and both random intercept and slope models based on the input given by the user. The random effects term is specified via the formula \\(y \\sim x + (x \\mid z)\\) where x is a covariate included also as a fixed effects in the model and z is the clustering variable over which the random effects for x are specified. It is possible to remove the random intercept if desired by adding 0 + inside the brackets (by default this is included).\nSecond, new types of posterior predictive checks can now be chosen using the function ppc for each type of model fitted using the function of the package. These include plotting the Bayesian posterior p-values (which should not be confused with the usual p-values as they are completely different) based on the posterior replications of the models and a given statistics computed from the observed data. The statistic can be provided by the user under the form of a univariate function (e.g. mean or sd) or a specific type of bivariate function (e.g. cor).\nThird, a new generic function called coef has been added which allows to extract the regression coefficients from each type of model, either in terms of fixed effects or random effects (if specified).\n\nI am quite proud of this new update as it is something I considered for a long time which is now available. If even one person find this useful, I think it will be worth all my effort. Very nice.\n\n\n\n\n\nOh, and yes you can also find the new version of missingHE on my GitHub page. I plan to upload a more serious tutorial on how to use all the functions of the package at some point (hopefully not too far from now).\nSo, now that all the fun part is done, I need to go back to doing meetings, reviews, writing papers, etc … It will be a quite busy period again but now I feel motivated. Let’s see for how long this will last."
  },
  {
    "objectID": "posts/2020-03-20-my-blog-post/index.html",
    "href": "posts/2020-03-20-my-blog-post/index.html",
    "title": "Living and working at home is nice, right?",
    "section": "",
    "text": "It has been roughtly a week and a half now since this whole shutdown started here in London and things are not going to be easy in the next few weeks. I am lucky, in that my job allows me to work remotely with limited inconveniences. Other people have to go outside for working and, if not risking thier life, at least put at risk the life of those who they care most. Last week was particularly bad in terms of supermarket products which were sold out for the most part. This week is a bit better as people may have realised that for the moment, if we just buy products as usual, we still have food and toilet paper for everyone.\nTo be honest, not much to update on my work which has slowed down due to this whole situation and also to me not feeling at my best. I hope I will have some time to look at the different projects I am involved with in the next few days. In the meantime, I worked a bit on my website with new JAGS and STAN tutorials and I have also uploaded on my GitHub page some materials (e.g. software code) related to some of the projects I did. For example, here the link to the JAGS and STAN code for the model I used to predict volleyball results\nNot sure what gif to use this time to conclude the post. So I guess I will just go for a random cat picture, which does not make any sense but which is always nice to look at."
  },
  {
    "objectID": "posts/2020-02-10-my-blog-post/index.html",
    "href": "posts/2020-02-10-my-blog-post/index.html",
    "title": "Let us do some work",
    "section": "",
    "text": "The moment for the second edition of the HEART’s one-day introductory course to health economics arrived at last! The course, led by Rachael Hunter and called “Understanding health economics in clinical trials”, took place on Tuesday 11 February and was prepared in collaboration between the HEART group and the Institute of Clinical Trials and Methodology (ICTM). I believe this second edition of the course was a success both in terms of the quality/quantity of the material covered during the six sessions throughout the day, as well as in terms of the positive feedback we received from the participants. Also, this time a new HEART member (Marie) joined the group and very nicely delivered the session about patient reported outcome measures (PROMs), engaging in nice discussions with the audience.\nFinally, a couple of personal notes.\n\nI recently attended a very interesting meeting about missing data methodology which was held by an international group of very talented senior and junior statisticians from different universities, including people like Ian White and James Carpenter from UCL and the LSHTM. It was really an amazing experience to meet so many people working in different stats area but with a common passion about missing data methods (also mine!). From what I understood this series of meetings (called “MiDIA”) have been held since years but do not have a very regular schedule due to people being busy I guess, which makes totally sense. Not sure when the next one will be held but now I am definitely looking forward to the next meetings!\n\n\n\n\n\n\n\nI would also like to highlight a recent tweet from UCL PRIMENT CTU, which advertises a new position as health economist in our HEART group for performing health economics using data from clinical trials. I would encourage anyone interested in some good applied health economic work to apply for this position. Deadline 15 March 2020.\n\n\n\n\n\n\n\nTo conclude, I would also like to say that I have done some updates to this website. From the inclusion of new tutorials on the use of JAGS and STAN on different statistical topics, to a restyle of the website. In particular I had fun by playing around with some Markdown code to add new features, e.g. customised alert notes and emoji, for example. Something like this:\n\n\n\n\n\n\n\n King’s note ! \n\n\n\nI  \\(\\LaTeX\\) very much.\n\n\nThis took me so much time but I am quite satisfied with the result if I may say so. You really never stop learning new things!"
  },
  {
    "objectID": "posts/2020-01-09-my-blog-post/index.html",
    "href": "posts/2020-01-09-my-blog-post/index.html",
    "title": "Let us do some work",
    "section": "",
    "text": "After the terrible start of this year, things are going ok now and I am quite busy with different projects that I left a bit behind. First, I can confirm that me and my colleagues from the HEART group are going to give an introductory course to health economic evaluations next month for different groups of people from academia and clinical trial units. The course has been generally structured based on our “pilot” we gave last year (which went really well by the way) and involves many different topics that will cover the entire day of February 11th. The attending list is already full and thw waiting list is also quite big; happy to see so much interest in economic evaluations.\nSecond, I will give a talk at the PRIMENT statistics and health economics and methodology seminar about an on-going project on missing data in trial-based analysis on Tuesday 28th, at UCL PRIMENT CTU. I am really happy to be back at these seminars which I feel I really nice and where you have the opportunity to interact with people from different backgrounds and job positions who may give some useful feedback on my work. Hopefully, people will find my research interesting!. I would also like to mention the fact that one of my HEART colleague, Marie, will give another talk at the same seminar just before me. Her topic is the economic analysis plan for a trial she has been involved with and I think she is really good, so may worth check her presentaiton out.\nThird, I have finalised a long-waited submission for a paper which has been discussed, written and re-written many times. I really hope we can get some useful feedback on it as I personally worked very hard to keep this work alive. Let see if my efforts have not been in vain and fingers crossed!\n\n\n\n\n\nFourth, as a side note, I have recently bought a new book on missing data called Semiparametric Thoery and Missing Data by Tsiatis, which looks very interesting. To be honest, the book is quite technical with many theoretical concpets and proofs which sometimes I find hard to follow. However, so far it gives a nice introduction to semiparametric models and I look forward to see how it approaches the missing data topic from a non likelihood-based approach. If you are into non/semiparametric statistics and want to find out more about this, I recommend the reading.\nFinally, more work is also coming up in the next weeks and some of this is not going to be very enjoyable, I think. Anyway, let us go through this busy period at our best and see how things will go."
  },
  {
    "objectID": "posts/2019-11-09-my-blog-post/index.html",
    "href": "posts/2019-11-09-my-blog-post/index.html",
    "title": "Too many things, again….",
    "section": "",
    "text": "I did not have much time to post anything this month until now as it has been a quite busy period. I have been involved in many different works and I have also involved other people in what I think could be some very interesting new projects. Not that I complain about having many different things to do (most of them are actually cool) but doing everything in a short period is not the best.\nA couple of things have come/are coming up. First, I have seriously started working on the coding of a decision model for some health economic evaluation project I have been involved in since last year. Everything seems ok after I spent lots of days and time fixing some small bugs in my code. I am about half way through the model and I hope I will be able to finish it before Christmas (I doubt it though).\nSecond, I have finished reviewing an interesting paper about some new methods for improving current practice for dealing with missing data, which I kinda enjoy reading (very good!).\nThird, I would like to quickly summarise my first experience at ISPOR Europe in Copenhagen. I was really excited to attend this conference which, as expected, revealed itself as huge with people coming from all over the world and with many interesting sessions and discussion topics. I had the chance to meet new and old people, such as professor Andrea Manca and the always very kind Chris Sampson for whom I was like a stalker asking for more and more information about himself and his work. I also met some of my old collegues from MapiGroup, now under ICON plc. It was very fun to hang out with these old friends and see what they have been up to during this time. Among them, I gladly caught up with my dear friend Ryan Pulleyblank, now doing a PhD at the University of Southern Denmark. My poster was a success with (unexpectedly) many people stopping by and asking for more information on my work. I was genuinely surprised by this as ISPOR is mostly a conference dedicated to companies rather than academic works and networking. To sum up, it was a very nice and fun experience and despite the level of statistical methodology was not particularly high I enjoyed my time there and I also had the chance to visit Copenhagen for the first time.\nFinally, as a side note, I have found the time to upload on my arXiv page a nice application of Bayesian hierarchical models for the prediction of volleyball matches which I have been working on the past summer, taking inspiration from the work of Gianluca about predicting football macthes. I hope my work can turn out in something cool as well.\n\n\n\n\n\nThis is all for the moment but soon I will be heading back to another quite busy period for me. I hope this will be the last for some time, especially given that Christmas is coming and I would like to have some free time to properly enjoy this period, which I really like, even more than Christmas itself."
  },
  {
    "objectID": "posts/2019-10-01-my-blog-post/index.html",
    "href": "posts/2019-10-01-my-blog-post/index.html",
    "title": "More good news…",
    "section": "",
    "text": "I have got two news coming up. First, the paper I wrote with Michael and Gianluca on Bayesian methods for longitudinal data in trial-based economic evaluations has finally been published as early view on JRSSA. As I said in some earlier posts, I am super happy about this collaboration and I hope I can continue working on similar projects in the future.\nSecond, I will soon give a talk about this work at the ICTMC conference in Brighton, next Monday. This will be the first time at this conference and unfortunately I will only be able to remain around for one day as I need to go back to London pretty soon. I hope I will be able to enjoy my day at the conference, even though I will miss the talks of Baptiste and Alexina which are scheduled for the last day of the conference. I hope I can at least have a quick chat with them the day I am around.\n\n\n\n\n\nI am also excited to visit Brighton, since many people keep telling me that I should go and visit this sort of british version of “Rimini”. To be honest, I do not expect to find a nice weather, given that in this period it is raining a lot in London, but I hope I will be lucky and get the only sunny day of the week.\nFinally, I have started a rubric called missing data on my website, where I try to describe some of the most popular methods to handle missing data and to provide some references for anyone who could be interested in this field. I am really fascinated by statistical methods for dealing with missingness, perhaps because it was the main focus of my PhD, but I am eager to review different methods and see if I can find something really interesting. Of course, to complete this it will take more time, which I hope I will be able to find in the next months."
  },
  {
    "objectID": "posts/2019-09-15-my-blog-post/index.html",
    "href": "posts/2019-09-15-my-blog-post/index.html",
    "title": "Discussing my thesis",
    "section": "",
    "text": "I have been kindly invited by the amazing person Chris Sampson to talk about the work I inlcuded in my PhD thesis for his monthly rubric entitled “Thesis Thursday” on the The Academic Health Economists blog.\nI happily accepted Chris’s invitation as I believe this initiative is really interesting and represents a nice way for newly graduated PhD students to advertise their work while also giving the chance to people interested in health economics to read about some academic work which is typically freely available to everyone.\nHere you can find the full interview, which is not very long and resolves around 5 questions that Chris asked me about my work. I already new this blog but I have never had a proper chance to read through its posts carefully, which is a shame.\n\n\n\n\n\nI shall promise myself to try to check it more often from now on, using this interview as a nice motivation to do so. In fact, there are not many blogs around health economics matters (here a non-comprehensive list), among which The Academic Health Economists and Gianluca’s blog are my favourites.\nI hope I will be able to find some time to write some nice posts about some health economic applications of my work in the next future as this is still the most interesting field for me at the moment."
  },
  {
    "objectID": "posts/2019-07-03-my-blog-post/index.html",
    "href": "posts/2019-07-03-my-blog-post/index.html",
    "title": "HESG Summer Meeting 2019",
    "section": "",
    "text": "I have just come back form my first Health Economists’ Study Group (HESG) meeting, which this year was held at the University of East Anglia in the beautiful city of Norwich, south east of England, and where I presented some preliminary results from one of my on-going works. I have to say, it was a remarkable experience which I really liked thanks to a wonderful and welcoming environment. I had the pleasure to talk to many people from different research areas involved in health economics (both from academia and industry) and to see many different projects and works.\nI particularly enjoy the structure of the meeting, which requires some chair and discussant who have to present and discuss the paper of the authors, who are only allowed to provide some clarification if needed. At first I thought this structure of the sessions was strange, but after attending many sessions and experiencing this for my own paper, I feel that it is a very good way to encourage discussion about works from different people rather than just focussing on your own presentation. Plus, the weather and always sunny, it felt like Italy for a few days.\n\n\n\nThe beautiful Norwich’s cathedral\n\n\nOther nice people and colleagues from HEART and other UCL department came to HESG with me, including Caroline and Ekaterina (aka Katia), you can see them in thumbnail of this post. I was also pleased to meet Baptiste from LSHTM, who shares with me the interest in missing data methods for cost-effectiveness analysis and who presented some very nice work on that. I had the chance to give some feedback to him and he did the same for me. It felt so nice when we started discussing about some aspects of our analyses and after some minutes we simply lost track of time and everyone else disappeared. I also had the opportunity to talk about my work with the discussant of my session, Catrin Plumpton from the Centre for Health Economics and Medicines Evaluation, who gave me some nice feedback which I really appreciated, especially given her mathematical background.\nAn important contribution to the success of the meeting was also given by the wonderful organisation of the event, including an accommodation located very closely to the main building of the meeting, plenty of food provided during each day, a nice bus tour of the city and a wonderful conference dinner. I must thank all the people, who organised the event who were very extremely nice to us and who were always ready to help us for whatever need we had, with a special mention for Emma Mcmanus who was amazing.\n\n\n\n\n\nIn summary, everything was good. Well, almost. Going back to the works presented, as usual, the only less positive note that I would like to make is the almost total absence of Bayesian applications. Some authors mentioned that they used some popular Bayesian program, such as WinBUGS, but this was mainly related to the usual meta-analysis stuff which is pretty standardised. I hope next time I will be able to see more people going Bayesian as this is what I am."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Assistant Professor in Statistics   Department of Methodology and Statistics  Faculty of Health Medicine and Life Sciences  Maastricht University",
    "section": "",
    "text": "I am an assistant professor in Statistics in the Department of methodology and statistics of the Faculty of Health Medicine and Life Sciences at Maastricht University in the Netherlands.\nMy main interests are in Bayesian statistical modelling for cost-effectiveness analysis and decision-making problems in the health systems. During my PhD I have specifically focused on the study and adoption of Bayesian methods to handle missing data in health economic evaluations and to assess the impact of their uncertainty on the output of the decision-making process. My research area involves different topics: from systematic literature reviews, case study applications, survival analysis, meta-analytic methods, multilevel models and trial-based clinical and economic analyses.\n\n\n\nI am very interested in the analysis of longitudinal data, with a focus on different types of statistical methods to deal with missingness. My preferred statistical programming software and the one I am most familiar with is R/RStudio by far, but I do also possess a good knowledge of other software such as STATA and MATLAB. I am quite expert in the use of free open-source Bayesian software programs, such as JAGS and Stan.\nI have collaborated with the Statistics for Health Economic Evaluation research group in the Department of Statistical Science at UCL, which is mainly focused on the development and application of Bayesian methods for health economic evaluations. The group works in collaboration with academics from different institutions and its activities are aimed at providing advice to statisticians, health economists and clinicians working in economic evaluations.\nI have also collaborated with the Health Economics Analysis and Research methodology Team in the Institute for Clinical Trials and Methodology at UCL, working primarily with the members of the Priment Clinical Trials Unit. The group focuses on the development of methodological tools for the analysis of the economic components in randomised control trials across a wide range of clinical areas and is formed by a group of interdisciplinary and varied experience.\n\n\n\n\n\n\n King’s note ! \n\n\n\nI am a huge fan of RStudio and its tools, such as Rmarkdown and blogdown packages and Quarto, which are aimed at the construction of documents that combine text, R code and the output from the execution of that code: from html and pdf files to multi-page web sites and e-books (yes this website is written in Markdown and Quarto!). Oh, and I loves using \\(\\LaTeX\\) !\n\n\n\n\nInterests\n\nMissing Data\nBayesian Statistics\nHealth Economics\nLongitudinal Data\nStatistical Methods for Health and Medical Data\n\n\nEducation\n\n PhD in Statistics, 2019\n\nUniversity College London (UK)\n\n MSc in Statistics and Econometrics, 2015\n\nUniversity of Essex (UK)\n\n MSc in Applied Economics, 2014\n\nUniversity of Pavia (Italy)\n\n BSc in Economics, 2012\n\nUniversity of Pavia (Italy)"
  },
  {
    "objectID": "index.html#biography",
    "href": "index.html#biography",
    "title": "Assistant Professor in Statistics   Department of Methodology and Statistics  Faculty of Health Medicine and Life Sciences  Maastricht University",
    "section": "",
    "text": "I am an assistant professor in Statistics in the Department of methodology and statistics of the Faculty of Health Medicine and Life Sciences at Maastricht University in the Netherlands.\nMy main interests are in Bayesian statistical modelling for cost-effectiveness analysis and decision-making problems in the health systems. During my PhD I have specifically focused on the study and adoption of Bayesian methods to handle missing data in health economic evaluations and to assess the impact of their uncertainty on the output of the decision-making process. My research area involves different topics: from systematic literature reviews, case study applications, survival analysis, meta-analytic methods, multilevel models and trial-based clinical and economic analyses."
  },
  {
    "objectID": "index.html#research-and-work",
    "href": "index.html#research-and-work",
    "title": "Assistant Professor in Statistics   Department of Methodology and Statistics  Faculty of Health Medicine and Life Sciences  Maastricht University",
    "section": "",
    "text": "I am very interested in the analysis of longitudinal data, with a focus on different types of statistical methods to deal with missingness. My preferred statistical programming software and the one I am most familiar with is R/RStudio by far, but I do also possess a good knowledge of other software such as STATA and MATLAB. I am quite expert in the use of free open-source Bayesian software programs, such as JAGS and Stan.\nI have collaborated with the Statistics for Health Economic Evaluation research group in the Department of Statistical Science at UCL, which is mainly focused on the development and application of Bayesian methods for health economic evaluations. The group works in collaboration with academics from different institutions and its activities are aimed at providing advice to statisticians, health economists and clinicians working in economic evaluations.\nI have also collaborated with the Health Economics Analysis and Research methodology Team in the Institute for Clinical Trials and Methodology at UCL, working primarily with the members of the Priment Clinical Trials Unit. The group focuses on the development of methodological tools for the analysis of the economic components in randomised control trials across a wide range of clinical areas and is formed by a group of interdisciplinary and varied experience.\n\n\n\n\n\n\n King’s note ! \n\n\n\nI am a huge fan of RStudio and its tools, such as Rmarkdown and blogdown packages and Quarto, which are aimed at the construction of documents that combine text, R code and the output from the execution of that code: from html and pdf files to multi-page web sites and e-books (yes this website is written in Markdown and Quarto!). Oh, and I loves using \\(\\LaTeX\\) !\n\n\n\n\nInterests\n\nMissing Data\nBayesian Statistics\nHealth Economics\nLongitudinal Data\nStatistical Methods for Health and Medical Data\n\n\nEducation\n\n PhD in Statistics, 2019\n\nUniversity College London (UK)\n\n MSc in Statistics and Econometrics, 2015\n\nUniversity of Essex (UK)\n\n MSc in Applied Economics, 2014\n\nUniversity of Pavia (Italy)\n\n BSc in Economics, 2012\n\nUniversity of Pavia (Italy)"
  },
  {
    "objectID": "posts/2019-08-03-my-blog-post/index.html",
    "href": "posts/2019-08-03-my-blog-post/index.html",
    "title": "The P value fallacy",
    "section": "",
    "text": "Today, I would like to briefly comment an interesting research article written by Goodman, who provided a clear and exemplary discussion about the typical incorrect interpretation of a standard frequentist analysis in the field of medical research. I will now briefly summarise the main argument of the paper and then add some personal comments.\nEssentially, the article describes the characteristics of the dominant school of medical statistics and highlights the logical fallacy at the heart of the typical frequentist analysis in clinical studies. This is based on a deductive inferential approach, which starts with a given hypothesis and makes conclusions under the assumption that the hypothesis is true. This is in contrast with a inductive approach, which uses the observed evidence to evaluate what hypothesis is most tenable. The two most popular methods of the frequentist paradigm are the P value proposed by Fisher and the hypothesis testing developed by Neyman and Pearson.\nThe P value is defined as the probability, under the assumption of no effect (null hypothesis), of obtaining a result equal to or more extreme than what was actually observed. Fisher proposed it as an informal index to be used as a measure of discrepancy between the data and the null hypothesis and therefore should not be interpreted as a formal inferential method. For example, since the P value can only be calculated on the assumption that the null hypothesis is true, it cannot be a direct measure of the probability that the null hypothesis is false. However, the main criticism to the P value is perhaps that it does not take into account the size of the observed effect, i.e. a small effect in a study with a large sample size can have the same P value as a large effect in a small study.\nHypothesis testing was proposed by Neyman and Pearson as an alternative approach to the P value, which assumes the existence of a null hypothesis (e.g. no effect) and an alternative hypothesis (e.g. nonzero effect). The outcome of the test is then simply to reject one hypothesis in favour of the other, solely based on the data. This exposes the researcher to two types of errors: type I error or false-positive (\\(\\alpha\\)) and type II error or false-negative (\\(\\beta\\)) result. Rather than focussing on single experiments, like the P value, hypothesis testing is effectively based on a deductive approach to minimise the errors over a large number of experiments. However, the price to pay to obtain this objectivity is the impossibility to make any inferential statement about a single experiment. The procedure only guarantees that in the long run, i.e. after considering many experiments, we shall not often be wrong.\nOver time a combination between the P value and hypothesis testing was developed under the assumption that the two approaches can be complementary. The idea was that the P value could be used to measure evidence in a single experiment while not violating the long run logic of hypothesis testing. The combined method is characterized by setting \\(\\alpha\\) and power \\(\\beta\\) before the experiment, then calculating a P value and rejecting the null hypothesis if the P value is less than the preset type I error rate. This means that the P value is considered a false-positive error rate specific to the data and also a measure of evidence against the null hypothesis. The P value fallacy is born from this statement, which assumes that an event can be seen simultaneously from a long run perspective (where the observed results are put together with other results that might have occurred in hypothetical repetitions of the experiment) and from a short run perspective (where the observed results are interpreted only with respect to the single experiment). However, these views are not reconcilable since a result cannot be at the same time an interchangeable (long-run) and unique (short-run) member of a group of results.\n\n\n\n\n\nI personally find this discussion fascinating and I believe that it is important to recognise the inconsistencies between the two alternative approaches to inference. The original authors of the two paradigms were well aware of the implications of their methods and never supported the combination of these. However, the combined approach has somehow become widely accepted in practice while its internal inconsistencies and conceptual limitations are hardly recognised.\nI feel that, since the two methods are perceived as “objective”, it is generally accepted that, if combined, they can produce reliable conclusions. This, however, is not necessarily true. Accepting at face value the significance result as a binary indicator of whether or not a relation is real is dangeroues and potentially misleading. This practice wants to show that conclusions are being drawn directly from the data, without any external influence, because direct inference from data to hypothesis is thought to result in mistaken conclusions only rarely and is therefore regarded as “scientific”.\nThis misguided approach has led to a much stronger emphasis towards the quantitative results alone (without any external input). In contrast, I believe that such perspective has the serious drawback of ignoring potentially useful information which is available (e.g. relevant medical knowledge or historical data) and which should be included in the analysis. Of course, I am aware of the potential issues that may arise from the selection and incorporation of external evidence, but I believe this should not be considered as “less reliable” or “more prone to mistakes” compared with the evidence from the available data. It is important that an agreement is reached about the selection of the type of evidence and methods to be used to perform the analysis solely based on their relevance with respect to the context analysed."
  },
  {
    "objectID": "posts/2019-09-25-my-blog-post/index.html",
    "href": "posts/2019-09-25-my-blog-post/index.html",
    "title": "MissingHE 1.2.1",
    "section": "",
    "text": "I have finally found some time to update the version for my R package missingHE, for which version 1.2.1 is now available on CRAN. I included two main features to the previous version of the package.\nFirst, I have added a new type of identifying restriction when fitting pattern mixture models through the function “pattern”. Before, only the complete case restriction was available, which identifies the distributions of the missing data with those from the completers. Now the alternative available case restriction is can also be selected, which relies on the distributions that can be identified among the non-completers to identify the distributions of the missing data. In this way, people can choose among at least two options for the type of restrictions and compare how this choice may affect the final estimates.\nSecond, I added a new accessory function called “ppc”, which allows to perform posterior predictive checks using the conditional parameters saved from the fitted model to generate replications of the data at each posterior iteration of the model. The function implements a relatively large number of checks, mostly taken from the R package bayesplot, which allow to assess the fit of the model to the observed data by type of outcome (effects and costs) and treatment group (control and intervention). For example, overalyed density plots can be generated to compare the empirical and replicated densities of the data to detect possible failures of the model.\n\n\n\nDensity plots for the observed and replicated data\n\n\nI feel this is very important as when fitting a Bayesian model it is crucial to assess whether the model seems to adequately capture the different characteristics of the observed data (e.g. skewness, structural values, etc.). A wide range of predictive checks are available, including histograms (see thumbnail pciture), scatterplots, error intervals, empirical cumulative distribution functions, statistics of interest and many others. In addition, these checks can be performed for each type of missingness model and parametric distribution chosen within missingHE.\n\n\n\n\n\nOf course, it is important to remember that, when dealing with missing data the fit of the model can only be checked with respect to the observed values and therefore this check is only partial since the fit to the unobserved values can never be checked. This is also why it is not meaningful to assess the fit of a model fitted under a missing not at random assumption because this is based on information which is not directly available from the data at hand and thus impossible to check."
  },
  {
    "objectID": "posts/2019-10-28-my-blog-post/index.html",
    "href": "posts/2019-10-28-my-blog-post/index.html",
    "title": "Copenhagen, I am coming …",
    "section": "",
    "text": "Finally the time of ISPOR Europe 2019 has arrived and I will depart in a few days for Copenhagen, where the conference is held this year. I am actually looking forward to this as I am curious to see what type of conference ISPOR is, that is, whether I will be able to find some interesting works and have some “applied statistics”-related discussions or the attention is more placed on “economics and clinical” matters. From what I heard by other people who routinely attend the conference, there should be a bit of both sides, even though I really hope I will be able to see some intersting methods and engage in discussion with some authors.\nI know the conference is mainly related to address the needs of pharmaceutical and consultancy companies, but I hope I will be able to see some familiar faces there. Well, to be honest I know that some people I already know are going, which is good considering that their work is really cool. As for me, I will present the same work that I showed at ICTMC 2019 (some slides available here), but this time in the format of a poster, of which I am kind of very proud in terms of the final output, if I may say so.\nApart from this nice event, there are many things coming up when I will be back from the conference, which I really need to start working on. Mostly, these are related to some routine work for some trial analyses at PRIMENT, which by the way is advertising a new health economist job vacancy for those who might be interested. Other tasks include writing down and code a decision model on which I have been working since ages, papers review, other collaborations with different people, starting my co-supervision for a new PhD student at stats and, after I can find some free time, do some research work on my beloved missing data. Am I ready? not sure about that …"
  },
  {
    "objectID": "posts/2019-12-09-my-blog-post/index.html",
    "href": "posts/2019-12-09-my-blog-post/index.html",
    "title": "Not a very good start…",
    "section": "",
    "text": "After some nice holiday break, I came back to work ready for an exciting 2020 … or so I thought. Unfortunately, I have recently been caught by a terrible flu which forced me to postpone my flight back to London of a week. The worst part is that I was basically a dead corpse moving around with high fever and an awful condition for more than 4 days. It was quite a bad experience which I rarely had in my life. I am just glad I survived this.\n\n\n\n\n\nGoing back to more interesting news. Before my cursed period, I was smart enough to work on different things and I am happy to announce a new update for my missingHE package, which is available both on my GitHub page and on the CRAN repository. Its new version is 1.3.2 and has the nice addition of making available more choices for the parametric distributions that can be selected in all main functions of the package to handle missing data in trial-based economic evaluations. In particular, it is now possible to choose among new probability distributions for the health outcomes, including continuous (Gamma, Weibull, Exponential, Logistic), discrete (Poisson, Negative Binomial) and binary (Bernoulli) distributions. These may be useful when the analysis is not based on utilities scores but some other types of effects, such as survival time, number of events or binary outcomes. I have also included some examples for each type of outcome in the MenSS dataset (available directly once installed the package on your machine) so that people can play around with the new distributions.\nAnother good news is that the last paper written with Michael about missing data handling in economic evaluations will soon be published in the February issue of JRSSA, which will make the final and official version of the article that can be cited, I think.\nFinally, an announcement about the one-day course I am holding together with my mates from the HEART group about an introduction to economic evaluations to people who are not familiar with health economics. The course will take place next month, I believe on Feb 11th, in central London (soon an update about the exact location) and, as the previous edition, I am happy to see that all spots have been taken and everything is sold out (well, to be precise the course is free …). Need to meet up with the others to make the last changes and prepare the slides but I am quite excited about this, given also the good response we got last time.\nNow I am (hopefully) ready to start the new year and there are many things already piling up on my list of things to do in the next days. Let’s try again 2020."
  },
  {
    "objectID": "posts/2020-02-01-my-blog-post/index.html",
    "href": "posts/2020-02-01-my-blog-post/index.html",
    "title": "Finally here …",
    "section": "",
    "text": "The new year is finally taking off for me and I have a couple of updates. First, I would like to remind everyone about the exciting new course “understanding health economics in clinical trials” that me and the rest of our research team HEART have put together to support the dissemination of health economics among all people involved in the design and analysis of clinical trials. I look forward to deliver this one-day short course together with my colleagues from the UCL PCPH department which will be structured into different sessions during the day of Feb 11th at the UCL CCTU - 2nd Floor, 90 High Holborn, London. The course is specifically intended for those who would like to know more about health economics, which has become an important component in the design, analysis and most crucially, for the funding approval of clinical trials. The course will focus on the following aspects:\n\nA short intorduction to the basic concepts of health economics and why these can be relevent to different people\nA review of different types of intruments and tools used to collect health economic data in clinical trials\nA quick look at decision models with some examples\nA summary of the typical results from health economic analyses and how to interpret them\n\nThe course is still in its pilot form and therefore it is free of charge. If there are still places available, you are very welcome to join and give us your feedback!.\n\n\n\n\n\nSecond, I am happy to announce that my recent paper about the use of Bayesian Hierarchical Models for the Prediction of Volleyball Results has finally been published on the Journal of Applied Statistics. I am really proud of this paper as it is my first solo paper published and because I have always been very invested in the general topic of predicting sport results using probability models. To be able to publish something about this based on my own efforts is very rewarding in terms of the (small) contribution to research that I hope I was able to provide.\n\n\n\n\n\nFinally, I have submitted an abstract to the 2020 European Health Economics Association Conference, which this year will be held in Oslo, Norway. I have now to patiently wait for the review of the abstracts and see if my work made it, either as an oral presentation or as a poster. Fingers crossed!."
  },
  {
    "objectID": "posts/2020-03-01-my-blog-post/index.html",
    "href": "posts/2020-03-01-my-blog-post/index.html",
    "title": "Lockdown",
    "section": "",
    "text": "I have to admit, although I expected some fear to spread because of the virus which is currently and quickly infecting the world, I was surprised by the frenzy surrounding us, especially in my homecountry (Italy) and particularly in my parents’ region which is at the moment under lockdown. I will also probably cancel my planned trip home for Eastern and perhaps also after that since the situations is still unclear and I may be unable to come back to the UK in the short time. This is pretty scary to all the people living in those territories, who are now forbidden to have any sort of public meeting and are strongly recommended to stay at home. I am afraid this will not be enough to stop the virus from spreading but of course it is useful as it is the only way we have if we want to contain it. The hope is that by summer time the heath will reduce the ability of the virus to spread and give us some time to come up with a possible vaccine in the next months.\nThere have already been attempts to estimate the fatality ratio of the virus using statistical methods. Here I post the tweet from Andrew Gelman\n\nwhich refers to epidemiologists who tried to use Stan for achieving this objective, although an additional reference to another work based on the use of differential equation analysis is also made. However, results are still preliminary and subject to limitations for the type of data and assumptions used. From a statistical perspective I am sure this new epidemic will be interesting to study and I guess lots of funding will be devoted to analyse the upcoming data to get a better idea of the actual threat it represents for the people. I am not an epidemiologist, so I do not have a big statistical interest in this, although I am pretty much worried as any other person. Hopefully, this will become clearer as time passes and let us just hope the number of deaths will not be very high.\nSorry about talking about this here, but from time to time I would also like to highlight was is currently happening around me. As for my research, nothing has changed much for me at the moment and life as usual continues with another busy upcoming period with lots of boring meetings, reports and standard analyses to do, but hopefully I can also save some time to do some methodological work. I am also waiting for the decision about my abstract which I submitted to the EuHEA conference 2020 and which is supposed to be held in Oslo this July. I really hope I have a chance to presenting my work there as I have never been to this specific health economic conference. Fingers crossed! Of course, nobody knows what will happen from here till July and much is to be discussed also with respect to how the spreading of the virus may affect everyone’s schedule in the next months."
  },
  {
    "objectID": "posts/2020-04-01-my-blog-post/index.html",
    "href": "posts/2020-04-01-my-blog-post/index.html",
    "title": "So much time but also not really",
    "section": "",
    "text": "The lockdown proceeds also here in the UK, as in the rest of the world, and at the moment we have no clear idea how long it will last. Not much we can do apart from staying at home all the time and practicing social distancing. I am still ok with living at home 24/7 but this has affected my productivity, especially in terms of collaborating with other people.\nAlthough I have a lot of time to dedicate to some works, I am making a slow progress and the time of the day seems to fly in an instant with so many things to do. This months is particularly busy as I am trying to submit a revision for a paper which hanted me for quite a lot of time now and which I must finish by the first days of May. I am also working on side projects but these have been slowed down due to the current situation. I hope I can find the time (and the will) to do some more updates to my website by adding more tutorials and similar stuff. I do have some nice ideas about possible projects and collaboration but I need to wait until after this weird period.\nI am also planning to prepare a new version for my R package missingHE to add some nice additional features for post-processing the results of Bayesian models and to implement new types of models. These, however, will take time, which at the moment is one thing that I do have but that I also do not have.\n\n\n\n\n\nAnyway, not much of an update this one, but I hope things will move quicker in the next couple of months or so."
  },
  {
    "objectID": "posts/2020-05-18-my-blog-post/index.html",
    "href": "posts/2020-05-18-my-blog-post/index.html",
    "title": "Sorry, an error occurred",
    "section": "",
    "text": "It has been a while from my last update on this website, but this has been an incredibly busy period with lots of routine work that I had to do. Now the situation has clamed down a bit, and I have also some news to report. So, here I am finally.\nIn the past few days, I had an extremely interesting email correspondence with one guy (don’t want to say the name for privacy) interested in using my R package missingHE to do some trial-based CEA. Awesome, I thought. He was looking for some advice for how to customise some models in the package and how to get the results he wanted. Unfortunately for my pride, but fortunately for my package, we discovered a small bug in the code when trying to specify a hurdle model (two-part model) to handle the zero costs when not including any covariate inside the logit model to estimate the probability of having a zero cost. Essentially, under these specific circumstances, the function did not correctly backtransformed the estimate of the mean costs on the appropriate scale and the results provided were incorrect. Sorry about that!\n\n\n\n\n\nTo be honest, you can get away by simply including some baseline covariate into the logit model for the structural zero costs, in which case the estimates produced by the function are correct. I have immediately updated the package version to correct this bug on my GitHub page, where you can find the most up to date version (1.4.1). The version on CRAN will be updated at the next iteration as I have recently uploaded the 1.4.0 version in May. In the meantime, if you want to avoid having that issue above, you can download and install the updated version from my GitHub page.\nI think this is also a good chance to tell that I have updated an old paper on my on my Arxiv account. Both the content and title of the paper have changed considerably, but overall I feel that the overall message and quality of the article has improved. It is still an on-going version, but I am quite satisfied with its current status given all the effort I put into it. Have a lool in case you are interested. New title : “Joint longitudinal models for dealing with missing at random data in trial-based economic evaluations”.\nI believe that is all for this update. Not much going on due to the whole lockdown situation here in the UK but hopefully things are improving a little in a two-months time we will be able to at least go to the office. Let’s see."
  },
  {
    "objectID": "posts/2020-07-07-my-blog-post/index.html",
    "href": "posts/2020-07-07-my-blog-post/index.html",
    "title": "Why be Bayesian",
    "section": "",
    "text": "Many times I have been asked by co-workers and people around me who are a bit familiar with statistics why I choose to be Bayesian and whether I feel confident in using this approach for my data analysis rather than the most widely accepted frequentist methods, at least in my research area. Well, I am sure there are many valid arguments I could use to reply to this question but if I have to summarise my answer in two words I would say: why not?\nNow, a bit more into the details for those who were not extremely annoyed by my previous sentence. So, I truly believe that the Bayesian approach can be considered as a complement rather than a substitute to the frequentist paradigm. The main reason is relate to its much stronger links with probability theory compared with the classical approach in that not only are sampling distributions required for summaries of data, but also a wide range of distributions are used to represent prior opinion about proportions, event rates, and other unknown quantities. In a nutshell, the key difference between the two approaches is how they confront the concept of probability of a certain event. In fact, although there is general consensus about the rules of probability, that there is no universal concept of probability, and two quite different definitions come from the frequentist and Bayesian approach:\nRather than debating on philosophical debates about the foundations of statistics I prefer to focus on those aspects which I believe make the Bayesian approach, if not more intuitive than the frequentist counterpart, at least more attractive. Be worn I am not trying to start a war as I think both approaches could be used without the need to completely discard the other. The simple fact of being able to choose between two methods, rather than restricting themselves to a single option, seems a good enough reason for me to advocate the use of both approaches. I terms of your own knowledge, experience and skills, You do not gain anything by saying “I will never be Bayesian” or “I will never be a frequentist”. On the contrary, by opening your mind and explore the use of one or the other method you will be able to have more options at your disposal that you can use to tackle the different problems you will face in your analyses.\nFor the purpose of this post I just want to highlight some aspects which make the Bayesian approach particularly useful and, in some cases, even arguably preferable than the frequentist approach. Note that I am well aware there could be cases where the opposite holds and this is precisely why I believe it is important that statisticians should become familiar with both methods. By doing so they will be able to overcome the limitations/concerns associated with one method for a specific problem at hand using the instruments made available from the other method. Since I am a Bayesian, here I want to report the reasons and situations in which the Bayesian approach could provide a powerful tool.\nLet us start with a quick recap of the basic principle behind Bayesian methods. Bayesian statistical analysis relies on Bayes’s Theorem, which tells us how to update prior beliefs about parameters and hypotheses in light of data, to yield posterior beliefs. The theorem itself is utterly uncontroversial and follows directly from the conventional definition of conditional probability. If \\(\\theta\\) is some object of interest, but subject to uncertainty, e.g. a parameter, a hypothesis, a model, a data point, then Bayes Theorem tells us how to rationally revise prior beliefs about \\(\\theta\\), \\(p(\\theta)\\), in light of the data \\(y\\), to yield posterior beliefs \\(p(y \\mid \\theta)\\). In this way Bayes Theorem provides a solution to the general problem of induction, while in the specific case of statistical inference, Bayes Theorem provides a solution to problem of how to learn from data. Thus, in a general sense, Bayesian statistical analysis is remarkably simple and even elegant, relying on this same simple recipe in each and every application.\nAs I see it, there are a few major reasons why statisticians should consider learning about the Bayesian approach to statistical inference, and in the social sciences in particular:\nThe result of a Bayesian analysis is a posterior probability statement, ‘posterior’ in the literal sense, in that such a statement characterizes beliefs after looking at data. Examples include: the posterior probability that a regression coefficient is positive, negative or lies in a particular interval; the posterior probability that a subject belongs to a particular latent class; the posterior probabilities that a particular statistical model is true model among a family of statistical models.\nNote that the posterior probability statements produced by a Bayesian analysis are probability statements over the quantities or objects of direct substantive interest to the researcher (e.g. parameters, hypotheses, models, predictions from models). Bayesian procedures condition on the data at hand to produce posterior probability statements about parameters and hypotheses. Frequentist procedures do just the reverse: one conditions on a null hypothesis to assess the plausibility of the data one observes (and more ‘extreme’ data sets that one did not observe but we might have had we done additional sampling), with another step of reasoning required to either reject or fail to reject the null hypothesis. Thus, compared to frequentist procedures, Bayesian procedures are simple and straightforward, at least conceptually.\nThe prior density also provides a way for model expansion when we work with data sets that pool data over multiple units and/or time periods. Data sets of this sort abound in the social sciences. Individuals live in different locations, with environmental factors that are constant for anyone within that location, but vary across locations. key question in research of this type is how the causal structure that operates at one level of analysis (e.g. individuals) varies across a ‘higher’ level of analysis (e.g. localities or time periods). The Bayesian approach to statistical inference is extremely well-suited to answering this question. Recall that in the Bayesian approach parameters are always random variables, typically (and most basically) in the sense that the researcher is unsure as to their value, but can characterize that uncertainty in the form of a prior density \\(p(\\theta)\\). We can replace the prior with a stochastic model formalizing the researcher’s assumptions about the way that parameters \\(\\theta\\) might vary across groups \\(j = 1,..., J\\) , perhaps as a function of observable characteristics of the groups; e.g., \\(\\theta_j \\sim f (z_j, \\gamma )\\), where now \\(\\gamma\\) is a set of unknown hyperparameters. That is, the model is now comprised of a nested hierarchy of stochastic relations: the data from unit \\(j\\), \\(y_j\\), are modeled as a function of covariates and parameters \\(\\theta_j\\) , while cross-unit heterogeneity in the \\(\\theta_j\\) is modeled as function of unit-specific covariates \\(z_j\\) and hyperparameters \\(\\gamma\\). Models of this sort are known to Bayesians as hierarchical models, but go by many different names in different parts of the social sciences depending on the specific form of the model and the estimation strategy being used (e.g. ‘random’ or ‘varying’ coefficients models, ‘multilevel’ or ‘mixed’ models). Compared with the frequentist counterpart, thanks to the use of Markov chain Monte Carlo (MCMC) methods, Bayesian computation for these models has also become rather simple. Indeed, MCMC algorithms have proven themselves amazingly powerful and flexible, and have brought wide classes of models and data sets out of the ‘too hard’ basket. Other modelling examples include data sets with lots of missing data, or models with lots of parameters, model with latent variables, mixture models, and flexible semi-and non-parametric models.\nFrequentist inference asks assuming hypothesis \\(H_0\\) is true, how often would we obtain a result at least as extreme as the result actually obtained?’, where ‘extreme’ is relative to the hypothesis being tested. If results such as the one obtained are sufficiently rare under hypothesis \\(H_0\\) (e.g. generate a sufficiently small p value), then we conclude that \\(H_0\\) is incorrect, rejecting it in favor of some alternative hypothesis. Indeed, we teach our students to say that when the preceding conditions hold, we have a statistically significant result. My experience is that in substituting this phrase for the much longer textbook definition, people quickly forget the frequentist underpinnings of what it is they are really asserting, and, hence seldom question whether the appeal to the long-run, repeated sampling properties of a statistical procedure is logical or realistic. In the Bayesian approach we condition on the data at hand to assess the plausibility of a hypothesis (via Bayes Rule), while the frequentist approach conditions on a hypothesis to assess the plausibility of the data (or more extreme data sets), with another step of reasoning required to either reject or fail to reject hypotheses. The frequentist p-value is the relative frequency of obtaining a result at least as extreme as the result actually obtained, assuming hypothesis \\(H_0\\) to be true, where the sampling distribution of the result tells us how to assess relative frequencies of possible different results, under \\(H_0\\). But what about cases where repeated sampling makes no sense, even as a thought experiment?\nRecall that in the frequentist approach, parameters are fixed characteristics of populations, so \\(\\mu\\) either lies in the interval or it doesn’t. The correct interpretation of a frequentist confidence interval concerns the repeated sampling characteristics of a sample statistic. In the case of a \\(95\\%\\) confidence interval, the correct frequentist interpretation is that \\(95\\%\\) of the \\(95\\%\\) confidence intervals one would draw in repeated samples will include \\(\\mu\\). Now, is the \\(95\\%\\) confidence interval that one constructs from the data set at hand one of the lucky \\(95\\%\\) that actually contains \\(\\mu\\), or not? No ones knows.\nFinally, aside from acknowledging the subjectivity inherent to the general scientific exercise, the Bayesian approach rests on a subjective notion of probability, but demands that subjective beliefs conform to the laws of probability. Put differently, in the Bayesian approach, the subjectivity of scientists is acknowledged, but simultaneously insists that subjectivity be rational, in the sense that when confronted with evidence, subjective beliefs are updated rationally, in accord with the axioms of probability. Again, it is in this sense that Bayesian procedures offer a more direct path to inference; as I put it earlier, the Bayesian approach lets researchers mean what they say and say what they mean. For instance, the statement, having looked at the data, I am \\(95\\%\\) sure that \\(\\mu\\) is included in an interval is a natural product of a Bayesian analysis, a characterization of the researcher’s beliefs about a parameter in formal, probabilistic terms, rather than a statement about the repeated sampling properties of a statistical procedure."
  },
  {
    "objectID": "posts/2020-07-07-my-blog-post/index.html#conclusions",
    "href": "posts/2020-07-07-my-blog-post/index.html#conclusions",
    "title": "Why be Bayesian",
    "section": "Conclusions",
    "text": "Conclusions\nThe mathematics and computation underlying Bayesian analysis has been dramatically simplified via a suite of MCMC algorithms. The combination of the popularization of MCMC and vast increases in the computing power available to social scientists means that Bayesian analysis is now well and truly part of the mainstream of quantitative social science. Despite these important pragmatic reasons for adopting the Bayesian approach, it is important to remember that MCMC algorithms are Bayesian algorithms: they are tools that simplify the computation of posterior densities. So, before we can fully and sensibly exploit the power of MCMC algorithms, it is important that we understand the foundations of Bayesian inference.\nThis time I went overboard with the discussion but I thought it could be interesting to clarify here the key points, in my opinion, which make the Bayesian approach not only valid and efficient, but even a powerful tool that, once grasped the underlying philosophy, can be used to overcome the difficulties of standard methods, especially when dealing with complex analyses.\nSo what are you waiting for? do not sit in your frequentist comfort zone but expand your statistical knowledge! Evolve!"
  },
  {
    "objectID": "posts/2020-09-10-my-blog-post/index.html",
    "href": "posts/2020-09-10-my-blog-post/index.html",
    "title": "Starting a new adventure!",
    "section": "",
    "text": "Hello dear readers, I have some exciting news about myself and my future which I am eager to communicate on this blog. I know that it is not exactly the most interesting news for everybody but I have recently joined a new research team in the Department of methodology and statistics at Maastricht University, in the Netherlands.\nI must say, it was not easy for me to leave UCL, where I have studied and obtained my PhD degree and spent quite a few years of my life. I am glad that during this time I met so many fantastic people and colleagues from whom I have leared so much and which I would like to thank from the bottom of my heart. I must say thank you to all my previous PhD supervisors, namely Gianluca Baio, Alexina J. Mason and Rachael Hunter, who were incredibly supportive throughout my entire PhD and with whom I also collaborated during my experience as a research fellow at the department of statistical sciences and PCPH. Particularly during my first post-doc position, I came to know so many colleagues and share so many ideas, which became an essential part of my academic activity and personal growth. Among others, special thanks are also due to my colleagues and friends from the HEART team at PCPH with whom I am still collaborating on some interesting research projects.\nAlthough both my personal and working experience at UCL were amazing, after almost 4 years of PhD and 2 years as a reasearch fellow, I felt that it was time for a change in my life. London can be a quite stressfull city to live in and I wanted to see if I could take some new opportunity to enhance my personal growth by taking a new perspective. This is why I decided to accept a new position as assistant professor in statistics at UM, which I have officially joined a few days ago. During my interview for the position, I was really intrigued by the prospect of this job in terms of both increasing my teaching experience at the university level as well as obtaining more independence in regard to the research topics I would like to explore. Don’t get me wrong, I will still work on stats methods for CEA as this will still be the main focus of my research activity for the next years to come. I love it so much! However, it will also be exciting to see how I can use the experience and knowledge acquired at UCL to contribute (and hopefully improve) the current approach in CEA in the Netherlands.\nI already know that, especially at the beginning, it will not be easy. From studying a new language (yes it something that I would like to do!), to getting acquainted with the new place and rules, the new teaching duties, new colleagues, etc… There is much to learn and to do and I am really excited to see where this new adventure will lead me in a few months/years from now. In the meantime I have to do my best to adapt this big change in my life. My new colleagues at the stats department have been very nice to welcome me during these difficult times where interaction with people must be limited to avoid the spread of the virus. I hope this situation will change in the next years and that we can gradually go back to “normality”, if you want to call it that way.\nTo conclude, I would still thank all the people who have supported me during these exciting but also quite challenging times, first among all my parents to whom my unconditional love goes. I will try to post my future updates in a regular way on my website but, especially at the beginning, I will probably need some time to adapt to the new changes and I will be quite busy. I still hope to continue my on-going collaborations with all my previous colleagues from UCL and the UK while also being able to meet new people and start new relationships here in Maastricht and the Netherlands.\n\n\n\n\n\nThank you and see you soon !\nDank u en tot ziens!"
  },
  {
    "objectID": "posts/2020-11-05-my-blog-post/index.html",
    "href": "posts/2020-11-05-my-blog-post/index.html",
    "title": "A couple of updates",
    "section": "",
    "text": "Finally some exciting updates! I really need some good news after all that happened this year. So, first of all I have recently found out that one of the paper I co-authored got published early this year but I actually forgot to check it. The work is an interesting long-term CEA for a new drug in prostate cancer patients, extrapolating the data from an RCT named STAMPEDE. I was only partially involved with this work, which has been mostly done by the talented and always cheerful Caroline Clarke with whom I collaborated during my post-doc experience at UCL. My contribution mostly resolved about double checking the R code for the model which, I have to say, was pretty sophisticated and not always clear given that was originally done by someone else (I we know how understanding someone else’s code can be an hard task). Overall, I am really happy for this paper which I hope may be of interest to someone specialised in that type of analysis and population.\nNext, I am also proud of having contributed to the third round for the course “Understanding Health Economics in Clinical Trials”, which I delivered together with my ex-colleagues and co-members of the HEART team from UCL. This is the third time we have delivered the course in the past two years and I was really impressed by how far we have come since our first attempt. The structure of the course is now very nice and most of the people attending out last edition (in online format of course, thanks 2020!) provided very nice feedback. This, I believe, was the last time we offered the course for free and from the next time we will start charging a small fee to the participants. We have some minor adjustments to make in a couple of sessions, but overall I feel pretty good about it. I would like to thank all my HEART buddies, with a special mention to Ekaterina Bordea, with whom I share the delivery of the final session of the course. Although we were a bit strict on time, and I ended up taking most of it (sorry Ekaterina!), we both got very positive comments from the attendants. I hope I will still be involved with these guys for the future editions of the course as it was really fun.\nAslo, a quick update about my acadmic work. I have written an abstract for a paper together with some very nice people involved in missing data analysis in CEA, including the amazing Baptiste Leurent and Catrin Plumpton. We submitted the abstract to HESG 2021 which has been accepted as a presentation. We still need to write up the actual paper which we need to submit by the end of this month but I am confident we will make it. This work is again about missing data in CEA but based on a different perspective compared with the standard imputation approach and instead explores the use of mixed models as a possible alternative under some assumptions. I really enjoyed working on this, especially with Baptiste with whom I started working on this quite a few months ago. I am not sure I will be able to be present at HESG as I have some heavy teaching duties in Maastircht in that period but I look forward receiving some feedback for our idea.\nI have also found some time to upload on ArXiv a drafted version for a paper I strated working on about one year ago at UCL. The work is a sort of experiment for me where I tried to apply some Bayesian methods for jointly modelling patient-level partitioned survival cost-utility data. The idea is pretty nice, I think, but I will probably need to polish off a few things before finalising the paper. I was also excited to implement something in STAN which I have started using more frequently in the last months. Although in my experience other Bayesian software seem to be pretty good for different analyses, I think STAN has a great potential in the future, especially thanks to all the support and community posting online solutions and code for different types of problems and analyses.\nFinally, I have been involved in some stats teaching for a master course in epidemiology at my affiliated faculty FHML at UM. Everyting was done online, but overall I was happy about how I delivered my sessions of the course and with the feedback I received from the sudents. This was my first teaching experience at UM and, considering the special circumstances most of us are in this year, I think I managed pretty good. Next month I will involved in the marking of the exams for this course while from Jan 2021 I will be quite busy with lots of courses and a quite frightening time schedule. Good luck to me!\n\n\n\n\n\nSo, lots of news but this is also becuase I did not find much time in the past weeks to update my blog. I hope to be more consistent in the future but you never know, especially this year. One thing I am looking for is to explore and visit Maastricht for which I haven’t had a chance since I started my contract last September. Everyone says that the Christmas market is particularly beautiful but this year there won’t be one because of \\(\\ldots\\) well 2020!"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "It is Xmas again Yeah\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\n\n\n\n\n\n\n\nDec 20, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nA couple of updates\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nHealth Economics\n\n\npublication\n\n\n\n\n\n\n\n\n\nNov 5, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nWhy health economists do not care about statistical significance?\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nHealth Economics\n\n\n\n\n\n\n\n\n\nOct 10, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nStarting a new adventure!\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\n\n\n\n\n\n\n\nSep 10, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Bayesian inference?\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nAug 7, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nWhy be Bayesian\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nJul 7, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nNew tutorials for missingHE\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nmissingHE\n\n\n\n\n\n\n\n\n\nJun 5, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nSorry, an error occurred\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nmissingHE\n\n\n\n\n\n\n\n\n\nMay 18, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nNew updates for missingHE\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nmissingHE\n\n\n\n\n\n\n\n\n\nApr 20, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nSo much time but also not really\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nmissingHE\n\n\n\n\n\n\n\n\n\nApr 1, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nLiving and working at home is nice, right?\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\n\n\n\n\n\n\n\nMar 20, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nLockdown\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nconference\n\n\n\n\n\n\n\n\n\nMar 1, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nLet us do some work\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nworkshop\n\n\n\n\n\n\n\n\n\nFeb 10, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nFinally here …\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nworkshop\n\n\nconference\n\n\npublication\n\n\n\n\n\n\n\n\n\nFeb 1, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nLet us do some work\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nworkshop\n\n\n\n\n\n\n\n\n\nJan 9, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nNot a very good start…\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nmissingHE\n\n\n\n\n\n\n\n\n\nDec 9, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nToo many things, again….\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nconference\n\n\n\n\n\n\n\n\n\nNov 9, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nCopenhagen, I am coming …\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nconference\n\n\n\n\n\n\n\n\n\nOct 28, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nMore good news…\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nconference\n\n\n\n\n\n\n\n\n\nOct 1, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nMissingHE 1.2.1\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nmissingHE\n\n\n\n\n\n\n\n\n\nSep 25, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussing my thesis\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\ninterview\n\n\n\n\n\n\n\n\n\nSep 15, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nThe P value fallacy\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nAug 3, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nHESG Summer Meeting 2019\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nHealth Economics\n\n\n\n\n\n\n\n\n\nJul 3, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/bookHTA/bookHTA.html",
    "href": "research/bookHTA/bookHTA.html",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "Introduction\nThe type of data used in economic evaluations typically come from a range of sources, whose evidence is combined to inform HTA decision-making. Traditionally, relative effectiveness data are derived from randomised controlled clinical trials (RCTs), while healthcare resource utilisation, costs and preference-based quality of life data may come from the same study that estimated the clinical effectiveness or not. A number of HTA agencies have developed their own methodological guidelines to support the generation of the evidence required to inform their decisions. In this context, the primary role of economic evaluation for HTA is not the estimation of the quantities of interest (e.g. the computation of point or interval estimation, or hypothesis testing), but to aid decision making. The implication of this is that the standard frequentist analyses that rely on power calculations and \\(P\\)-values to estimate statistical and clinical significance, typically used in RCTs, are not well-suited for addressing these HTA requirements.\nIt has been argued that, to be consistent with its intended role in HTA, economic evaluation should embrace a decision-theoretic paradigm and develop ideally within a Bayesian statistical framework to inform two decisions\n\nwhether the treatments under evaluation are cost-effective given the available evidence and\nwhether the level of uncertainty surrounding the decision is acceptable (i.e. the potential benefits are worth the costs of making the wrong decision).\n\nThis corresponds to quantify the impact of the uncertainty in the evidence on the entire decision-making process (e.g. to what extent the uncertainty in the estimation of the effectiveness of a new intervention affects the decision about whether it is paid for by the public provider).\n\n\nBayesian methods in HTA\nThere are several reasons that make the use of Bayesian methods in economic evaluations particularly appealing. First, Bayesian modelling is naturally embedded in the wider scheme of decision theory; by taking a probabilistic approach, based on decision rules and available information, it is possible to explicitly account for relevant sources of uncertainty in the decision process and obtain an optimal course of action. Second, Bayesian methods allow extreme flexibility in modelling using computational algorithms such as Markov Chain Monte Carlo (MCMC) methods; this allows to handle in a relatively easy way the generally sophisticated structure of the relationships and complexities that characterise effectiveness, quality of life and cost data. Third, through the use of prior distributions, the Bayesian approach naturally allows the incorporation of evidence from different sources in the analysis (e.g. expert opinion or multiple studies), which may improve the estimation of the quantities of interest; the process is generally referred to as evidence synthesis and finds its most common application in the use of meta-analytic tools. This may be extremely important when, as it often happens, there is only some partial (imperfect) information to identify the model parameters. In this case analysts are required to develop chain-of-evidence models. When required by the limitations in the evidence base, subjective prior distributions can be specified based on the synthesis and elicitation of expert opinion to identify the model, and their impact on the results can be assessed by presenting or combining the results across a range of plausible alternatives. Finally, under a Bayesian approach, it is straightforward to conduct sensitivity analysis to properly account for the impact of uncertainty in all inputs of the decision process; this is a required component in the approval or reimbursement of a new intervention for many decision-making bodies, such as NICE in the UK.\nThe general process of conducting a Bayesian analysis (with a view of using the results of the model to perform an economic evaluation) can be broken down in several steps, which are graphically summarized in Figure 1.\n\n\n\n\n\n\nFigure 1: Diagram representation of the process for health economic evaluation.\n\n\n\nThe starting point is the identification of the decision problem, which defines the objective of the economic evaluation (e.g. the interventions being compared, the target population, the relevant time horizon). In line with the decision problem, a statistical model is constructed to describe the (by necessity, limited) knowledge of the underlying clinical pathways. This implies, for example, the definition of suitable models to describe variability in potentially observed data (e.g. the number of patients recovering from the disease because of a given treatment), as well as the epistemic uncertainty in the population parameters (e.g. the underlying probability that a random individual in the target population is cured, if given the treatment under study). At this point, all the relevant data are identified, collected and quantitatively sytnthesised to derive the estimates of the input parameters of interest for the model.\nThese parameter estimates (and associated uncertainties) are then fed to the economic model, with the objective of obtaining some relevant summaries indicating the benefits and costs for each intervention under evaluation. Uncertainty analysis represents some sort of detour from the straight path going from the statistical model to the decision analysis: if the output of the statistical model allowed us to know with perfect certainty the true value of the model parameters, then it would be possible to simply run the decision analysis and make the decision. Of course, even if the statistical model were the true representation of the underlying data generating process (which it most certainly is not), because the data may be limited in terms of length of follow up, or sample size, the uncertainty in the value of the model parameters would still remain. This parameter (and structural) uncertainty is propagated throughout the whole process to evaluate its impact on the decision-making. In some cases, although there might be substantial uncertainty in the model inputs, this may not turn out to modify substantially the output of the decision analysis, i.e. the new treatment would be deemed as optimal irrespectively. In other cases, however, even a small amount of uncertainty in the inputs could be associated with very serious consequences. In such circumstances, the decision-maker may conclude that the available evidence is not sufficient to decide on which intervention to select and require more information before a decision can be made.\nThe results of the above analysis can be used to inform policy makers about two related decisions:\n\nwhether the new intervention is to be considered (on average) value for money, given the evidence base available at the time of decision, and\nwhether the consequences (in terms of net health loss) of making the wrong decision would warrant further research to reduce this decision uncertaint.\n\nWhile the type and specification of the statistical and economic models vary with the nature of the underlying data (e.g. individual (ILD) level versus aggregated (ALD) data, the decision and uncertainty analyses have a more standardised set up.\n\n\nConclusions\nHTA has been slow to adopt Bayesian methods; this could be due to a reluctance to use prior opinions, unfamiliarity, mathematical complexity, lack of software, or conservatism of the healthcare establishment and, in particular, the regulatory authorities. However, the use of Bayesian approach has been increasingly advocated as an efficient tool to integrate statistical evidence synthesis and parameter estimation with probabilistic decision analysis in an unified framework for HTA. This enables a transparent evidence-based decision modelling, reflecting the uncertainty and the structural relationships in all the available data.\nWith respect to trial-based analyses, the flexibility and modularity of the Bayesian modelling structure are well-suited to jointly account for the typical complexities that affect ILD. In addition, prior distributions can be used as convenient means to incorporate external information into the model when the evidence from the data is limited or absent (e.g. for missing values). In the context of evidence synthesis, the Bayesian approach is particularly appealing in that it allows for all the uncertainty and correlation induced by the often heterogeneous nature of the evidence (either ALD only or both ALD and ILD) to be synthesised in a way that can be easily integrated within a decision modelling framework.\nThe availability and spread of Bayesian software among practitioners since the late 1990s, such as OpenBUGS or JAGS, has greatly improved the applicability and reduced the computational costs of these models. Thus, analysts are provided with a powerful framework, which has been termed comprehensive decision modelling, for simultaneously estimating posterior distributions for parameters based on specified prior knowledge and data evidence, and for translating this into the ultimate measures used in the decision analysis to inform cost-effectiveness conclusions."
  },
  {
    "objectID": "research/jointHTA/jointHTA.html",
    "href": "research/jointHTA/jointHTA.html",
    "title": "Joint Longitudinal Models for Dealing With Missing at Random Data in Trial-Based Economic Evaluations",
    "section": "",
    "text": "Introduction\nIn trial-based economic evaluation, some individuals are typically associated with missing data at some time point, so that their corresponding aggregated outcomes (e.g. quality-adjusted life-years) cannot be evaluated. Restricting the analysis to the complete cases is inefficient and can result in biased estimates, while imputation methods are often implemented under a missing at random (MAR) assumption. We propose the use of joint longitudinal models to extend standard approaches by taking into account the longitudinal structure to improve the estimation of the targeted quantities under MAR.\n\n\nStandard approach in trial-based CEA\nAccording to recent reviews, standard practice in trial-based CEAs handles missingness at the level of the aggregated outcomes and baseline variables. Indeed, estimates of interest are obtained by directly modeling the aggregated outcomes rather than the utility and cost data at each time. This requires the analyst to process the data collected on individual \\(i\\) at time \\(j\\) in treatment \\(t\\), to derive the aggregated measures over the study duration.\nFigure 1 shows a typical data set of trial-based CEA, formed by the sets of utility and cost variables collected at baseline \\(j = 0\\) and some follow-ups \\(j = 1,\\ldots,J\\). The graph represents the standard procedure for processing the data and identifying the variables used in the analysis.\n\n\n\n\n\n\nFigure 1: Schematic representation of the standard procedure for processing trial-based CEA data\n\n\n\nA general limitation of any aggregated method is to ignore the longitudinal nature of the data and discard all follow-up values for partially observed individuals. Conversely, methods that handle missingness at each time point account for the longitudinal structure, incorporate all available evidence, and potentially make the missingness assumptions (e.g. missing at random or MAR) more reasonable.\n\n\nMethods\nWe propose the use of joint longitudinal models to extend standard approaches by taking into account the longitudinal structure to improve the estimation of the targeted quantities under MAR. We compare the results from methods that handle missingness at an aggregated (case deletion, baseline imputation, and joint aggregated models) and disaggregated (joint longitudinal models) level under MAR. The methods are compared using a simulation study and applied to data from 2 real case studies.\n\n\nConclusions\nJoint longitudinal models provide an alternative and potentially less biased approach for handling missing data with respect to current practice under a missing at random assumption. Methods that ignore some of the available information may be associated with biased results and mislead the decision-making process. This is a potentially serious issue for those who use these evaluations in their decision making, thus possibly leading to incorrect policy decisions about the cost-effectiveness of new treatment options."
  },
  {
    "objectID": "research/missingHE/missingHE.html",
    "href": "research/missingHE/missingHE.html",
    "title": "missingHE",
    "section": "",
    "text": "missingHE is a R package, available on CRAN which is aimed at providing some useful tools to analysts in order to handle missing outcome data under a full Bayesian framework in economic evaluations. The package relies on the R package R2jags to implement Bayesian methods via the statistical software JAGS to obtain inferences using Markov Chain Monte Carlo (MCMC) methods. Different types of missing data models are implemented in the package, including selection models, pattern mixture models and hurdle models. A range of parametric distributions can be specified when modelling the typical outcomes in an trial-based economic evaluations, namely the effectiveness and cost variables, while simultaneously incorporating different assumptions about the missingness mechanism, which allows to easily perform sensitivity analysis to a range of alternative missing data assumptions according to the modelling choices selected by the user.\nmissingHE also provides functions, taken and adapted from other R packages, to assess the results of each type of model, including summaries of the posterior distributions of each model parameter, range and imputations of the missing values, different types of model diagnostics to assess convergence of the algorithm, posterior predictive checks, model assessment measures based on the fit to the observed data, and a general summary of the economic evaluations, including the results from probabilistic sensitivity analyses which are automatically performed within a Bayesian modelling framework.\nFor example, the function plot can produce graphs, such as those shown in Figure 1, which compare the observed and imputed values for both cost and benefit measures in each treatment group to detect possible concerns about the plausibility of the imputations.\n\n\n\n\n\n\nFigure 1: Plot of observed (black dots) and imputed (red dots and lines) effectiveness and cost data by treatment group.\n\n\n\nMore information, including new updates, about missingHE can be found on my dedicated GitHub repository or via the most up to date version of the package on CRAN."
  },
  {
    "objectID": "research/partsurvHTA/partsurvHTA.html",
    "href": "research/partsurvHTA/partsurvHTA.html",
    "title": "A Bayesian Framework for Patient-Level Partitioned Survival Cost-Utility Analysis",
    "section": "",
    "text": "Modelling Framework\nwe extend the current methods for modelling trial-based partitioned survival cost-utility data, taking advantage of the flexibility of the Bayesian approach, and specify a joint probabilistic model for the health economic outcomes. We propose a general framework that is able to account for the multiple types of complexities affecting individual level data (correlation, missingness, skewness and structural values), while also explicitly modelling the dependence relationships between different types of quality of life and cost components.\nConsider a clinical trial in which patient-level information on a set of suitably defined effectiveness and cost variables is collected at \\(J\\) time points on \\(N\\) individuals, who have been allocated to \\(T\\) intervention groups. Assume that the primary endpoint of the trial is OS, while secondary endpoints include PFS, a self-reported health-related quality of life questionnaire (e.g. EQ-5D) and health records on different types of services (e.g. drug frequency and dosage, hospital visits, etc.). Following standard health economic notation, we denote with \\(\\boldsymbol e_{it}\\) and \\(\\boldsymbol c_{it}\\) the two sets of health economic outcomes (effectiveness and costs) collected for the \\(i\\)-th individual in treatment \\(t\\) of the trial. For simplicity, we define \\(\\boldsymbol e_{it}\\) and \\(\\boldsymbol c_{it}\\) based on the variables used in the analysis.\nThe effectiveness outcomes are represented by pre-progression (\\(e^{PFS}\\_{it}=\\text{QAS}^{\\text{PFS}}\\)) and post-progression (\\(e^{PPS}\\_{it}=\\text{QAS}^{\\text{PPS}}\\)) QAS data calculated using survival and utility data collected up to and beyond progression. We denote the full set of effectiveness variables as \\(\\boldsymbol e_{it}=(e^{\\text{PFS}}\\_{it},e^{\\text{PPS}}\\_{it})\\), formed by the pre and post-progression components. The cost outcomes are represented by a set of \\(K\\) variables (\\(c\\_{it}=c^k\\_{it}\\), for \\(k=1,\\ldots,K\\)) calculated based on \\(K\\) different types of health services and associated unit prices. We denote the full set of cost variables as \\(\\boldsymbol c\\_{it}=(c^1\\_{it},\\ldots,c^K\\_{it})\\), formed by the \\(K\\) different cost components.\nThe objective of the economic evaluation is to perform a patient-level partitioned survival cost-utility analysis by specifying a joint model \\(p\\boldsymbol e\\_{it}, \\boldsymbol c\\_{it} \\mid \\boldsymbol \\theta)\\), where \\(\\boldsymbol \\theta\\) denotes the full set of model parameters. Among these parameters, interest is in the marginal mean effectiveness and costs \\(\\boldsymbol \\mu=(\\mu\\_{et},\\mu\\_{ct})\\) which are used to inform the decision-making process. Different approaches can be used to specify \\(p\\boldsymbol e\\_{it}, \\boldsymbol c\\_{it} \\mid \\boldsymbol \\theta)\\). Here, we express the joint distribution as\n\\[\np(\\boldsymbol e_{it}, \\boldsymbol c_{it} \\mid \\boldsymbol \\theta) = p(\\boldsymbol e_{it} \\mid \\boldsymbol \\theta_e)p(\\boldsymbol c_{it} \\mid \\boldsymbol  e_{it} , \\boldsymbol  \\theta_c),\n\\tag{1}\\]\nwhere \\(p(\\boldsymbol e_{it} \\mid \\boldsymbol  \\theta_e)\\) is the marginal distribution of the effectiveness and \\(p(\\boldsymbol  c_{it} \\mid \\boldsymbol  e_{it} \\boldsymbol  \\theta_c)\\) is the conditional distribution of the costs given the effectiveness, respectively indexed by \\(\\boldsymbol  \\theta_e\\) and \\(\\boldsymbol  \\theta_c\\), with \\(\\boldsymbol  \\theta=(\\boldsymbol  \\theta_e,\\boldsymbol  \\theta_c)\\). We specify the model in Equation 1 in terms of a marginal distribution for the effectiveness and a conditional distribution for the costs. A key advantage of using a conditional factorisation, compared to a multivariate marginal approach, is that univariate models for each variable can be flexibly specified to tackle the idiosyncrasies of the data (e.g. non-normality ans spikes) while also capturing the potential correlation between the variables. We now describe how the two factors on the right-hand side of the Equation can be specified.\nFigure 1 provides a visual representation of the proposed modelling framework.\n\n\n\n\n\n\nFigure 1: Visual representation of the proposed modelling framework\n\n\n\nThe effectiveness and cost distributions are represented in terms of combined “modules”- the red and blue boxes - in which the random quantities are linked through logical relationships. Notably, this is general enough to be extended to any suitable distributional assumption, as well as to handle covariates in each module.\n\n\nConclusions\nAlthough our approach may not be applicable to all cases, the data analysed are very much representative of the “typical” data used in partitioned survival cost-utility analysis alongside clinical trials. Thus, it is highly likely that the same features apply to other real cases. This is a very important, if somewhat overlooked problem, as methods that do not take into account the complexities affecting patient-level data, while being easier to implement and well established among practitioners, may ultimately mislead cost-effectiveness conclusions and bias the decision-making process."
  },
  {
    "objectID": "research/reviewQES/reviewQES.html",
    "href": "research/reviewQES/reviewQES.html",
    "title": "Missingness Methods in trial-based Cost-Effectiveness Analysis",
    "section": "",
    "text": "We performed a systematic literature review that assesses the quality of the information reported and type of methods used to handle missing outcome data in trial-based economic evaluations. The purpose of this review is to critically appraise the current literature in within-trial CEAs with respect to the quality of the information reported and the methods used to deal with missingness for both effectiveness and costs. The review complements previous work, covering 2003-2009 (88 articles) with a new systematic review, covering 2009-2015 (81 articles) and focuses on two perspectives.\nFirst, we provide guidelines on how the information about missingness and related methods should be presented to improve the reporting and handling of missing data. We propose to address this issue by means of a Quality Evaluation Scheme (QES), providing a structured approach that can be used to guide the collection of information, formulation of the assumptions, choice of methods, and considerations of possible limitations for the given missingness problem. Second, we review the description of the missing data, the statistical methods used to deal with them and the quality of the judgement underpinning the choice of these methods."
  },
  {
    "objectID": "research/reviewQES/reviewQES.html#descriptive-review",
    "href": "research/reviewQES/reviewQES.html#descriptive-review",
    "title": "Missingness Methods in trial-based Cost-Effectiveness Analysis",
    "section": "Descriptive Review",
    "text": "Descriptive Review\n\n\n\n\n\n\nFigure 2: Missingness methods by outcome and period.\n\n\n\nFrom the comparison of the base-case methods used for the costs and effects between 2009 and 2015, the Figure above shows a marked reduction in the number of methods not clearly described for the effects, compared to those for the costs. A possible reason for this is that, while clinical effectiveness measures are often collected through self-reported questionnaires, which are naturally prone to missingness, cost measures rely more on clinical patient files which may ensure a higher completeness rate. It was not possible to confirm this interpretation in the reviewed studies due to the high proportions of articles not clearly reporting the missing rates in both 2003-2009 and 2009-2015 periods, for effects (\\(\\approx 45\\%\\) and \\(\\approx 38\\%\\)) and costs ( \\(\\approx 50\\%\\) and \\(\\approx 62\\%\\)). In addition, clinical outcomes are almost invariably the main objective of RCTs and are usually subject to more advanced and standardised analyses. Arguably, costs are often considered as an add-on to the standard trial: for instance, sample size calculations are almost always performed with the effectiveness measure as the only outcome of interest. Consequently, missing data methods are less frequently well thought through for the analysis of the costs. However, this situation is likely to change as cost data from different perspectives (e.g. caregivers, patients, society, etc.) are being increasingly used in trials, leading to the more frequent adoption of self-report cost data which may start to exhibit similar missingness characteristics to effect data.\nThe review identified only a few articles using more than one alternative method. In addition, these analyses are typically conducted without any clear justification about their underlying missing data assumptions and may therefore not provide a concrete assessment of the impact of missingness uncertainty. This situation indicates a gap in the literature associated with an under-implementation of sensitivity analysis, which may significantly affect the whole decision-making process outcome, under the perspective of a body who is responsible for providing recommendations about the implementation of alternative interventions for health care matters.\nLimiting the assessment of missingness assumptions to a single case is unlikely to provide a reliable picture of the underlying mechanism. This, in turn, may have a significant impact on the CEA and mislead its conclusions, suggesting the implementation of non-cost-effective treatments. Robustness analyses assess the sensitivity of the results to alternative missing data methods but do not justify the choice of these methods and their underlying assumptions about missingness which may therefore be inappropriate in the specific context analysed. By contrast, sensitivity analyses, which rely on external information to explore plausible alternative methods and missingness assumptions, represent an important and more appropriate tool to provide realistic assessments of the impact of missing data uncertainty on the final conclusions."
  },
  {
    "objectID": "research/reviewQES/reviewQES.html#quality-assessment",
    "href": "research/reviewQES/reviewQES.html#quality-assessment",
    "title": "Missingness Methods in trial-based Cost-Effectiveness Analysis",
    "section": "Quality assessment",
    "text": "Quality assessment\nGenerally speaking, most of the reviewed papers achieved an unsatisfactory quality score under the QES. Indeed, the benchmark area on the top-right corner of the graphs is barely reached by less than \\(7\\%\\) of the articles, both for cost and effect data.\nOverall, the proportions of the studies associated with the lowest category (E) prevails in the majority of the years, with a similar pattern over time between missing costs and effects. All the articles that are associated with the top category (A) belong to the period 2013-2015, with the highest proportions of articles falling in this category being observed in 2015 for both outcomes. The opportunity of reaching such a target might be precluded by the choice of the method adopted, which may not be able to support less restrictive assumptions about missingness, even when this would be desirable. As a result, when simple methods cannot be fully justified it is necessary to replace them with more flexible ones that can relax assumptions and incorporate more alternatives. In settings such as those involving MNAR, sensitivity analysis might represent the only possible approach to account for the uncertainty due to the missingness in a principled way. However, due to the lack of studies either performing a sensitivity analysis or providing high quality scores on the assumptions, missingness is not adequately addressed in most studies. This could have the serious consequence of imposing too restrictive assumptions about missingness and affect the outcome of decision making."
  }
]