[
  {
    "objectID": "research/volley/volley.html",
    "href": "research/volley/volley.html",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "We extend and adapt the modelling frameworks typically used for the analysis of football data and propose a novel Bayesian hierarchical modelling framework for the analysis and prediction of volleyball results in regular seasons. Three different sub-models or “modules” form our framework: (1) The module of the observed number of points scored by the two opposing teams in a match (\\(y_h\\) and \\(y_a\\)); (2) the module of the binary indicator for the number of sets played (\\(d^s\\)); (3) the module of the binary indicator for the winner of the match (\\(d^m\\)). These three modules are jointly modelled using a flexible Bayesian parametric approach, which allows to fully propagate the uncertainty for each unobserved quantity and to assess the predictive performance of the model in a relatively easy way. In the following, we describe the notation and the model used in each of the three modules.\n\n\nIn the first module of the framework, we model the number of points scored by the home and away team in the \\(i\\)-th match of the season \\(\\boldsymbol y=(y_{hi},y_{ai})\\) using two independent Poisson distributions, as shown in Equation 1 and Equation 2:\n\\[\ny_{hi} \\sim Poisson(\\theta_{hi}),\n\\tag{1}\\]\n\\[\ny_{ai} \\sim Poisson(\\theta_{ai}),\n\\tag{2}\\]\nconditionally on the set of parameters \\(\\boldsymbol \\theta=(\\theta_{hi},\\theta_{ai})\\), representing the scoring intensity in the \\(i\\)-th match for the home and away team, respectively. These parameters are then modelled using the log-linear regressions, as shown in Equation 3 and Equation 4:\n\\[\nlog(\\theta_{hi}) =\\mu + \\lambda + att_{h(i)} + def_{a(i)},\n\\tag{3}\\]\n\\[\nlog(\\theta_{ai}) =\\mu + att_{a(i)} + def_{h(i)},\n\\tag{4}\\]\nwhich corresponds to a Poisson log-linear model. Within these formulae, \\(\\mu\\) is a constant, while \\(\\lambda\\) can be identified as the home effect and represents the advantage for the team hosting the game which is typically assumed to be constant for all the teams and throughout the season. The overall offensive and defensive performances of the \\(k\\)-th team is captured by the parameters \\(att\\) and \\(def\\), whose nested indexes \\(h(i), a(i)=1,\\ldots,K\\) identify the home and away team in the \\(i\\)-th game of the season, where \\(K\\) denotes the total number of the teams.\nWe then expand the modelling framework to incorporate match-specific statistics related to the offensive and defensive performances of the home and away teams. More specifically, Equation 5 and Equation 6 show the effects associated with the attack intensity of the home teams and the defence effect of the away teams:\n\\[\natt_{h(i)} =\\alpha_{0h(i)} + \\alpha_{1h(i)}att^{eff}_{hi} + \\alpha_{2h(i)}ser^{eff}_{hi},\n\\tag{5}\\]\n\\[\ndef_{a(i)} =\\beta_{0a(i)} + \\beta_{1a(i)}def^{eff}_{ai} + \\beta_{2a(i)}blo^{eff}_{ai}.\n\\tag{6}\\]\nWe omit the index \\(i\\) from the terms to the left-hand side of the above formulae to ease notation, i.e. \\(att_{h(i)}=att_{h(i)i}\\) and \\(def_{a(i)}=def_{a(i)i}\\). The overall offensive effect of the home teams is a function of a baseline team specific parameter \\(\\alpha_{0h(i)}\\), and the attack and serve efficiencies of the home team, whose impact is captured by the parameters \\(\\alpha_{1h(i)}\\) and \\(\\alpha_{2h(i)}\\). The overall defensive effect of the away team is a function of a baseline team-specific parameter \\(\\beta_{0a(i)}\\), and the defence and block efficiencies of the away team, whose impact is captured by the parameters \\(\\beta_{1a(i)}\\) and \\(\\beta_{2a(i)}\\), respectively. Similarly, Equation 7 and Equation 8 show the effects associated with the attack intensity of the away teams and the defence effect of the home teams:\n\\[\natt_{a(i)} =\\alpha_{0a(i)} + \\alpha_{1a(i)}att^{eff}_{ai}+ \\alpha_{2a(i)}ser^{eff}_{ai},\n\\tag{7}\\]\n\\[\ndef_{h(i)} =\\beta_{0h(i)} + \\beta_{1h(i)}def^{eff}_{hi}+ \\beta_{2h(i)}blo^{eff}_{hi},\n\\tag{8}\\]\nTo achieve identifiability of the model, a set of parametric constraints needs to be imposed. We impose sum-to-zero constraints on the team-specific parameters, i.e. we set \\(\\sum_{k=1}^{K}\\alpha_{jk}=0\\) and \\(\\sum_{k=1}^{K}\\beta_{jk}=0\\), for \\(k=1,\\ldots,K\\) and \\(j=(0,1,2)\\). Under this set of constraints, the overall offensive and defensive effects of the teams are expressed as departures from a team of average offensive and defensive performance. Within a Bayesian framework, prior distributions need to be specified for all random parameters in the model. Weakly informative Normal distributions centred at \\(0\\) with a relatively large variances are specified for the fixed effect parameters.\n\n\n\nIn the second module, we explicitly model the chance of playing \\(5\\) sets in the \\(i\\)-th match of the season, i.e. the sum of the sets won by the home (\\(s_{hi}\\)) and away (\\(s_{ai}\\)) team is equal to \\(5\\). This is necessary when generating predictions in order to correctly assign the points to the winning/losing teams throughout the season and evaluate the rankings of the teams at the end of the season. We model the indicator variable \\(d^s_{i}\\), taking value \\(1\\) if \\(5\\) sets were played in the \\(i-\\)th match and \\(0\\) otherwise, as shown in Equation 9 and Equation 10, using a Bernoulli distribution\n\\[\nd^s_{i}:=\\mathbb{I}(s_{hi}+s_{ai}=5)\\sim\\mbox{Bernoulli}(\\pi^s_{i}),\n\\tag{9}\\]\nwhere\n\\[\nlogit(\\pi^s_{i})= \\gamma_0 + \\gamma_1y_{hi} + \\gamma_2y_{ai}.  \n\\tag{10}\\]\n\n\n\nThe last module deals with the chance of the home team to win the \\(i\\)-th match, i.e. the total number of sets won by the home team (\\(s_{hi}\\)) is larger than that of the away team (\\(s_{ai}\\)) – we note that we could have also equivalently decided to model the chance of the away team to win the \\(i\\)-th match. This part of the model is again necessary when predicting the results for future matches, since the team associated with the higher number of points scored in the \\(i\\)-th match may not correspond to the winning team. We model the indicator variable \\(d^m_{i}\\), taking value \\(1\\) if the home team won the \\(i-\\)th match and \\(0\\) otherwise, as shown in Equation 11 and Equation 12, using another Bernoulli distribution\n\\[\nd^m_{i}:=\\mathbb{I}(s_{hi}&gt;s_{ai}) \\sim\\mbox{Bernoulli}(\\pi^m_{i}),\n\\tag{11}\\]\nwhere\n\\[\nlogit(\\pi^m_{i})= \\eta_0 + \\eta_1y_{hi} + \\eta_2y_{ai} + \\eta_3 d^s_i.\n\\tag{12}\\]\nFigure 1 shows a graphical representation of the modelling framework proposed.\n\n\n\n\n\n\nFigure 1: Graphical representation of the modelling framework.\n\n\n\nThe framework corresponds to a joint distribution for all the observed quantities which are explicitly modelled. This is factored into the product of the marginal distribution of the total number of points scored by the two teams in each match, Module 1 – \\(p(\\boldsymbol y)\\), the conditional distribution of the probability of playing \\(5\\) sets in a match given \\(\\boldsymbol y\\), Module 2 – \\(p(d^s_i \\mid \\boldsymbol y)\\), and the conditional probability of winning the match given \\(\\boldsymbol y\\) and \\(d^s_i\\), Module 3 – \\(p(d^m_i\\mid \\boldsymbol y, d^s_i)\\). Module 1 also includes the different in-game statistics as covariates in the model. These are related to the either the offensive (serve and attack efficiency) or defensive (defence and block efficiency) effects of the home and away teams in each match of the season, and are respectively denoted in the graph as \\(\\boldsymbol x^{att}_{ti}=(ser^{eff}_{ti}, att^{eff}_{ti})\\) and \\(\\boldsymbol x^{def}_{ti}=(def^{eff}_{ti}, blo^{eff}_{ti})\\) to ease notation, for \\(t=(h,a)\\).\n\n\n\nAlthough the individual-level correlation between the observable variables \\(y_{hi}\\) and \\(y_{ai}\\) is taken into account through the hierarchical structure of the framework, a potential limitation of the model is that it ignores the possible multilevel correlation between the team-specific offensive \\(\\alpha_{jk}\\) and defensive \\(\\beta_{jk}\\) coefficients, for \\(j=(0,1,2)\\) and \\(k=1,\\ldots,K\\). In an alternative analysis, we account for the multilevel correlation using Inverse-Wishart distributions on the covariance matrix of the team specific parameters $ {}$ and $ {}$, which are scaled in order to facilitate the specification of the priors."
  },
  {
    "objectID": "research/volley/volley.html#module-1-modelling-the-scoring-intensity",
    "href": "research/volley/volley.html#module-1-modelling-the-scoring-intensity",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "In the first module of the framework, we model the number of points scored by the home and away team in the \\(i\\)-th match of the season \\(\\boldsymbol y=(y_{hi},y_{ai})\\) using two independent Poisson distributions, as shown in Equation 1 and Equation 2:\n\\[\ny_{hi} \\sim Poisson(\\theta_{hi}),\n\\tag{1}\\]\n\\[\ny_{ai} \\sim Poisson(\\theta_{ai}),\n\\tag{2}\\]\nconditionally on the set of parameters \\(\\boldsymbol \\theta=(\\theta_{hi},\\theta_{ai})\\), representing the scoring intensity in the \\(i\\)-th match for the home and away team, respectively. These parameters are then modelled using the log-linear regressions, as shown in Equation 3 and Equation 4:\n\\[\nlog(\\theta_{hi}) =\\mu + \\lambda + att_{h(i)} + def_{a(i)},\n\\tag{3}\\]\n\\[\nlog(\\theta_{ai}) =\\mu + att_{a(i)} + def_{h(i)},\n\\tag{4}\\]\nwhich corresponds to a Poisson log-linear model. Within these formulae, \\(\\mu\\) is a constant, while \\(\\lambda\\) can be identified as the home effect and represents the advantage for the team hosting the game which is typically assumed to be constant for all the teams and throughout the season. The overall offensive and defensive performances of the \\(k\\)-th team is captured by the parameters \\(att\\) and \\(def\\), whose nested indexes \\(h(i), a(i)=1,\\ldots,K\\) identify the home and away team in the \\(i\\)-th game of the season, where \\(K\\) denotes the total number of the teams.\nWe then expand the modelling framework to incorporate match-specific statistics related to the offensive and defensive performances of the home and away teams. More specifically, Equation 5 and Equation 6 show the effects associated with the attack intensity of the home teams and the defence effect of the away teams:\n\\[\natt_{h(i)} =\\alpha_{0h(i)} + \\alpha_{1h(i)}att^{eff}_{hi} + \\alpha_{2h(i)}ser^{eff}_{hi},\n\\tag{5}\\]\n\\[\ndef_{a(i)} =\\beta_{0a(i)} + \\beta_{1a(i)}def^{eff}_{ai} + \\beta_{2a(i)}blo^{eff}_{ai}.\n\\tag{6}\\]\nWe omit the index \\(i\\) from the terms to the left-hand side of the above formulae to ease notation, i.e. \\(att_{h(i)}=att_{h(i)i}\\) and \\(def_{a(i)}=def_{a(i)i}\\). The overall offensive effect of the home teams is a function of a baseline team specific parameter \\(\\alpha_{0h(i)}\\), and the attack and serve efficiencies of the home team, whose impact is captured by the parameters \\(\\alpha_{1h(i)}\\) and \\(\\alpha_{2h(i)}\\). The overall defensive effect of the away team is a function of a baseline team-specific parameter \\(\\beta_{0a(i)}\\), and the defence and block efficiencies of the away team, whose impact is captured by the parameters \\(\\beta_{1a(i)}\\) and \\(\\beta_{2a(i)}\\), respectively. Similarly, Equation 7 and Equation 8 show the effects associated with the attack intensity of the away teams and the defence effect of the home teams:\n\\[\natt_{a(i)} =\\alpha_{0a(i)} + \\alpha_{1a(i)}att^{eff}_{ai}+ \\alpha_{2a(i)}ser^{eff}_{ai},\n\\tag{7}\\]\n\\[\ndef_{h(i)} =\\beta_{0h(i)} + \\beta_{1h(i)}def^{eff}_{hi}+ \\beta_{2h(i)}blo^{eff}_{hi},\n\\tag{8}\\]\nTo achieve identifiability of the model, a set of parametric constraints needs to be imposed. We impose sum-to-zero constraints on the team-specific parameters, i.e. we set \\(\\sum_{k=1}^{K}\\alpha_{jk}=0\\) and \\(\\sum_{k=1}^{K}\\beta_{jk}=0\\), for \\(k=1,\\ldots,K\\) and \\(j=(0,1,2)\\). Under this set of constraints, the overall offensive and defensive effects of the teams are expressed as departures from a team of average offensive and defensive performance. Within a Bayesian framework, prior distributions need to be specified for all random parameters in the model. Weakly informative Normal distributions centred at \\(0\\) with a relatively large variances are specified for the fixed effect parameters."
  },
  {
    "objectID": "research/volley/volley.html#module-2-modelling-the-probability-of-playing-5-sets",
    "href": "research/volley/volley.html#module-2-modelling-the-probability-of-playing-5-sets",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "In the second module, we explicitly model the chance of playing \\(5\\) sets in the \\(i\\)-th match of the season, i.e. the sum of the sets won by the home (\\(s_{hi}\\)) and away (\\(s_{ai}\\)) team is equal to \\(5\\). This is necessary when generating predictions in order to correctly assign the points to the winning/losing teams throughout the season and evaluate the rankings of the teams at the end of the season. We model the indicator variable \\(d^s_{i}\\), taking value \\(1\\) if \\(5\\) sets were played in the \\(i-\\)th match and \\(0\\) otherwise, as shown in Equation 9 and Equation 10, using a Bernoulli distribution\n\\[\nd^s_{i}:=\\mathbb{I}(s_{hi}+s_{ai}=5)\\sim\\mbox{Bernoulli}(\\pi^s_{i}),\n\\tag{9}\\]\nwhere\n\\[\nlogit(\\pi^s_{i})= \\gamma_0 + \\gamma_1y_{hi} + \\gamma_2y_{ai}.  \n\\tag{10}\\]"
  },
  {
    "objectID": "research/volley/volley.html#module-3-modelling-the-probability-of-winning-the-match",
    "href": "research/volley/volley.html#module-3-modelling-the-probability-of-winning-the-match",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "The last module deals with the chance of the home team to win the \\(i\\)-th match, i.e. the total number of sets won by the home team (\\(s_{hi}\\)) is larger than that of the away team (\\(s_{ai}\\)) – we note that we could have also equivalently decided to model the chance of the away team to win the \\(i\\)-th match. This part of the model is again necessary when predicting the results for future matches, since the team associated with the higher number of points scored in the \\(i\\)-th match may not correspond to the winning team. We model the indicator variable \\(d^m_{i}\\), taking value \\(1\\) if the home team won the \\(i-\\)th match and \\(0\\) otherwise, as shown in Equation 11 and Equation 12, using another Bernoulli distribution\n\\[\nd^m_{i}:=\\mathbb{I}(s_{hi}&gt;s_{ai}) \\sim\\mbox{Bernoulli}(\\pi^m_{i}),\n\\tag{11}\\]\nwhere\n\\[\nlogit(\\pi^m_{i})= \\eta_0 + \\eta_1y_{hi} + \\eta_2y_{ai} + \\eta_3 d^s_i.\n\\tag{12}\\]\nFigure 1 shows a graphical representation of the modelling framework proposed.\n\n\n\n\n\n\nFigure 1: Graphical representation of the modelling framework.\n\n\n\nThe framework corresponds to a joint distribution for all the observed quantities which are explicitly modelled. This is factored into the product of the marginal distribution of the total number of points scored by the two teams in each match, Module 1 – \\(p(\\boldsymbol y)\\), the conditional distribution of the probability of playing \\(5\\) sets in a match given \\(\\boldsymbol y\\), Module 2 – \\(p(d^s_i \\mid \\boldsymbol y)\\), and the conditional probability of winning the match given \\(\\boldsymbol y\\) and \\(d^s_i\\), Module 3 – \\(p(d^m_i\\mid \\boldsymbol y, d^s_i)\\). Module 1 also includes the different in-game statistics as covariates in the model. These are related to the either the offensive (serve and attack efficiency) or defensive (defence and block efficiency) effects of the home and away teams in each match of the season, and are respectively denoted in the graph as \\(\\boldsymbol x^{att}_{ti}=(ser^{eff}_{ti}, att^{eff}_{ti})\\) and \\(\\boldsymbol x^{def}_{ti}=(def^{eff}_{ti}, blo^{eff}_{ti})\\) to ease notation, for \\(t=(h,a)\\)."
  },
  {
    "objectID": "research/volley/volley.html#accounting-for-the-multilevel-correlation",
    "href": "research/volley/volley.html#accounting-for-the-multilevel-correlation",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "Although the individual-level correlation between the observable variables \\(y_{hi}\\) and \\(y_{ai}\\) is taken into account through the hierarchical structure of the framework, a potential limitation of the model is that it ignores the possible multilevel correlation between the team-specific offensive \\(\\alpha_{jk}\\) and defensive \\(\\beta_{jk}\\) coefficients, for \\(j=(0,1,2)\\) and \\(k=1,\\ldots,K\\). In an alternative analysis, we account for the multilevel correlation using Inverse-Wishart distributions on the covariance matrix of the team specific parameters $ {}$ and $ {}$, which are scaled in order to facilitate the specification of the priors."
  },
  {
    "objectID": "research/reviewNL/reviewNL.html",
    "href": "research/reviewNL/reviewNL.html",
    "title": "A review of heath economic evaluation practice in the Netherlands: are we moving forward?",
    "section": "",
    "text": "Introduction\nIn the Netherlands, the Dutch National Health Care Institute (Zorginstituut Nederland or ZIN) is the body in charge of issuing recommendations and guidance on good practice in health economic evaluations, not just for pharmaceutical products, but also in relation to other fields of application such as medical devices, long-term care and forensics. In 2016, ZIN issued an update on the guidance for health economic evaluations, which aggregated into a single document and revised three separately published guidelines for pharmacoeconomics evaluation, outcomes research and costing manual. The novel aspects and future policy direction introduced by these guidelines have already been object of discussion, particularly with respect to the potential impact and concerns associated with their implementation in standard health economics practice in the Netherlands. Given the importance covered by these guidelines, an assessment of their impact on economic evaluation practice is desirable.\nThe objective of this paper was to review the evolution of health economic evaluation practice in the Netherlands before and after the introduction of the ZIN’s 2016 guidelines. Based on some key components within the health economics framework addressed by the new guidelines, we specifically focus on reviewing the statistical methods, missing data methods and software implemented by health economists. Given the intrinsic complexity of analysing health economics data, the choice of the analytical approaches to deal with these problems as well as transparent information on their implementation is crucial in determining the degree of confidence that decision-makers should have towards cost-effectiveness results obtained from these studies\n\n\nThe ZIN 2016 guidelines\nThe main objective of the guidelines is to ensure the comparability and quality of health economic evaluations in the Netherlands, therefore facilitating the task of the decision-maker regarding the reimbursement of new health care interventions. Following the example of guidelines issued by decision-making bodies in other countries, including the National Institute for Health and Care Excellence (NICE) in the UK, the recommended features for economic evaluations are summarised in a typical scenario referred to as ‘reference case’, although deviations from it are allowed when properly justified.\nBased on the structure of the reference case, four essential components of a health economic evaluation are identified: framework, analytic approach, input data and reporting. For the purpose of the review, we only focus on these components in the reference case as the main elements upon which evaluating health economics practice.\n\n\nMethods\nWe performed a bibliographic search in June 2021 using search engines of two online full-text journal repositories: (1) PubMed and (2) Zorginstituut. These sources were chosen to maximise the number of studies that could be accessed given the scoping nature of the review and the lack of a search strategy based on a pre-defined and rigid approach typical of systematic reviews. Articles were considered eligible for the review only if they were cost-effectiveness or cost-utility analyses targeting a Dutch population. To allow the inclusion of a reasonable amount of studies, the key words used in the search strategy were (cost-effectiveness OR cost-utility OR economic evaluation), and we targeted studies published between January 2016 and April 2021.\n\n\nAnalytic approaches\nAlmost all reviewed empirical analyses used bootstrapping (95%), although the number of replications varied largely across the studies, with the most popular choices being 5000 (55%) followed by 2000 (29%). Studies showed even more variability in the choice of the methods used in combination with bootstrapping. Seven general classes of statistical approaches were identified, among which unadjusted methods were the most popular choice across both time periods. A clear change in the type of statistical methods used between the two periods is denoted by a strong decrease (from 64 to 39) in the number of unadjusted analyses in 2016–2020 compared to the earlier period, which is compensated by a rise in the number of adjusted analyses using either SUR or linear mixed effects model (LMM) methods.\nFrom Figure 1 we can look at the different type and combination of software programs used as an indication of the implementation preferences of analysts for health economic evaluations. Although in principle the choice of software should have no impact on the quality of the statistical methods implemented, it has been highlighted how use of simpler software (e.g. spreadsheet calculators such as Excel) may become increasingly cumbersome for matching more realistic and therefore complex modelling requirements.\n\n\n\n\n\n\nFigure 1: Heatmap of the combination of software programs used\n\n\n\nThe most popular software was SPSS, chosen by 87 (52%) of the studies, either in the base-case (33%) or secondary (19%) analyses, often used in combination with Excel or by itself. When either STATA (26%) or R (13%) was used in the base-case analysis, SPSS was still the most popular choice in secondary analyses. Other combinations of software were less frequently chosen, even though 38 (23%) of the studies were unclear about the software implemented.\n\n\nMissing data methods\nAcross both periods limited changes are observed in terms of order of choice for missing data methods, with MI being the most popular base-case analysis, followed by complete case analysis (CCA), as the most popular SA choice. However, two noticeable variations in the frequency of these methods are observed between the two periods. First, the proportion of studies using MI in the base-case analysis has considerably increased over time (from 28 to 39%), which is compensated by a decrease in the proportion of less advanced methods such as CCA (from 14 to 5%) and single imputation (SI) (from 21 to 16%). Second, the number of studies not clearly reporting the methods has also considerably decreased (from 12 to 5%). The observed trend between the two periods may be the result of the specific recommendations from the 2016 guidelines in regards to the ‘optimal’ missing data strategy, resulting in a more frequent adoption of MI techniques and, at the same time, a less frequent use of CCA in the base-case analysis. However, in contrast to these guidelines, a large number of studies still does not perform any SA to missing data assumptions (about 65% in 2010–2015 and 63% in 2016–2020).\nMost of the studies lie in the middle and lower parts of the plot, and are associated with a limited or sufficient quality of information. However, only a few of these studies rely on very strong and unjustified missing data assumptions, while the majority provides either adequate justifications or uses methods associated with weak assumptions. Only 11 (14%) studies are associated with both high-quality scores and less restrictive missingness assumptions. No study was associated with either full information or adequate justifications for the assumptions explored in base-case and sensitivity analysis.\n\n\nDiscussion\nDescriptive information extracted from the reviewed studies provides some first insights about changes in practice in the years following the publication of the guidelines. First, a clear trend is observed towards an increase in the adoption of a societal and health care perspective and of CUA as the reference base-case analysis approach. Second, a similar increment is observed in the use of recommended instruments for the collection and valuation of health economic outcomes, such as EQ-5D-5L for QALYs and friction method for costs. Most of these changes are in accordance with the 2016 guidelines, which are likely to have played a role in influencing analysts and practitioners towards a clearer and more standardised way to report health economic results.\nWhen looking at the type of statistical methods used to perform the analysis, an important shift occurs between the two periods towards the use of methods that allow for regression adjustment, with a considerable uptake in the use of SURs and LMMs in the context of empirical analyses. These techniques are strongly supported by the 2016 guidelines in that they allow us to correct for potential bias due to confounding effects, deal with clustered data and formally take into account the correlation between costs and effects. Bootstrapping remains the most popular methods to quantify uncertainty around parameter estimates across both periods. However, the health economic analysis framework requires that the level of complexity of the analysis model is reflected in the way uncertainty surrounding the estimates is generated.\nThe transition between the two time periods reveals an increase in the use of MI techniques in the base-case analysis together with a decrease in the overall use of CCA. This change is in line with the 2016 guidelines which warns about the inherent limitations and potential bias of simple methods (e.g. CCA) when compared to MI as the potential reference method to handle missing values. Nevertheless, improvements are still needed given that many studies (more than 6%) performed the analysis under a single missing data assumption. This is not ideal since by definition missing data assumptions can never be checked, making the results obtained under a specific method (i.e. assumption) potentially biased.\n\n\nConclusions\nGiven the complexity of the health economics framework, the implementation of simple but likely inadequate analytic approaches may lead to imprecise cost-effectiveness results. This is a potentially serious issue for bodies such as ZIN in the Netherlands that use these evaluations in their decision making, thus possibly leading to incorrect policy decisions about the cost-effectiveness of new health care interventions. Our review shows, over time, a change in common practice with respect to different analysis components in accordance with the recent ZIN’s 2016 guidelines. This is an encouraging movement towards the standardised use of more suitable and robust analytic methods in terms of both statistical, uncertainty and missing data analysis. Improvements are however still needed, particularly in the choice of adequate statistical techniques to deal with the complexity of the data analysed and in the assessment of the impact of alternative missing data assumptions on the results in SA."
  },
  {
    "objectID": "research/mnarHTA/mnarHTA.html",
    "href": "research/mnarHTA/mnarHTA.html",
    "title": "Nonignorable Missingness Models in Health Technology Assessment",
    "section": "",
    "text": "Economic evaluation alongside Randomised Clinical Trials (RCTs) is an important and increasingly popular component of the process of technology appraisal. The typical analysis of individual level data involves the comparison of two interventions for which suitable measures of clinical benefits and costs are observed on each patient enrolled in the trial at different time points throughout the follow up. Individual level data from RCTs are almost invariably affected by missingness. The recorded outcome process is often incomplete due to individuals who drop out or are observed intermittently throughout the study, causing some observations to be missing. In most applications, the economic evaluation is performed on the cross-sectional variables, computed using only the data from the individuals who are observed at each time point in the trial (completers), with at most limited sensitivity analysis to missingness assumptions. This, however, is an extremely inefficient approach as the information from the responses of all partially observed subjects is completely lost and it is also likely biased unless the completers are a random sample of the subjects on each arm. The problem of missingness is often embedded within a more complex framework, which makes the modelling task in economic evaluations particularly challenging. Specifically, the effectiveness and cost data typically present a series of complexities that need to be simultaneously addressed to avoid biased results.\nUsing a recent randomised trial as our motivating example, we present a Bayesian parametric model for conducting inference on a bivariate health economic longitudinal response. We specify our model to account for the different types of complexities affecting the data while accommodating a sensitivity analysis to explore the impact of alternative missingness assumptions on the inferences and on the decision-making process for health technology assessment."
  },
  {
    "objectID": "research/mnarHTA/mnarHTA.html#modelling-framework",
    "href": "research/mnarHTA/mnarHTA.html#modelling-framework",
    "title": "Nonignorable Missingness Models in Health Technology Assessment",
    "section": "Modelling framework",
    "text": "Modelling framework\nThe distribution of the observed responses \\(\\boldsymbol y_{ijt}=(u_{ijv},c_{ijt})\\) is specified in terms of a model for the utility and cost variables at time \\(j=\\{0,1,2\\}\\), which are jointly modelled without using a multilevel approach and separately by treatment group. In particular, the joint distribution for \\(\\boldsymbol y_{ijt}\\) is specified as a series of conditional distributions that capture the dependence between utilities and costs as well as the time dependence.\nFollowing the recommendations from the published literature, we account for the skewness using Beta and Log-Normal distributions for the utilities and costs, respectively. Since the Beta distribution does not allow for negative values, we scaled the utilities on \\([0,1]\\) through the transformation \\(u^{\\star}_{ij}=\\frac{u_{ij}-\\text{min}(\\boldsymbol u_{j})}{\\text{max}(\\boldsymbol u_{j})-\\text{min}(\\boldsymbol u_{j})}\\), and fit the model to these transformed variables. To account for the structural values \\(u_{ij} = 1\\) and \\(c_{ij} = 0\\) we use a hurdle approach by including in the model the indicator variables \\(d^u_{ij}:=\\mathbb{I}(u_{ij}=1)\\) and \\(d^c_{ij}:=\\mathbb{I}(c_{ij}=0)\\), which take value \\(1\\) if subject \\(i\\) is associated with a structural value at time \\(j\\) and 0 otherwise. The probabilities of observing these values, as well as the mean of each variable, are then modelled conditionally on other variables via linear regressions defined on the logit or log scale. Specifically, at time \\(j=1,2\\), the probability of observing a zero and the mean costs are modelled conditionally on the utilities and costs at the previous times, while the probability of observing a one and the mean utilities are modelled conditionally on the current costs (also at \\(j=0\\)) and the utilities at the previous times (only at \\(j=\\{1,2\\}\\)). The model is summarised by Figure 1:\n\n\n\n\n\n\nFigure 1: Longitudinal model for missingness.\n\n\n\nWe use partial identifying restrictions to link the observed data distribution \\(p(\\boldsymbol y_{obs},\\boldsymbol r)\\) to the extrapolation distribution \\(p(\\boldsymbol y_{mis} \\mid \\boldsymbol y_{obs},\\boldsymbol r)\\) and consider interpretable deviations from a benchmark scenario to assess how inferences are driven by our assumptions. Specifically, we identify the marginal mean of the missing responses in each pattern \\(\\boldsymbol y^{\\boldsymbol r}_{mis}\\) by averaging across the corresponding components that are observed and add the sensitivity parameters \\(\\boldsymbol \\Delta_j\\).\nWe define \\(\\boldsymbol \\Delta_j=(\\Delta_{c_{j}},\\Delta_{u_{j}})\\) to be time-specific location shifts at the marginal mean in each pattern and set \\(\\boldsymbol \\Delta_j = \\boldsymbol 0\\) as the benchmark scenario. We then explore departures from this benchmark using alternative priors on \\(\\boldsymbol \\Delta_j\\), which are calibrated using the observed standard deviations for costs and utilities at each time \\(j\\) to define the amplitude of the departures from \\(\\boldsymbol \\Delta_j=\\boldsymbol 0\\)."
  },
  {
    "objectID": "research/lmmHTA/lmmHTA.html",
    "href": "research/lmmHTA/lmmHTA.html",
    "title": "Linear mixed models to handle missing at random data in trial based economic evaluations",
    "section": "",
    "text": "Introduction\nCost‐effectiveness analyses (CEAs) conducted alongside randomised controlled trials are an important source of information for decision-makers in the process of technology appraisal (Ramsey et al., 2015). The analysis is based on healthcare outcome data and health service use, typically collected at multiple time points and then combined into overall measures of effectiveness and cost. A popular approach to handle missingness is to discard the participants with incomplete observations (complete case analysis or CCA), allowing for derivation of the overall measures based on the completers alone. We note that slightly different definitions of CCA are possible, depending on the form of the model of interest, the type of missingness and the inclusion of observed covariates. This approach, although appealing by its simplicity, has well-recognised limitations including loss of efficiency and an increased risk of bias. We propose the use of linear mixed effects models (LMMs) as an alternative approach under MAR. LMMs are commonly used for the modelling of dependent data (e.g. repeated-measures) and belong to the general class of likelihood-based methods. LMMs appear surprisingly uncommon for the analysis of repeated measures in trial-based CEA, perhaps because of a lack of awareness or familiarity with fitting LMMs.\n\n\nMethods\nLinear mixed model extends the usual linear model framework by the addition of “random effect” terms, which can take into account the dependence between observations.\n\\[\nY_{ij}=\\beta_1+\\beta_2 X_{i1}+\\ldots+\\beta_(P+1) X_{iP}+\\omega_i+\\epsilon_{ij},\n\\tag{1}\\]\nwhere \\(Y_{ij}\\) denotes the outcome repeatedly collected for each individual \\(i=1,\\ldots,N\\) at multiple times \\(j=1,\\ldots,J\\). The model parameters commonly referred to as fixed effects include an intercept \\(\\beta_1\\) and the coefficients \\((\\beta_2,\\ldots,\\beta_{(P+1)})\\) associated with the predictors \\(X_{i1},\\ldots,X_{iP}\\), while \\(\\omega_i\\) and \\(\\epsilon_{ij}\\) are two random terms: \\(\\epsilon_{ij}\\) is the usual error term and \\(\\omega_i\\) is a random intercept which captures variation in outcomes between individuals. The models can be extended to deal with more complex structures, for example by allowing the effect of the covariates to vary across individuals (random slope) or a different covariance structure of the errors. LMMs can be fitted even if some outcome data are missing and provide correct inferences under MAR.\nA particular type of LMMs commonly used in the analysis of repeated measures in clinical trials is referred to as Mixed Model for Repeated Measurement (MMRM). The model includes a categorical effect for time, an interaction between time and treatment arm, and allows errors to have different variance and correlation over time (i.e. unstructured covariance structure). Figure 1 shows some examples of possible covariance structures that may be explored for LMMs.\n\n\n\n\n\n\nFigure 1: Some examples of covariance structures in LMM\n\n\n\nIncremental (between-group) or marginal (within-group) estimates for aggregated outcomes over the trial period, such as quality-adjusted life years (QALYs) or total costs can be retrieved as linear combinations of the parameter estimates from Equation 1. For example, the mean difference in total cost is obtained by summing up the estimated differences at each follow-up point, while differences on a QALY scale can be obtained as weighted linear combinations of the coefficient estimates of the utility model.\n\n\nConclusions\nWe believe LMMs represent an alternative approach which can overcome some of these limitations.\n\nFirst, practitioners may be more comfortable with the standard regression framework.\nSecond, LMMs can be tailored to address other data features (e.g. cluster-randomised trials or non-normal distribution) while also easily combined with bootstrapping.\nThird, LMMs do not rely on imputation, and results are therefore deterministic and easily reproducible, whereas the Monte Carlo error associated with multiple imputation may cause results to vary from one imputation to another, unless the number of imputations is sufficiently large.\n\nAlthough the methodology illustrated is already known, particularly in the area of statistical analyses, to our knowledge LMMs have rarely been applied to health economic data collected alongside randomised trials. We believe the proposed methods is preferable to a complete-case analysis when CEA data are incomplete, and that it can offer an interesting alternative to imputation methods."
  },
  {
    "objectID": "research/hurdleHTA/hurdleHTA.html",
    "href": "research/hurdleHTA/hurdleHTA.html",
    "title": "Bayesian Modelling for Health Economic Evaluations",
    "section": "",
    "text": "Modelling Framework\nWe propose a unified Bayesian framework that jointly accounts for the typical complexities of the data (e.g. correlation, skewness, spikes at the boundaries and missingness), and that can be implemented in a relatively easy way.\nConsider the usual cross-sectional bivariate outcome formed by the QALYs and total cost variables \\((e_{it}, c_{it})\\) calculated for the \\(i-\\)th person in group \\(t\\) of the trial. To simplify the notation, unless necessary, we suppress the treatment indicator \\(t\\). Equation 1 specifies the joint distribution \\(p(e_i,c_i)\\) as\n\\[\np(e_i,c_i) = p(c_i)p(e_i\\mid c_i) = p(e_i)p(c_i\\mid e_i)\n\\tag{1}\\]\nwhere, for example, \\(p(e_i)\\) is the marginal distribution of the QALYs and \\(p(c_i\\mid e_i)\\) is the conditional distribution of the costs given the QALYs. Note that, although the two factorisations are mathematically equivalent, the choice of which to use has different practical implications. From a statistical point of view, the factorisations require the specifications of different statistical models, e.g. \\(p(e_i)\\) or \\(p(e_i\\mid c_i)\\), which may have different approximation errors. From a clinical point of view, the two versions make different assumptions about the casual relationships between the outcomes, i.e. either \\(e_i\\) determines \\(c_i\\) or vice versa. We describe our analysis under the assumption that the costs are determined by the effectiveness measures and therefore we specify the joint distribution \\(p(e_i,c_i)\\) in terms of a marginal distribution for the QALYs and a conditional distribution for the costs.\nFor each individual we consider a marginal distribution \\(p(e_i \\mid \\boldsymbol \\theta_e)\\) indexed by a set of parameters \\(\\boldsymbol \\theta_e\\) comprising a location \\(\\boldsymbol \\phi_{ie}\\) and a set of ancillary parameters \\(\\boldsymbol\\psi_e\\) typically including some measure of marginal variance \\(\\sigma^2_e\\). Equation 2 models the location parameter using a generalised linear structure\n\\[\ng_e(\\phi_{ie})= \\alpha_0 \\,\\,[+ \\ldots]\n\\tag{2}\\]\nwhere \\(\\alpha\\_0\\) is the intercept and the notation \\([+\\ldots]\\) indicates that other terms (e.g. quantifying the effect of relevant covariates) may or may not be included. In the absence of covariates or assuming that a centered version \\(x_i^{\\star} = (x_i - \\bar{x})\\) is used, the parameter \\(\\mu_e = g_e^{-1}(\\alpha_0)\\) represents the population average QALYs. For the costs, we consider a conditional model \\(p(c_i\\mid e_i,\\boldsymbol\\theta_c)\\), which explicitly depends on the QALYs, as well as on a set of quantities \\(\\boldsymbol\\theta_c\\), again comprising a location \\(\\phi_{ic}\\) and ancillary parameters \\(\\boldsymbol \\psi_{c}\\). For example, when normal distributions are assumed for both \\(p(e_i \\mid \\boldsymbol \\theta\\_e)\\) and \\(p(c_i \\mid e_i, \\boldsymbol \\theta_c)\\), i.e. bivariate normal on both outcomes, the ancillary parameters \\(\\boldsymbol\\psi_c\\) include a conditional variance \\(\\tau^2_c\\), which can be expressed as a function of the marginal variance \\(\\sigma^2_c\\). More specifically, the conditional variance of \\(p(c_i \\mid e_i, \\boldsymbol \\theta_c)\\) is a function of the marginal effectiveness and cost variances and has the closed form \\(\\tau^2_c=\\sigma^2_c - \\sigma^2_e \\beta^2\\), where \\(\\beta=\\rho \\frac{\\sigma_c}{\\sigma_e}\\) and \\(\\rho\\) is the parameter capturing the correlation between the variables.\nEquation 3 models the location as a function of the QALYs as\n\\[\ng\\_c(\\phi\\_{ic}) = \\beta\\_{0} + \\beta\\_{1}(e\\_{i}-\\mu\\_{e})\\,\\,[+\\ldots]\n\\tag{3}\\]\nHere, \\((e_i-\\mu_e)\\) is the centered version of the QALYs, while \\(\\beta_{1}\\) quantifies the correlation between costs and QALYs. Assuming other covariates are either also centered or absent, \\(\\mu_c = g_c^{-1}(\\beta_{0})\\) is the estimated population average cost. The Figure 1 shows a graphical representation of the general modelling framework.\n\n\n\n\n\n\nFigure 1: Modelling framework.\n\n\n\nThe QALYs and cost distributions are represented in terms of combined modules, the blue and the red boxes, in which the random quantities are linked through logical relationships. This ensures the full characterisation of the uncertainty for each variable in the model. Notably, this is general enough to be extended to any suitable distributional assumption, as well as to handle covariates in either or both the modules.\nThe proposed framework allows jointly tackling of the different complexities that affect the data in a relatively easy way by means of its modular structure and flexible choice for the distributions of the QALYs and cost variables. Using the MenSS trial as motivating example, we start from the original analysis and expand the model using alternative specifications that progressively account for an increasing number of complexities in the outcomes. We specifically focus on appropriately modelling spikes at the boundary and missingness, as they have substantial implications in terms of inferences and, crucially, cost-effectiveness results.\n\n\nExample\nThree model specifications are considered and applied to QALY data from a RCT case study: 1) Normal marginal for the QALYs and Normal conditional for the costs (which is identical to a Bivariate Normal distribution for the two outcomes); 2) Beta marginal for the QALYs and Gamma conditional for the costs; and 3) Hurdle Model. Figure 2 shows the observed QALYs in both treatment groups (indicated with black crosses) as well as summaries of the posterior distributions for the imputed values, obtained from each model. Imputations are distinguished based on whether the corresponding baseline utility value is observed or missing (blue or red lines and dots, respectively) and are summarised in terms of posterior mean and \\(90\\%\\) HPD intervals.\n\n\n\n\n\n\nFigure 2: Imputed QALYs under alternative model specifications.\n\n\n\nThere are clear differences in the imputed values and corresponding credible intervals between the three models in both treatment groups. Neither the Bivariate Normal nor the Beta-Gamma models produce imputed values that capture the structural one component in the data. In addition, as to be expected, the Bivariate Normal fails to respect the natural support for the observed QALYs, with many of the imputations exceeding the unit threshold bound. These unrealistic imputed values highlight the inadequacy of the Normal distribution for the data and may lead to distorted inferences. Conversely, imputations under the Hurdle Model are more realistic, as they can replicate values in the whole range of the observed data, including the structural ones. Imputed unit QALYs with no discernible interval are only observed in the intervention group due to the original data composition, i.e. individuals associated with a unit baseline utility and missing QALYs are almost exclusively present in the intervention group.\n\n\nConclusions\nWe have presented a flexible Bayesian framework that can handle the typical complexities affecting outcome data in CEA, while also being relatively easy to implement using freely available Bayesian software. This is a key advantage that can encourage practitioners to move away from likely biased methods and promote the use of our framework in routine analyses. In conclusion, the proposed framework can:\n\nJointly model costs and QALYs;\nAccount for skewness and structural values;\nAssess the robustness of the results under a set of differing missingness assumptions.\n\nThe original contribution of this work consists in the joint implementation of methods that account for the complexities of the data within a unique and flexible framework that is relatively easy to apply. In the next chapter we will take a step forward in the analysis and present a longitudinal model that can use all observed utility and cost data in the analysis, explore alternative nonignorable missing data assumptions, while simultaneously handling the complexities that affect the data."
  },
  {
    "objectID": "posts/2022-05-05-my-blog-post/index.html",
    "href": "posts/2022-05-05-my-blog-post/index.html",
    "title": "R markdown for teaching",
    "section": "",
    "text": "Hi guys, today I wanted to post something different from usual research tediousness that is only liked by statisticians and try to focus on another component my academic career, education or teaching, which has recently become quite prominent (with pros and cons of course).\nSpecifically, I have been recently interested in incorporating some reproducible documentation within my own teaching material since I am not course coordinators in statistics for bachelor students here at UM. In the past, I saw that people tended to use these very nice but really time-consuming word documents that needed update every year for making questions and assignment tasks. Do not get me wrong the different examples and ideas behind these assessments are really nice but I have to admit that, given the inevitable need to use the same type of dataset every year, the chance that students received some tips about these tasks was quite nonignorable.\nThus, after receiving some inputs from one of my colleagues (thanks Sophie!), I have decided to try out full R markdown documentation to generate examples similar to the ones used in the past but that could be generated in a slightly different way every year taking advantage of the reproducibility of such documents with an incorporated R coding at the basis of the data generation procedure. I really think these approaches will represent the future for any type of teaching activity that involves some sort of data analysis and interpretation such as statistics: the possibility they offer to efficiently provide new examples without the need to look for new data or each time generate new datasets is so enticing for teachers and researchers who are involved in education. It makes our job so much easier in the long term which more than compensates the cost of learning how to implement these approaches.\nAs an example, I will focus here on the topic of simple linear regression which I give to students of the second year. For example, let’s say that the objective of the tutorial is to learn some basic concepts of linear regression modelling, e.g. interpretation of coefficients and correlation measures. You can start with an introductory part to describe the type of dataset students will need to work on.\nBackground\nThe dataset XXX comprises 40 countries (Country) in the world with populations of more than 20 million as of 1990 and records the life expectancy at birth (Lex), the number of people per television set (Ptel), and the number of people per physician (Pphy). The average life expectancies between males and females are provided as the country’s overall life expectancy.\nThis text will directly appear on the final document as you typed in the Rmd file. So for it works like a standard word file but it is now that the magic happens. We can create chunks of R code and embed them within the Rmd file and use different options to decide whether to show or not such code lines. For example, let’s say we want to generate the data of the above mentioned dataset. We can create a new chunk which incorporates the following R code that simulates the desired variables, their relationships, and combine them together into the new dataset.\n\nset.seed(768)\nn &lt;- 40\nln_pphy &lt;- rnorm(n, 7.2, 1.3)\nln_ptel &lt;- rnorm(n, 2.38, 1.56)\npphy &lt;- exp(ln_pphy)\nptel &lt;- exp(ln_ptel)\nerror &lt;- rnorm(n, 0, 6)\nlex &lt;- 70 - 0.023*ptel - 0.001*pphy + error\ncountry &lt;- c(\"Argentina\", \"Bangladesh\", \"Brazil\", \"Canada\", \"China\", \"Colombia\", \"Egypt\", \"Ethiopia\", \"France\", \"Germany\", \"India\", \"Indonesia\", \"Iran\", \"Italy\", \"Japan\", \"Kenya\", \"Korea North\", \"Korea South\", \"Mexico\", \"Morocco\", \"Myanmar Bur\", \"Pakistan\", \"Peru\", \"Philippines\", \"Poland\", \"Romania\", \"Russia\", \"South Afric\", \"Spain\", \"Sudan\", \"Taiwan\", \"Tanzania\", \"Thailand\", \"Turkey\", \"Ukraine\", \"UK\", \"USA\", \"Venezuela\", \"Vietnam\", \"Zaire\")\ndata_le_tv_sim &lt;- data.frame(country, lex, ptel, pphy)\n\nNow all the generated variables, namely lex, ptel, pphy and country, are included into the data frame called data_le_tv_sim which has been created in the R workspace. If the focus on the tutorial is not on coding, it is desirable to hide the R code from the final document so that it is not displayed (but is still present and saved within R). R markdown allows you to do this in a really straightforward way by means of chunk options which can be customised for each created chunk. For example,\n\nThe option echo=FALSE allows to hide the R lines (while setting it to TRUE shows the lines)\nThe option eval=TRUE tells the software to actually run the lines inside the chunk (setting it to FALSE prevents from doing so)\n\nThese are only two of many different options that can be customised in regard to the display of tables, figures, code lines color, size, font, etc…. For a full illustration of the high degree of customisation provided by R markdown I refer to the dedicated webpage.\nAfter the introduction is done and perhaps the context and objective of the analysis presented, we can start asking questions. Let’s start with somehting simple, such as\nExercise\na. Examine and comment on the distributions of each variable separately (descriptives, histograms etc).\nWell we can answer our own question (i.e. we give ourselves the solutions) by creating a new chunk in which we generate the desired output in R. Since we generated the dataset in the chunk before, it is still available within the R environment and we do not need to re-create it!\n\nlibrary(ggplot2)\nhist_lex &lt;- r2spss::histogram(data_le_tv_sim, variable = \"lex\") + xlab(\"life expectancy\")\nhist_ptel &lt;- r2spss::histogram(data_le_tv_sim, variable = \"ptel\") + xlab(\"people per television\")\nhist_pphy &lt;- r2spss::histogram(data_le_tv_sim, variable = \"pphy\") + xlab(\"people per physician\")\nbox_lex &lt;- r2spss::box_plot(data_le_tv_sim, variables = \"lex\") + xlab(\"life expectancy\")\nbox_ptel &lt;- r2spss::box_plot(data_le_tv_sim, variables = \"ptel\") + xlab(\"people per television\")\nbox_pphy &lt;- r2spss::box_plot(data_le_tv_sim, variables = \"pphy\") + xlab(\"people per physician\")\ngridExtra::grid.arrange(hist_lex, hist_ptel, hist_pphy,\n             box_lex, box_ptel, box_pphy, nrow = 2)\n\n\n\n\n\n\n\n\nHere I used some ggplot2 coding to generate some boxplots and histograms of the data but of course the choice is entirely yours to decide which graphs or summaries should be provided by the students. For generating tables, we can also take advantage of the package knitr and its function kable which allows to display standard R tables in a much prettier format.\nlibrary(knitr)\nlibrary(dplyr)\nd.summary.extended &lt;- data_le_tv_sim %&gt;%\n    dplyr::select(lex, ptel, pphy) %&gt;%\n    psych::describe(quant=c(.25,.75)) %&gt;%\n    as_tibble(rownames=\"rowname\")\nd.summary &lt;- d.summary.extended %&gt;%\n    dplyr::select(var=rowname, min, q25=Q0.25, median, q75=Q0.75, max, mean, sd)\nkable(d.summary, caption = \"Summary statistics\", format = \"html\", digits = 1)\n\n\nSummary statistics\n\n\nvar\nmin\nq25\nmedian\nq75\nmax\nmean\nsd\n\n\n\n\nlex\n28.7\n62.3\n68.0\n70.0\n81.3\n65.9\n8.2\n\n\nptel\n0.2\n3.6\n19.5\n41.2\n1349.5\n62.9\n211.4\n\n\npphy\n106.1\n710.2\n1375.0\n2767.5\n11031.2\n2206.0\n2321.1\n\n\n\n\n\n\nAlso kable provides a lot of customisation options that allow to have many different types of formats and styles for your table, which can also be further extended using the package kableExtra. But let’s continue with our test.\nb. Would you expect an association between a country’s life expectancy and its density of people per television set? If yes, would that be positive or negative (use the scatter plot function)?\nTo answer this question we could produce some scatter plots between the variables in R. An alternative would be to calculate the Pearson’s correlation coefficient as an indicative number. We could obtain such number by creating a corresponding chunk code but given that we only need a number it is actually more convenient to generate it within our line of text. How can we do that? simple, in the Rmd file you can include some inline R code by using the quotes signs. This means that instead of having:\n\n#corr coeff\nround(cor(lex, ptel, method = c(\"pearson\")), digits = 2)\n\n[1] -0.76\n\n#corr coeff test\nas.numeric(cor.test(lex, ptel, method = c(\"pearson\"))[\"p.value\"])\n\n[1] 1.70736e-08\n\n\nyou can directly write a sentence an inlcude the output of the two above functions within the text. The results would be something like: the estimated Pearson’s correlation coefficient value between life expectancy and people per television is -0.76. The p-value given by the correlation test is 1.7073596^{-8}. What if now we want to include some theoretical stuff for the students?\nd. Interpret your results. Would the sending of television shiploads to countries with short life expectancies improve the latter? Is there an explanation for your findings?\nWe can do it in an easy way since R markdown also supports latex math environments in combination with inline R code. For example, we can answer with something like the following. Since we are looking at the correlation coefficient \\(\\rho\\) (Pearson’s), then the null and alternative hypotheses about the test for the linear association between life expectancy and people per television can be formulated as:\nH0. \\(\\rho = 0\\) (no linear association between variables)\nH1. \\(\\rho \\neq 0\\) (linear association between variables)\nWe can use the correspnding p-value of this test equal to 1.7073596^{-8} to make a decision to whether reject or not the null hypothesis based on the evidence from the observed data. Finally, let’s do what we are here for, linear regression!\ne. Perform a procedure that predicts life expectancy from people per television\nIn the following code I run the model and save some key output that I will need to display later on. A convenient feature of R markdown writing is that, after running a code chunk, everything that was successfully run is saved in the current R workspace and can be called back later on in the document very easily.\n\nfitm1_r &lt;- lm(lex ~ ptel, data = data_le_tv_sim)\nfitm1rsq &lt;- round(summary(fitm1_r)$r.square, digits = 2)\nfitm1RSS &lt;- round(anova(fitm1_r)[\"ptel\", \"Sum Sq\"], digits = 2)\nfitm1ESS &lt;- round(anova(fitm1_r)[\"Residuals\", \"Sum Sq\"], digits = 2)\nfitm1TSS &lt;- fitm1RSS + fitm1ESS\nfitm1pval &lt;- round(summary(fitm1_r)$coefficients[2, 4], digits = 4)  \n\nWe can then decide whether we want to show the summary results directly from the R output\n\nsummary(fitm1_r)\n\n\nCall:\nlm(formula = lex ~ ptel, data = data_le_tv_sim)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.792 -3.218  1.341  3.231 13.850 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 67.784092   0.895708  75.677  &lt; 2e-16 ***\nptel        -0.029235   0.004108  -7.117 1.71e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.424 on 38 degrees of freedom\nMultiple R-squared:  0.5714,    Adjusted R-squared:  0.5601 \nF-statistic: 50.66 on 1 and 38 DF,  p-value: 1.707e-08\n\n\nor perhaps hide the above output (setting the chunk option echo=FALSE) and provide a textual explanation while also embedding the R code generating the numeric results of interest (e.g. estimates or CI bounds) within the written text. If doing so, then we could have something like the following.\nThe model intercept and coefficient are 67.78 and -0.03, respectively. This means that 67.78 change in life expectancy is associated with a one unit increase in people per television. The p-value of this estimate is 0 with corresponding \\(95\\%\\) confidence interval being (-0.04, -0.02).\nThe analysis of variance reveals that the residual sum of squares (ESS) is 1118.01, the regression sum of squares (RSS) is 1490.38, for a total sum of squares (TSS) of 2608.39. The corresponding \\(R^2=\\frac{\\text{RSS}}{TSS}\\) value of the model is given by 0.57, which suggests how about 57 % of the variation in life expectancy is explained by people per television.\nConclusions\nR markdown provides a really nice opportunity to replace old and static documents with files that can be easily updated by simply changing a couple of lines of code. The results of this exercise can be replicated by using the same seed number but can also be varied by simply changing such number or by setting it to be randomly generated every time. Although not very popular yet, I truly believe R markdown will innovate the way teaching is done, at least within the scientific and quantitative data analysis field. For statistics, this is perfect since it allows an automated procedure to obtain new datasets that are consistent with the coding source created by the educator while also providing students with different numeric examples that makes it harder to cheat by simply “copying the answers from the previous years”.\nI have started using R markdown for my own research a few years ago but its extreme flexibility and advantage over standard education tools for writing up documents has lead me to make it my best option also within my educational activities. Of course, I understand that the barrier to entry can be quite steep, especially for people who are not familiar with coding. However, I believe that getting familiar with these methods has become more and more relevant in the last years and will represent an essential skill for every teacher involved in data analysis topics.\nNot convinced yet? are you not a fan of latex or html coding? no problem, R markdown allows also to produce these documents in word and even power point formats!\nHurray for R markdown!!!"
  },
  {
    "objectID": "posts/2022-03-15-my-blog-post/index.html",
    "href": "posts/2022-03-15-my-blog-post/index.html",
    "title": "Attending conferences and invited talks",
    "section": "",
    "text": "Hello folks, here I am with my monthly update about my work and basically what I am up to. A few (mostly happy) things have happened since last time so thought it would be nice to share them with you.\n\nFirst, the work of a UCL PhD student I am currently supervising has been published in Value in Health, congratulations Xiaoxiao you did an excellent job! The paper is an updated review of current approaches used in the heath economics literature for dealing with missing data (you don’t say), this time taking a more diaggregated perspective compared to previous reviews and looking at which “level” missingness was reported/addressed in these studies, namely questionnarie item, questionnaire score or aggregate levels. It took a lot of work to get this work done since it requires a huge amount of time to review the literature as well as to find an intuitive way to communicate the results from this extraction work, which it turned out to be quite complex (weirdly enough).\nSecond, my abstract submission for lolaHESG 2022 has been accepted and I will be present my new work at the conference this upcoming May here in Maastricht. This is the first time I will be attending this health economics conference which is very specific to the Netherlands but which shares the same structure and format of the standard HESG confernce in the UK (which I already attended a couple of times). I think there will be room for some nice discussions about both theoretical and applied works and I am eager to meet up new colleagues and to share our research ideas and, why not, get to know each other and possibly find new collaborations!\nThird, my other abstract submission for EUHEA 2022 has also been accepted as an oral presentation which I will disucss this June in Oslo. I am very excited for this conference given that I have never been to Oslo before (which looks amazing, I mean look at the picture!) and which was initially planned in the summer of 2020 but that, for obvious reasons, was then cancelled. I will talk about my work in a more international context still related to health economics and I hope I will be able to meet up with some old colleagues and to have an update of what they have been doing these last couple of years. I stil need to sort out a few things about the dates due to some issues in booking my registration but I hope I will be able to fix these problems as soon as possible!\nFourth, I have been kindly invited to give some talks about my reserahc work at a few different online health economics seminar series in the next months, which I would be glad to do. These include: a seminar held by the Health Economics and Health Technology Assessment group at VU Amsterdam with whom I had a very nice chat and fruitful research discussion back in February; the Statistics, Health Economics and Methodology Seminar at PRIMENT Clinical Trials Unit which I attended for a few years while I was doing my post-doc research in the UK and for whom I would be happy to share my current work; the King’s Health Economics seminar held by the Health Service and Population Research Department which has showed some interest in my work about missing data methods which makes me already happy to give my availability. Lots of interesting stuff. I just need to find the time to prepare all these talks …\n\nAmong other things I have been doing and which are worth mentioning there is this on-going teaching qualification course I am attending which is very funny but also quite useful in order to take the official role of course coordinator for future courses, some talks with private consultancy companies for which I am about to start collaborating to earn some money on behalf of my department, and getting my booster vaccination dose which had a quite strong impact on my last two days (basically a walking zombie for most of the days). Perhaps, the last thing to mention is that I will need to start working on my research grant VENI pre-proposal, whose deadline is September 2022. Although this is just a pre-proposal phase (done to screen out the many applications), I really need to focus and put some effort in laying down my application to the best of my abilities to at least pass this phase. Last year I did not make it but this was also due to the fact that I only had about 2 months time to write down my proposal with basically no time for feedback from any colleagues which, instead, I fully intend to ask in order to improve the chances of success. Fingers crossed for this year!\nAt least I am excited to enjoy myself at one of these upcoming conferences and meet up with some new/old colleagues in my research field given that it has been a while since last time I was able to travel for research!"
  },
  {
    "objectID": "posts/2022-01-05-my-blog-post/index.html",
    "href": "posts/2022-01-05-my-blog-post/index.html",
    "title": "And now what?",
    "section": "",
    "text": "Hello folks and happy new year!\nDespite all the problems that occurred in this period in relation to the pandemic and travel restrictions, I hope you had a nice break and recharged your batteries for the upcoming months. As for me, my holidays were decent, let’s say that. To start the new year in the best possible way, I thought today I could share my opinion on a recently published work in my research area, specifically missing data models in trial-based economic analyses. The first author of this paper entitled Flexible Bayesian longitudinal models for cost-effectiveness analyses with informative missing data is one of my ex PhD supervisor, Dr. Alexina Mason, which by itself is already a guarantee of some nice and careful research work. I take this chance to go through her most recent work (of course Bayesian) and try to summarise the key aspects I am interested in.\nThe paper is a well-written and structured illustration of how standard modelling in trial-based CEAs can be extended, taking advantage of the Bayesian flexibility in model specification, in order to take into account some of the typical statistical idiosyncrasies that affect health economic outcome data, including:\n\nThe correlation between cost and effectiveness measures\nThe skewness in the distributions of costs and utilities\nThe presence of “structural values”, e.g. many people having a perfect health state corresponding to an utility of one, that cause a huge spike in the observed distributions\nThe need to take into account missing data uncertainty under MAR while also exploring some MNAR departures through a proper modelling approach and external information\n\nThese are all very valid and important aspects that, despite having been mentioned and discussed by many authors in the literature, are usually only partially tackled in routine analyses due to complexity of model specification based on standard software commands. Nonetheless, I believe this is still an important point that must be highlighted until people will start to move away from standard methods towards the implementation of more advanced approaches, either by using more powerful software (e.g. R instead of Excel) or by acquiring some basic knowledge to implement more advanced methods (e.g. multiple imputation or Bayesian approach). But I am getting away from the topic at hand. So, let’s go back to it.\nThe paper first summarises the literature that has been done in relation to discussing the issues and potential pitfalls of standard methods that do not properly recognise all the features of the data and provides a real case study characterised by the majority of these statistical issues. Next, the modelling framework is provided following the typical and very intuitive (in my opinion!) Bayesian structure based on submodules linked and built one on top of the other to create a coherent and fully probabilistic approach.\n1 The first module consists in the main analysis model required in order to derive the key estimates of interest, formed by the marginal model of the effectiveness variables (HRQoL data collected a five different time points) and the conditional model of the cost variables (aggregated over the whole trial duration). This corresponds to a longitudinal model for the utilities which is linked to a model for the total costs, therefore taking into account the longitudinal structure of the available data as well as the correlation between the two types of outcome. The authors accounts for the skewness in the distribution of the costs using Gamma distributions, while also dealing with the presence of structural ones in the HRQoL data via hurdle models in combination with Gamma distributions which are used to model the complement of the utilities, i.e. \\(u^\\star=1-u\\) (this transformation is done to allow Gamma distributions to be fitted to the data).\n2 The second module is optional and consists in an imputation model for any partially-observed covariate that enters the model in step 1. This is important as any unobserved covariate value will cause the removal of the corresponding case from the analysis unless the missing value is first imputed. Within a Bayesian framework this requires the specification of a distribution for these covariates or, when considered acceptable, some simple imputation methods may be used prior to the analysis.\n3 The third module is related to the missingness model for the utilities, which is specified in terms of a multinomial model in which different types of missingness (e.g. intermittent vs dropout) are associated with different probabilities, estimated based on the available missingness indicator patterns and prior probabilities. In addition, covariates may also be included in this module to make missingness assumptions more plausible.\nIn my opinion this third step is perhaps the most innovative compared to other literature works. Indeed, although this modelling framework is nothing new as it refers to a selection model specification based on a model for the outcome \\(\\boldsymbol u\\) and a conditional model for missingness \\(\\boldsymbol m\\):\n\\[\np(\\boldsymbol u,\\boldsymbol m) = p(\\boldsymbol u)p(\\boldsymbol m \\mid \\boldsymbol u),\n\\]\nthe use of a multinomial distribution within this context was not really considered before. This is because usually a Bernoulli distribution would be considered enough to distinguish missing from observed data. However, through this framework, analysts are free to choose different types of missingness patterns and assign to each of this a different prior probability, perhaps related to different reasons associated with the missing data. Thus, even if the selection model framework notoriously suffers from problems related to the not transparent identification of the unobserved distribution of the model (e.g. unclear weight of the choice of the modelling distributions on missingness assumptions or unclear definition of sensitivity parameters), the possibility to allow for different types of missingness within the same model specification gives more flexibility with respect to the formulation of the missingness assumptions. For example, analysts may wish to explore some MNAR assumption for a specific pattern of missing data (e.g. dropout) while keeping a MAR assumption for other patterns (e.g. intermittent). Of course, an inconvenience of this approach, compared to standard selection models, is that under MNAR the number of parameters to be identified via external information is increased since there are multiple types of missingness patterns. It is therefore required extra-care in the choice of these parameters (or informative priors on these parameters) in that different estimates should be considered for each pattern that is modelled. In addition, since sensitivity analysis is en essential component for any type of missing data model, the presence of additional informative parameters requires the exploration of more alternative scenarios in order to have a general idea of the impact that different assumptions for each of these parameters may have on the final conclusions.\nFinally, the authors conclude with applying the proposed methods to the case study data and elicit alternative prior specifications for the unidentified parameters of the missingness model to assess the robustness of their conclusions to alternative missingness assumptions. In total they consider up to \\(8\\) different missingness scenarios. These are distinguished in terms of different assumptions about the strength and direction for each missing data pattern (dropout vs intermitten) and treatment arm (control and intervention) in the study. The also compared the results obtained under the different MNAR scenarios with those from the complete cases and under MAR to give an idea of what is the impact that a change in the assumptions related to the different types of missingness may have on the final cost-effectiveness conclusions. They chose to summarise this using the the net benefit measure as the key indicator to reflect changes in cost-effectiveness under the different missingness scenarios.\nOverall, I must say that I really enjoed myself reading this paper since it provides a ver well-written piece of work which illustrates how relatively complex models can be fitted to health economic data using software that are available to everybody although they may require some time to be learned (authors provided the entire model code in JAGS). I really liked the model specification which sorts of corresponds to an extension of one of my previous works to take into account the longitudinal nature of the data and, especially, the specification of alternative MNAR scenarios via sensitivity analysis based on multinomial distributions. I think that providing these methods in some sort of package in R would be really helpful for analysts who are not familiar with Bayesian software. Otherwise, I am afraid that people will continue using more standardised methods (e.g. MI) simply on the basis of the fact that they can be implemented in more straightforward ways (although these are likely to be less flexible).\nReally some nice work Alexina, well done! This reminds me that I need to get back to my current projects (oh no, so much to do!)"
  },
  {
    "objectID": "posts/2021-10-10-my-blog-post/index.html",
    "href": "posts/2021-10-10-my-blog-post/index.html",
    "title": "Baseline adjustment in trial based CEA",
    "section": "",
    "text": "Recently I have come across something I found a little odd when performing a statistical analysis of trial-based CEA data and I would like to share here my experience in the hope that anybody may be able to read it (and correct me if I am wrong). It is something related to the implementation of baseline adjustment for utility score data via regression approach.\nTo give an idea of the context of the analysis I quickly use some simulated data as an example of a dataset that could be object for this type of analysis. To make things simple, I simulated individual-level utility score data which are measured at baseline (\\(u_0\\)), 6 (\\(u_1\\)) and 12 (\\(u_12\\)) months follow up for two competing intervention groups, say a control (t=0) and an intervention (t=1). Again, to make things super easy I simulated these assuming a multivariate normal distribution with constant variance and no time correlation. Although this is not realistic it only serves the purpose to illustrate the issue I am facing. So let’s simulate the data.\n\n#load and pre-process the simulated dataset\nlibrary(readstata13)\n\n#wide format\ndata_wide&lt;-read.dta13(\"ex_data.dta\")\ndata_wide$trt &lt;- as.numeric(data_wide$arm)\ndata_wide$subjects&lt;-rep(1:length(data_wide$arm))\ndata_wide &lt;- data_wide[data_wide$arm %in% c(\"Placebo\",\"Mirtazapine\"),]\ndata_wide$trt &lt;- ifelse(data_wide$trt == 1, 0, 1)\ndata_wide$id &lt;- rep(1:nrow(data_wide))\ndata_wide$eq5d_1 &lt;- data_wide$eq5d0\ndata_wide$eq5d_2 &lt;- data_wide$eq5d13\ndata_wide$eq5d_3 &lt;- data_wide$eq5d39\n\n#long format\nlibrary(reshape)\ndata_long.eq5d&lt;-reshape(data_wide, varying = c(\"eq5d_1\",\"eq5d_2\",\"eq5d_3\"),\n                        direction = \"long\",idvar = \"id\",sep = \"_\")\ndata_long.eq5d$time_u &lt;- data_long.eq5d$time\ndata_long.eq5d&lt;-data_long.eq5d[,c(\"id\",\"trt\",\"time_u\",\"eq5d\",\"eq5d0\",\"qaly\")]\n\nNext, I previously computed individual-level QALYs (\\(e_i\\)) in each group by aggregating the utilities over the duration of the analysis, i.e. 1 year, using the AUC formula (see post thumbnail):\n\\[\ne_i = \\sum_{j=1}^{J}\\frac{u_{ij-1} + u_{j}}{2} \\times \\delta,\n\\]\nwhere the subscript \\(i\\) and \\(j\\) denote the individual and time indices, while \\(\\delta\\) is the portion of time covered between each successive pair of measurements. Since these measures are assumed to be collected at 6 months intervals, then in our case \\(\\delta=0.5\\). At this point I have all the data I need to perform a regression analysis and try to estimate the mean QALYs in each group, adjusting for baseline values. The simplest way to do this is to fit a linear regression model at the level of the QALY variable and then include treatment to obtain estimates of unadjusted mean QALYs. If I also add \\(u_{i0}\\) as a covariate into the model, then I obtain adjusted mean estimates. The model is:\n\\[\ne_i = \\boldsymbol \\beta \\boldsymbol X_i + \\varepsilon_i,\n\\]\nwhere \\(\\boldsymbol \\beta\\) is the vector of regression parameters, while \\(\\boldsymbol X_i\\) is the matrix of predictors in the model (including an intercept, trt and \\(u_{i0}\\)).\n\n#perform the analysis\nlibrary(emmeans)\nlibrary(nlme)\nlibrary(lme4)\n\n#linear regression for QALYs (focus on complete cases for simplicity)\ndata_wide$trtf &lt;- factor(data_wide$trt)\ndata_wide.cc&lt;-data_wide[!is.na(data_wide$qaly),]\n\nols.cc.wide.qalys&lt;-lm(qaly ~ trtf + eq5d0,data = data_wide.cc)\nci_ols.cc_qalys.means&lt;-emmeans(ols.cc.wide.qalys,~trtf+ eq5d0)\nprint(ci_ols.cc_qalys.means)\n\n trtf eq5d0 emmean     SE  df lower.CL upper.CL\n 0    0.718  0.553 0.0152 103    0.523    0.583\n 1    0.718  0.582 0.0164 103    0.549    0.614\n\nConfidence level used: 0.95 \n\n\nSo far so good right? well now the problem pops up. It is generally known that, when some missing utility data occur, then it is more efficient (in the sense of using more information) to fit the model at the longitudinal level, i.e. at the level of the utility scores rather than at the QALYs level. In this was information from partially-observed cases will be used in the model when deriving the estimates for the mean utilities at each time, which can then be combined via the AUC formula to obtain the final QALY mean estimates. Here for simplicity we fit this longitudinal model even without any missingness. Although there is not much literature about this type of approach, let’s say that we want to fit a linear mixed-effects model to our data and then combine the model parameter estimates to derive the final estimates of interest. The model can be specified by including treatment, time, their first order interaction, and baseline values to derive the adjusted mean estimates.\n\\[\nu_{ij} = \\boldsymbol \\beta \\boldsymbol X_i + \\omega_i + \\varepsilon_{ij},\n\\]\nwhere \\(\\boldsymbol \\beta\\) is the vector of fixed effects, while \\(\\boldsymbol X_i\\) is the matrix of predictors in the model. This includes: an intercept; trt; \\(u_{i0}\\) and time, which is often expressed as a dummy-coded variable (with reference time \\(j=0\\)), and the interaction between trt and time. Finally, \\(\\omega_i\\) is the random effects term.\n\n#perform the analysis \ndata_long_cc &lt;- data_long.eq5d[!is.na(data_long.eq5d$qaly),]\ndata_long_cc$trtf &lt;- factor(data_long_cc$trt)\ndata_long_cc$timef_u &lt;- factor(data_long_cc$time_u)\n\n#mixed model for utilities (focus on complete cases for simplicity)\ncgm3_u_ml.cc&lt;-lme(eq5d ~ timef_u * trtf + eq5d0, random = ~ 1 | id, data=data_long_cc, method = \"ML\",na.action = na.omit)\n#derive mean utilities\nem3_u_ml.cc.eq5d&lt;-emmeans(cgm3_u_ml.cc,~timef_u * trtf)\n#derive mean QALYs as linear combination of mean utilities\ncgm3_u_ml.cc.qalys &lt;- contrast(em3_u_ml.cc.eq5d,list(mue1 = c(13/104,13/104 + 26/104,26/104,0,0,0), mue2=c(0,0,0,13/104,13/104 + 26/104,26/104)))\nci.cgm3_u_ml.cc.qalys &lt;- confint(cgm3_u_ml.cc.qalys)\nprint(ci.cgm3_u_ml.cc.qalys)\n\n contrast estimate     SE  df lower.CL upper.CL\n mue1        0.555 0.0126 103    0.530    0.580\n mue2        0.580 0.0136 103    0.553    0.607\n\nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\nDo you see the issue? the derived mean QALYs for both groups do not exactly match those obtained from the linear regression fitted to the QALYs despite the fact that the data used are the same, i.e. complete cases. This is a bit odd. However, what happens when I run the adjusted analysis including the interaction between time and baseline utilities alongside the main effects of \\(u_{i0}\\) ?\n\\[\nu_{ij} = \\boldsymbol \\beta \\boldsymbol X_i + \\omega_i + \\varepsilon_{ij},\n\\]\nwhere \\(\\boldsymbol \\beta\\) is the vector of fixed effects, while \\(\\boldsymbol X_i\\) is the matrix of predictors in the model. This includes: an intercept; trt; \\(u_{i0}\\) and time, which is often expressed as a dummy-coded variable (with reference time \\(j=0\\)), the interaction between trt and time, and also the interaction between time and \\(u_{i0}\\). Finally, \\(\\omega_i\\) is the random effects term.\n\ncgm3_u_ml.cc2 &lt;- lme(eq5d ~ timef_u * trtf + timef_u*eq5d0, random = ~ 1 | id, data=data_long_cc, method = \"ML\",na.action = na.omit)\n#derive mean utilities\nem3_u_ml.cc.eq5d2 &lt;- emmeans(cgm3_u_ml.cc2,~timef_u * trtf + timef_u*eq5d0)\n#derive mean QALYs as linear combination of mean utilities\ncgm3_u_ml.cc.qalys2 &lt;- contrast(em3_u_ml.cc.eq5d2,list(mue1 = c(13/104,13/104 + 26/104,26/104,0,0,0), mue2=c(0,0,0,13/104,13/104 + 26/104,26/104)))\nci.cgm3_u_ml.cc.qalys2&lt;-confint(cgm3_u_ml.cc.qalys2)\nprint(ci.cgm3_u_ml.cc.qalys2)\n\n contrast estimate     SE  df lower.CL upper.CL\n mue1        0.553 0.0125 103    0.528    0.578\n mue2        0.582 0.0135 103    0.555    0.608\n\nDegrees-of-freedom method: containment \nConfidence level used: 0.95 \n\n\nTa da! the estimates now perfectly coincide. It turns out that when fitting this longitudinal model for the utility, it is important that an interaction between time and baseline utilities is included in the model to match the adjusted estimates that would be obtained via standard linear regressions fitted at the QALY level.\nI am not totally convinced of why this is the case but perhaps it has something to do with the fact that baseline utilities are used as outcome and covariate at the same time in both types of models ? I need to study this in more detail."
  },
  {
    "objectID": "posts/2021-08-02-my-blog-post/index.html",
    "href": "posts/2021-08-02-my-blog-post/index.html",
    "title": "Holidays, finally",
    "section": "",
    "text": "Holidays! Yes, really holidays!\nThis year has been particularly difficult and hard for many people and I am no exception. With the moving to Maastricht, the need to familiarise with a new working and living environment, the lockdown measures, the new teaching and administrative tasks, and mnay other little things, I arrived at the end of the academic year almost annihilated. Luckily, I am now on annual leave and back in Italy with my family who I really missed throughout the past year. Begin together in flesh and blood is a really nice feeling, something that online calls cannot replace. I really needed a break from the usual routine and this vacation is just perfect in order to recover my energies and be ready for the upcoming academic year. I was also lucky enough to received both my vaccinations before leaving the Netherlands as otherwise I could have troubles when traveling between countries.\nSo, not mush to say apart that I will be taking these holidays as a chance to put together some research ideas for next year. From thinking about how to plan a new grant application, to prepare myself for attending future courses in relation to learning Dutch, new teaching modules, the teaching qualification certificate, and of course some nice research papers. Speaking of the devil, I flash out my most recently published paper in MDM in the area of partitioned surivival CUA modelling, which has now been officially released. If you are interested in the topic, check it out!\nFinally, I would also like to give a brief review of a textbook which I really enjoyed reading. I consider this the masterpiece of one of my top philosophical authors, David Hume. It is entitled Dialogues Concerning Natural Religion and is one of the latest works of the Scottish philosopher written, as the title suggests, in the form of dialogues among three people who talk about many interesting and general concepts. From the nature of God’s existence, good and evil, and the role of religion in society. It is truly one of the most incredible and fascinating philosophical debate I have ever read in that it touches subjects in which I am really interested. This was the first “real” philosophy textbook I read after my high school readings and it was stuck in my mind for a long time due to its well-done structure and reasoning. It can be hard to follow at times, especially if you are not used to how these types of texts are written, but I can surely say that it is worth it. Perhaps, I will refer to some of my other top philosophy readings in my future posts, just in case someone might be intrigued by them and would like to give them a chance.\nAnyway, that is all from me for now. Now time for some vacation and relax in my warm, humid and very polluted “Pianura Padana” in Italy which, however, I love very much and I am super happy to see again. Tot September folks!"
  },
  {
    "objectID": "posts/2021-06-15-my-blog-post/index.html",
    "href": "posts/2021-06-15-my-blog-post/index.html",
    "title": "Too hot",
    "section": "",
    "text": "This month is extremely busy and I seriously need a week of days made by 34 hours each in order to avoid any delay. Apart from the usual teaching which is finally coming close to the end of this period (hopefully by the end of the month) but which kept me busy at least 3 days per week, there have been lots of meetings and work to do. Add the fact that in the last few weeks people here were going crazy for a very hot weather (I think they are not used to such “high” and constant temperatures of 30 degrees Celsius), then you can understand how much willingness I must possess in order to keep up with everything. Not that I am complaining about having lots of work to do, rather it is something which I have always found to be exciting, but having everything squeezed into a couple of weeks is not exactly very healthy.\nA very quick recap of the most important things I have done include the followings. First, lots of consultation for medical students who are now close to the deadline for the submissions of their theses and internship reports. I think I got at least 2/3 requests per week which is quite a lot and given that students are afraid of learning anything that has to do with a statistical analysis, patience will be an essential virtue that I must show off in order to survive the upcoming days. Wish me all good luck ! Second, alongside with teaching statistics in a course I get lots of emails from the students on different parts of the course. This is not something that really bothers me expect when their questions are all about what type of questions there will be in the exam rather than “I would like to understand better this concept” which can be a bit frustrating from time to time. Again part of the job, so I can complain as much as I want but it is something I really need to do. Third, I have given my first invited talk at my department of methodology and statistics and it has been a very interesting and nice experience. I think I overdid it since I wanted to give a nice overview of what my research is about but I ended up putting too many slides for a 1 hour presentation. This hurt a bit the pace of my talk since I had to speed up in certain points and I felt that many people were lost, especially towards the end. It was very nice to introduce my work to my colleagues and receive their feedback on some parts of the work as well as suggestions to explore in the future. I hope next time I will be able to adjust the presentation based on the actual time given to me so to make sure people will not feel disoriented/bored (at least not too much). Finally, next week I will also give another invited talk about my missingHE package at the R-HTA summer workshop. This annual conference is specifically directed towards people who would like to use R in health economic analyses and I hope I can capture the attention of someone who might decide to try out my package on their own analyses. I still have to finish the presentation but since this time I only have 15 minutes I need to make sure to not talk for more than 1 hour!\n\n\n\n\n\nI am also trying to work on my research but the available time is very little so I have made basically no progress. Next month I will have some break from my teaching duties so I hope I will be able to focus on my work. However, another big enemy is approaching who may threaten my ability to complete this task: sunny waether. Now more than ever I would be happy to have a fresh July which would make my life much easier. Let’s see what it will be.\nOh by the way, did I perhaps mention that my new article has been published already? check it out!"
  },
  {
    "objectID": "posts/2021-04-15-my-blog-post/index.html",
    "href": "posts/2021-04-15-my-blog-post/index.html",
    "title": "Going back to teaching, hurray!",
    "section": "",
    "text": "Hello everbody and it is good to be back after a nice and cozy Easter break. As probably most of you, I was too forced to spend my Easter holidays away from my family this year but at least here in the Netherlands the weather was pretty nice during the Easter weekend and I was able to enjoy a nice walk through the city center of Maastricht which was an amazing experience. Even though the week after it started snowing for some absurd reason, I really needed this short break from my teaching duties at the university to recharge my batteries.\nI also took this chance to going back to some old projects that needed my attention and was able to move them forwards a bit with some extra work but that is fine since I love my research. Now, I am ready again for some new teaching and consultancy in the upcoming two months and, hopefully, these will be the last official commitments for this academic year. In addition, going towards summer time brings some new fresh air and hope for a period of the year with fewer restrictions and the chance to engage with some other human beings. It is not that I am a huge fan of interacting with people but it is definitely necessary to the human nature to avoid becoming crazy. I also hope I will soon be able to travel back to Italy and see my familiy and friends which I have not had the chance to see in person from more than one year ago. Anyway, sorry about all the wining and let’s get back to the actual important news.\nFirst, I am happy to announce that my paper on Bayesian methods for modelling data in survival-partitioned cost-utility analyses has been accepted for publication by the journal Medical Decision Making and in the upcoming months it should be officially published as an open access paper so that everyone who has an interest on the matter can have access to the paper. In the meantime, you can also see a past version of the paper on my ArXiv which in any case is pretty close to the final version, I think. I am exremely happy to publish to solo work on which I spent lots of time and effort and which represents a nice application of Bayesian methods as a comprehensive modelling framework for handling different types of statistical issues that usually affect health economic data. Nice!\n\n\n\n\n\nSecond, I am considering submitting an abstract to the R-HTA 2021 workshop which this year will take place in Dublin, at the Trinity College Dublin - see picture below, between July 1-2. This is a very interesting opportunity to spread my work as this annual conference invites people who have different levels of skills with R to present their projects, especially in the form of R packages, to support the statistical and health economic analyses within the international HTA environment, from applicative to methodological works. It could be a very nice spot where to promote my own package for handling missing HTA data, missingHE, as well as to receive some nice feedback for experts on how to further improve its structure/functions. I would also love to visit Dublin as I never had the chance. Unfortunately this may not be possible as it depends on the current and future restrictions that are imposed and tht may hinder my ability to travel even in the summer. Having this as an online thing is also another option, although not really exicting.\n\n\n\nTrinity College Dublin\n\n\nAnyway, now stop talking and let’s go back to work as I need to do some teaching before anything else!"
  },
  {
    "objectID": "posts/2021-02-15-my-blog-post/index.html",
    "href": "posts/2021-02-15-my-blog-post/index.html",
    "title": "Doing some teaching…",
    "section": "",
    "text": "Hello everybody, it is time for some quick updates about myself and what I have been doing the past month. Well, essentially, I have been crazy busy doing lots of teaching on statistics-related subjects, which is the primary reason I was hired here in Maastricht. The students are mostly from undergraduate medical or health science programmes and therefore the level of statistics that I need to teach them is rather basic, although the crucial aspects of data analysis is very well-treated in the courses. So far I had a wide variety of students with different backgrounds and from my first impressions it seems that those who are younger, say in their first undergraduate year, have shown more interest and effort to learn compared with their older colleagues. However, I have been teaching only two courses here at UM and many more are coming up in the next few weeks/months. The statistical concepts covered are the usual frequentist ones, such as p-values, confidence intervals, regression analysis, etc… So, nothing incredible but since I was a student too some years ago I can related with some of the questions they have about these concepts, especially given that statistics is not exactly their main programme focus (although it is essential to learn it!).\nOf course, since we are living in this moment, all teaching is done online and this makes things a little more difficult sometimes but in general I think I managed ok for answering the students’ questions and providing them with feedback on their exercises and group works. It is quite annoying that this being my first time teaching these courses (already prepared by other colleagues) I have to spend a decent amount of time to prepare all the lectures and be ready to the many possible questions the students may have on each topic. This, however, was expected from the beginning and it is part of the job so I cannot complain too much!\n\n\n\n\n\nApart from my rumblings about teaching duties killing me, I have some other good news on my research activity, finally! I am happy to announce that my paper on longitudinal models for dealing with missing data under MAR in trial-based cost-effectiveness analysis is in press by the journal Value in Health and can be accessed at the following link. I am really happy about this publication as this took lots of time and effort and seeing this article being published is something that encourages me a bit as a reminder that all the efforts done has now repaid. Although joint longitudinal models are not exactly a novelty in statistics or even health economics, the way I present them here is kind of unique in that I do it from a Bayesian perspective (who would have guessed?) and I compare the performance of the method with respect to other more standard missing data appraoches such as complete case analysis, single and multiple imputation. The key message is that any method which discards some observed data is inevitably less efficient and more prone to bias compared with methods that do not ignore these data. Thus, given the characteristics of trial-based analysis, it is recommended that longitudinal models, which properly account for the time dependence between outcomes and use information from all collected observations, are used to minimise the impact of missingness assumptions on the conclusions of the study. These models can also be fitted using multiple imputation although, in my perspective, the main issue of that is that results must then be combined with bootstrapping in order to quantify the impact of uncertainty on the conclusions (while this is not necessary in a Baysian analysis!).\nHopefully, analysts will find the methods I propose in this article interesting and useful and they will agree that using simple but likely biased methods should not be the optimal choice when approaching to a trial-based analysis. Ok, I think that is all for the moment and it is time to go back to my teaching and (when I have time) researching!"
  },
  {
    "objectID": "posts/2020-12-20-my-blog-post/index.html",
    "href": "posts/2020-12-20-my-blog-post/index.html",
    "title": "It is Xmas again Yeah",
    "section": "",
    "text": "It is Xmas again! Wow, how quickly time flies. The situation here in the Netherlands is not ideal as the number of infected is on the rise again and the government has declared a full lockdown until January 19th. Starting from next month I will start teaching in the department but everything will still be online due to the uncertainty of the pandemic. It is unfortunate but there is nothing we can do about it and let us just hope the situation will improve in the next few weeks/months.\nAside from sad stuff, I have some good news about my research. My paper about longitudinal models in trial-based CEA, whose pre-print is available on my ArXiv account, has been officially accepted for publication in Value in Health, after a very long time (more than 2 years of peer-review process I believe!). I am really happy as this is a paper for which I spent a lot of time and effort, so it is nice to see some ouput out of it. I am also glad to announce that a new paper I am co-authoring with Baptiste (LSHTM) and Catrin (Bangor University) which is a tutorial on the use of mixed effects models for trial-based CEAs. The main idea is to make available and spread awareness in the HE community about the possibility to use these models to derive CEA results, rather than relying on the standard linear regression models. The methods are nothing special but we believe health economists are not well aware of the fact that sometimes they can be implemented in a much easier way compared to standard approaches. We tried to summarise the main advantages and disadvantages of the methods while also providing software code (in STATA and R) to show how they can be fitted. The paper has been accepted as a contributed presentation at the next HESG meeting. Unfortunately, I will not be able to attend the meeting as I will be quite busy with the teaching that week but Catrin has kindly agreed to present in behalf of all authors. I look forward to receive the feedback from the people attending the conference!\nThe past month I have been quite busy with some teaching and consultancy work here at UM, mostly for medical students about basic statistical concepts and techniques but nonetheless very interesting for me in order to get acquainted with the new job and duties. A big thank is due to my new colleagues who have been incredibly nice to me and have helped me a lot to get into the system. I was actually planning to post my news earlier this month but with incredible disappointment I found out that the most recent updated of Hugo (the system used by blogdown for building this site) completely messed up my previous version of the website which was broken. I had to rebuild the website again copying and pasting all my previous material. Nothing crazy, but certainly very annoying to do. I just hope the next update will not force me to do it again! So, if you see something different compared to before, you know why (especially in the sections for the tutorials on using BUGS/JAGS/STAN). Related to this, I must acknowledge Mr. Kim who recently contacted me with regard to an incorrect specification for one of the STAN models in the tutorial on generalised linear mixed models. I would like to thank him again very much for noticing this mistake which I fixed in the new version of the website now online (apologies for the long time it took for me to make the changes!). I am happy to see that my code can be of help to anyone who might be interested in doing some nice modelling.\nFinally, a quick note about somehting I found curious. A medical statistician who I deeply respect and admire, Dr.Tim Morris from MRCCTU, posted the following provocative tweet\n\nwhere he asks about the interpretation of a Bayesian credible interval while also saying that quick and simple answers will not be considered reliable. Well, I have much to say on this but I feel like twitter is not the best location to try and argument a proper discussion. I hope I will be able to find some time to post on my website a more constructive answer. The topic is not very quick to grasp, especially if someone is used to think in frequentist terms and theory which, by definition, are not very useful to think at statistics from a Bayesian point of view. For the moment, as a quick answer, I would just say that in my opinion statistics is not a uniquely defined discipline but there are different ways it can be approached, and the rules and theory of one approach do not necessarily apply to others!\nNext time, perhaps, I will follow this with a proper argument, stay tuned!"
  },
  {
    "objectID": "posts/2020-10-10-my-blog-post/index.html",
    "href": "posts/2020-10-10-my-blog-post/index.html",
    "title": "Why health economists do not care about statistical significance?",
    "section": "",
    "text": "Hello dear readers!\nI have finally come back from my lethargy with a new exciting posts about why the job of health economists, although inevitably involving some statistics, can be very different from what standard statisticians typically do. To make this point, I will abuse and give my own opinion of the notorious (and now quite old) paper published by Dr. Claxton (Claxton (1999)).\nBefore starting my discussion I would like to point out a very nice post made by Sam Watson on the Academic Health Economists’ Blog, who nicely provided his own opinion on the matter and discussed the pros and cons of the paper. I really recommend people interested in knowing more about health economics to check out this blog which is one of the most popular in the field and which has contributions from many many health economists on different topics.\nIn this post, in contrast to what Sam did in his own, I will not focus on the technical details about the health economics decision-making problem of minimising the expected loss / maximising the utility function in terms of societal preferences (i.e. social welfare in terms of effectiveness and costs). Instead, here I would like to address the much simpler and basic question why health economists do not use statistical significance in their analyses?. In my own experience, especially in trial-based analyses, I had to work closely with both health economists and statisticians who often are unaware of what the other “people” are doing. Since cost-effectiveness analyses have become more and more important over the last decade, I believe it is imperative that even statisticians should learn at least the basics of what their colleagues are doing and most importantly why. Of course, the same applies for health economists who should know a bit of statistics in order to do their job. However, in general, I have the feeling statisticians are reluctant to care about health economics, perhaps due to different statistical methodology required for these analyses compared with standard methods used for the analysis of clinical outcomes.\nApologies for the very long premise. So, let’s start with the most basic question of all. what is the objective of health economics ? To inform decision-makers about the cost-effectiveness of a given treatment with respect to alternatives. Unlike the standard clinical analysis which focuses merely on detecting whether there is a “clinically relevant” difference between treatments in terms of some pre-defined outcome measure, the health economic analysis does not qualify as a mere yes/no problem due to the existence of opportunity costs. These are the losses, either in terms of forgone benefits or extra costs, that the society/health care provider would incur by funding a treatment which is not cost-effective compared to others. Precisely, the decision-making nature of the problem comes from the fact that only a limited amount of resources are available to decision-makers, who need to define some sort of rule to prioritise the distribution of the funds across a pool of possible treatments. Thus, if the objective is to maximise the health benefits for a given budget, then treatments should be selected based on a target quantity, which summarises effectiveness, costs and the preferences of decision-makers, while also taking into account the uncertainty about our conclusions. This quantity takes the name of incremental net benefit and is given by\n\\[\n\\text{INB} = K \\Delta_e - \\Delta_c,\n\\]\nwhere \\(\\Delta_e\\) and \\(\\Delta_c\\) are the mean differences between two competing interventions in terms of some effectiveness and cost measures, while \\(K\\) is the acceptance threshold representing the budget the decision-maker is willing to spend to obtain an increment of one unit of effectiveness (cost per unit of effectiveness gained). If we were to evaluate an hypothesis test about cost-effectiveness of treatment 2 vs treatment 1, we would then test: \\(\\text{INB} \\leq 0\\) (\\(H_0\\)) vs \\(\\text{INB} &gt; 0\\) (\\(H_1\\)).\nA natural test statistic for this hypothesis is:\n\\[\nt = \\frac{\\sqrt{n}Kd_e-d_c}{\\sqrt{K^2 s^2_e -2Ks_{ce}+s^2_c}},\n\\]\nwhere \\(d_e\\) and \\(d_c\\) are the in-sample estimates of the corresponding population values, while \\(s\\) are the sample standard deviations. Under the null, this test statistic has a t-student distribution with \\(n-1\\) degrees of freedom. However, we can see how not rejecting the null hypothesis when a new treatment has a positive but statistically insignificant mean incremental net benefit imposes unnecessary costs which can be valued in either monetary or effectiveness terms. Decisions should be based only on the mean irrespective of whether any differences in this quantity are regarded as statistically significant. This is because one of the mutually exclusive alternatives must be chosen and this decision cannot be deferred. The opportunity costs of failing to make the correct decision based on a single test statistic are symmetrical while the definition of which of the alternatives is regarded as current practice is completely irrelevant."
  },
  {
    "objectID": "posts/2020-10-10-my-blog-post/index.html#so-how-do-you-make-the-decision-if-there-no-hypothesis-test",
    "href": "posts/2020-10-10-my-blog-post/index.html#so-how-do-you-make-the-decision-if-there-no-hypothesis-test",
    "title": "Why health economists do not care about statistical significance?",
    "section": "So how do you make the decision if there no hypothesis test ?",
    "text": "So how do you make the decision if there no hypothesis test ?\nAlthough the ordinary rules of statistical analyses do not apply, we still need to summarise the uncertainty associated with our results based on the mean INB given that decision-making problems must take into account and quantify the impact that sampling uncertainty has on our conclusions. This is typically achieved through the use of re-sampling methods, such as bootstrapping, (frequentist approach) or Bayesian methods. The two approaches are very different from a theoretical perspective but I will not address this point to save me of few weeks of time. Here, I am most interested in what type of results we will obtain once applied these methods. Well, typically we will look at something like this:\n\n\n\nCost-Effectiveness Plane"
  },
  {
    "objectID": "posts/2020-08-07-my-blog-post/index.html",
    "href": "posts/2020-08-07-my-blog-post/index.html",
    "title": "What is Bayesian inference?",
    "section": "",
    "text": "What is probability? The answer to this question is generally acknowledged to be the one that respects the so called Kolmogorov axioms which can be brutally simplified to:\nOne of the ways in which Bayesian statistics differs from classical statistics is in the interpretation of probability. Differences in interpretation continue to be controversial, are critical to the distinction between Bayesian and non-Bayesian statistics.\nIn classical statistics probability is often understood as a property of the phenomenon being studied: for instance, the probability that a tossed coin will come up heads is a characteristic of the coin. Thus, by tossing the coin many times under more or less identical conditions, and noting the result of each toss, we can estimate the probability of a head, with the precision of the estimate monotonically increasing with the number of tosses. In this view, probability is the limit of a long-run, relative frequency; i.e. if \\(A\\) is an event of interest (e.g. the coin lands heads up) then\n\\[\n\\text{Pr}(A) = \\lim_{n\\rightarrow\\infty}\\frac{m}{n}\n\\]\nis the probability of \\(A\\), where \\(m\\) is the number of times we observe the event \\(A\\) and \\(n\\) is the number of repetitions. Given this definition of probability, we can understand why classicial statistics is sometimes referred to as frequentist and objectivist. However, historians of science stress that at least two notions of probability were under development from the late \\(1600\\)s onwards: the objectivist view described above, and a subjectivist view. With regard to the latter, we can consider different ‘degrees’ of belief to interpret probability, ‘from the very neighbourhourhood of certainty and demonstration, quite down to improbability and unlikeliness, even to the confines of impossibility’. For Locke, ‘Probability is likeliness to be true’, a definition in which (repeated) games of chance play no part. For Bernoulli, ‘Probability is degree of certainty and differs from absolute certainty as the part differs from the whole’, it being unequivocal that the ‘certainty’ referred to is a state of mind, but, critically, (1) varied from person to person (depending on one’s knowledge and experience) and (2) was quantifiable. Ramsey and de Finetti, working independently, showed that subjective probability is not just any set of subjective beliefs, but beliefs that conform to the axioms of probability. The Ramsey-de Finetti Theorem states that if \\(p_1, p_2, \\ldots\\) are a set of betting quotients on hypotheses \\(h_1, h_2,\\ldots\\) , then if the \\(p_j\\) do not satisfy the probability axioms, there exists a betting strategy and a set of stakes such that whoever follows this betting strategy will lose a finite sum whatever the truth values of the hypotheses turn out to be. In de Finetti’s terminology, subjective probabilities that fail to conform to the axioms of probability are incoherent or inconsistent. Thus, subjective probabilities are whatever a particular person believes, provided they satisfy the axioms of probability. Thus, if I do not update my subjective beliefs in light of new information (data) in a manner consistent with the probability axioms, and you can convince me to gamble with you, you have the opportunity to take advantage of my irrationality, and are guaranteed to profit at my expense. That is, while probability may be subjective, Bayes Rule governs how rational people should update subjective beliefs."
  },
  {
    "objectID": "posts/2020-08-07-my-blog-post/index.html#subjective-probability",
    "href": "posts/2020-08-07-my-blog-post/index.html#subjective-probability",
    "title": "What is Bayesian inference?",
    "section": "Subjective probability",
    "text": "Subjective probability\nBayesian probability statements are thus about states of mind over states of the world, and not about states of the world per se. Indeed, whatever one believes about determinism or chance in social processes, the meaningful uncertainty is that which resides in our brains, upon which we will base decisions and actions. This is why, in one of the more memorable and strongest statements of the subjectivist position, de Finetti writes probability does not exist: “The abandonment of superstitious beliefs about \\(\\ldots\\) Fairies and Witches was an essential step along the road to scientific thinking. Probability, too, if regarded as something endowed with some kind of objective existence, is not less a misleading misconception, an illusory attempt to exteriorize or materialize our true probabilistic beliefs. In investigating the reasonableness of our own modes of thought and behaviour under uncertainty, all we require, and all that we are reasonably entitled to, is consistency among these beliefs, and their reasonable relation to any kind of relevant objective data”.\nThe use of subjective probability also means that Bayesians can report probabilities without a “practically unlimited” sequence of observations. What is the frequentist probability of the truth of the proposition “Jackson was the eighth president”? Since there is only one relevant experiment for this problem, the frequentist probability is either zero (if Jackson was not the eighth president) or one (if Jackson was the eighth president). Non-trivial frequentist probabilities, it seems, are reserved for phenomena that are standardized and repeatable. Bayes Theorem itself is uncontroversial: it is merely an accounting identity that follows from the axioms of probability discussed above, plus the following additional definition.\n\nConditional probability. Let \\(A\\) and \\(B\\) be events with \\(P(B)&gt;0\\). Then the conditional probability of \\(A\\) given \\(B\\) is\n\n\\[\nP(A\\mid B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\nThe following two useful results are also implied by the probability axioms, plus the definition of conditional probability\n\nMultiplication rule\n\n\\[\nP(A \\cap B) = P(A\\mid B)P(B) = P(B\\mid A)P(A)\n\\]\n\nLaw of total probability\n\n\\[  \nP(B) = P(A\\cap B)+ P\\overline{(A\\cap B)} = P(B\\mid A)P(A) + P(B \\mid \\overline{A})P(\\overline{A})\n\\]"
  },
  {
    "objectID": "posts/2020-08-07-my-blog-post/index.html#bayes-theorem",
    "href": "posts/2020-08-07-my-blog-post/index.html#bayes-theorem",
    "title": "What is Bayesian inference?",
    "section": "Bayes theorem",
    "text": "Bayes theorem\nBayes Theorem can now be stated, following immediately from the definition of conditional probability. If \\(A\\) and \\(B\\) are events with \\(P(B)&gt;0\\), then\n\\[\nP(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B)}\n\\]\nIf we consider the event \\(A=H\\) to be an hypothesis and the event \\(B=E\\) to be observing some evidence, then \\(Pr(H\\mid E)\\) is the probability of \\(H\\) after obtaining \\(E\\), and \\(\\text{Pr}(H)\\) is the prior probability of \\(H\\) before considering \\(E\\). The conditional probability on the left-hand side of the theorem, \\(\\text{Pr}(H\\mid E)\\), is usually referred to as the posterior probability of \\(H\\). Bayes Theorem thus supplies a solution to the general problem of inference or induction, providing a mechanism for learning about the plausibility of a hypothesis \\(H\\) from data \\(E\\).\nIn most analyses in the social sciences, we want to learn about a continuous parameter, rather than the discrete parameters considered in the discussion thus far. Examples include the mean of a continuous variable, a proportion (a continuous parameter on the unit interval), a correlation, or a regression coefficient. In general, let the unknown parameter be \\(\\theta\\) and denote the data available for analysis as \\(\\boldsymbol y = (y_1, \\ldots , y_n)\\). In the case of continuous parameters, beliefs about the parameter are represented as probability density functions or pdfs; we denote the prior pdf as \\(p(\\theta)\\) and the posterior pdf as \\(p(\\theta \\mid \\boldsymbol y)\\). Then, Bayes Theorem for a continuous parameter is as follows:\n\\[\np(\\theta \\mid y) = \\frac{p(y \\mid \\theta) p(\\theta)}{\\int p(y \\mid \\theta) p(\\theta) d\\theta},\n\\]\nwhich is often approximated by\n\\[\np(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta),\n\\]\nwhere the proportionality constant is \\(\\left[ \\int p(y \\mid \\theta) p(\\theta) d\\theta \\right]^{-1}\\) which ensures that the posterior density integrates to one, as a proper probability density. The first term on the right hand side of the Equation is the likelihood function, the probability density of the data \\(y\\), considered as a function of \\(\\theta\\). This formulation of Bayes Rule highlights a particularly elegant feature of the Bayesian approach, showing how the likelihood function \\(p(\\boldsymbol y|\\theta)\\) can be “inverted” to generate a probability statement about \\(\\theta\\), given data \\(y\\). Thus, from a Bayesian perspective, likelihood based analyses of data assume prior ignorance, although seldom is this assumption made explicit, even if it were plausible. In other cases, when working with the so-called conjugate priors in the exponential family, the mean of the posterior distribution is a precision-weighted average of the prior and the likelihood. Suppose a prior density \\(p(\\theta)\\) belongs to a class of parametric of densities, \\(F\\). More specifically, the prior density is said to be conjugate with respect to a likelihood \\(p(y \\mid \\theta)\\) if the posterior density \\(p(\\theta \\mid y )\\) is also in \\(F\\).\nBayesian statistical inference is equivalent to combining information, marrying the information in the prior with the information in the data, with the relative contributions of prior and data to the posterior being proportional to their respective precision. That is, Bayesian analysis with conjugate priors over a parameter \\(\\theta\\) is equivalent to taking a precision-weighted average of prior information about \\(\\theta\\) and the information in the data about \\(\\theta\\). Thus:\n\nThus, when prior beliefs about \\(\\theta\\) are ‘vague’, ‘diffuse’, or, in the limit, uninformative, the posterior density will be dominated by the likelihood (i.e. the data contains much more information than the prior about the parameters);\nWhen prior information is available, the posterior incorporates it, and rationally, in the sense of being consistent with the laws of probability via Bayes Theorem. In fact, when prior beliefs are quite precise relative to the data, it is possible that the likelihood is largely ignored, and the posterior distribution will look almost exactly like the prior\n\nNote also that via Bayes Rule, if a particular region of the parameter space has zero prior probability, then it also has zero posterior probability. This feature of Bayesian updating has been dubbed Cromwell’s Rule by Lindley. The point here is that posterior distributions can sometimes look quite unusual, depending on the form of the prior and the likelihood for a particular problem. The fact that a posterior distribution may have a peculiar shape is of no great concern in a Bayesian analysis: provided one is updating prior beliefs via Bayes Rule, all is well. Unusual looking posterior distributions might suggest that one’s prior distribution was poorly specified, but, as a general rule, one should be extremely wary of engaging this kind of procedure. Bayes Rule is a procedure for generating posterior distributions over parameters in light of data. Although one can always re-run a Bayesian analysis with different priors (and indeed, this is usually a good idea), Bayesian procedures should not be used to hunt for priors that generate the most pleasing looking posterior distribution given a particular data set and likelihood. Indeed, such a practice would amount to an inversion of the Bayesian approach: i.e. if the researcher has strong ideas as to what values of \\(\\theta\\) are more likely than others, aside from the information in the data, then that auxiliary information should be considered a prior, with Bayes Rule providing a procedure for rationally combining that auxiliary information with the information in the data."
  },
  {
    "objectID": "posts/2020-08-07-my-blog-post/index.html#bayesian-updating-of-information",
    "href": "posts/2020-08-07-my-blog-post/index.html#bayesian-updating-of-information",
    "title": "What is Bayesian inference?",
    "section": "Bayesian updating of information",
    "text": "Bayesian updating of information\nBayesian procedures are often equivalent to combining the information in one set of data with another set of data. In fact, if prior beliefs represent the result of a previous data analysis (or perhaps many previous data analyses), then Bayesian analysis is equivalent to pooling information. This is a particularly compelling feature of Bayesian analysis, and one that takes on special significance when working with cojugate priors. In these cases, Bayesian procedures accumulate information in the sense that the posterior distribution is more precise than either the prior distribution or the likelihood alone. Further, as the amount of data increases, say through repeated applications of the data generation process, the posterior precision will continue to increase, eventually overwhelming any non-degenerate prior; the upshot is that analysts with different (non-degenerate) prior beliefs over a parameter will eventually find their beliefs coinciding, provided they (1) see enough data and (2) update their beliefs using Bayes Theorem. In this way Bayesian analysis has been proclaimed as a model for scientific practice acknowledging that while reasonable people may differ (at least prior to seeing data), our views will tend to converge as scientific knowledge accumulates, provided we update our views rationally, consistent with the laws of probability."
  },
  {
    "objectID": "posts/2020-08-07-my-blog-post/index.html#parameters-as-random-variables",
    "href": "posts/2020-08-07-my-blog-post/index.html#parameters-as-random-variables",
    "title": "What is Bayesian inference?",
    "section": "Parameters as random variables",
    "text": "Parameters as random variables\nOne of the critical ways in which Bayesian statistical inference differs from frequentist inference is that the result of a Bayesian analysis, the posterior density \\(p(\\theta \\mid y)\\) is just that, a probability density. Given a subjectivist interpretation of probabilty that most Bayesians adopt, the ‘randomness’ summarized by the posterior density is a reflection of the researcher’s uncertainty over \\(\\theta\\), conditional on having observed data. Contrast the frequentist approach, in which \\(\\theta\\) is not random, but a fixed (but unknown) property of a population from which we randomly sample data \\(\\boldsymbol y\\). Repeated applications of the sampling process, if undertaken, would yield different y, and different sample based estimates of θ, denoted \\(\\hat{\\theta} = \\hat{\\theta}(y)\\), this notation reminding us that estimates of parameters are functions of data. In the frequentist scheme, the \\(\\hat{\\theta}(y)\\) vary randomly across data sets (or would, if repeated sampling was undertaken), while the parameter \\(\\theta\\) is a constant feature of the population from which data sets are drawn. The distribution of values of \\(\\hat{\\theta}(y)\\) that would result from repeated application of the sampling process is called the sampling distribution, and is the basis of inference in the frequentist approach; the standard deviation of the sampling distribution of \\(\\hat{\\theta}\\) is the standard error of \\(\\hat{\\theta}\\), which plays a key role in frequentist inference. The Bayesian approach does not rely on how \\(\\hat{\\theta}\\) might vary over repeated applications of random sampling. Instead, Bayesian procedures center on a simple question: “what should I believe about \\(\\theta\\) in light of the data available for analysis, \\(y\\) ?”\nThe critical point to grasp is that in the Bayesian approach, the roles of \\(\\theta\\) and \\(\\hat{\\theta}\\) are reversed relative to their roles in classical, frequentist inference: \\(\\theta\\) is random, in the sense that the researcher is uncertain about its value, while \\(\\hat{\\theta}\\) is fixed, a feature of the data at hand."
  },
  {
    "objectID": "posts/2020-08-07-my-blog-post/index.html#conclusions",
    "href": "posts/2020-08-07-my-blog-post/index.html#conclusions",
    "title": "What is Bayesian inference?",
    "section": "Conclusions",
    "text": "Conclusions\nSo, we have seen a couple of interesting things about Bayesian statistics which people may not be aware of. First, Bayesian statistics is a scientific approach in that it provides a rational way to update subjective beliefs based on the available evidence through Bayes theorem which conforms the rules of probability. This ensures the scientific credibility of the posterior results while also providing a way to solve the inductive problem of learning from the data and update our belief about a parameter/hypothesis. Second, in contrast to the classical approach, Bayesian statistics do not rely on asymptotic results of a series of repeatable events in order to hold and therefore can be used to answer questions which do not have any meaning in the context of repeated events. Finally, Bayesian statistics sees any unknown quantity (e.g. parameters) as random variables and attach to them a probability distribution expressing the uncertainty around the estimates. Since the entire posterior distribution is derived based on Bayes theorem, this ensures correct propagation of uncertainty from the data and prior and does not require the additional step of classical statistics of deriving uncertainty measures in an “artificial way” or relying on asymptotic results.\nI hope this was a bit interesting for those who would like to get more familiar with the Bayesian philosophy and its underlying implications in terms of statistical assumptions and methods. Of course, being a Bayesian, this is the best way to go for me when doing an analysis and I would love to see more people embracing the Bayesian way as a new way of thinking statistics."
  },
  {
    "objectID": "posts/2020-06-05-my-blog-post/index.html",
    "href": "posts/2020-06-05-my-blog-post/index.html",
    "title": "New tutorials for missingHE",
    "section": "",
    "text": "Nothing major to report for the past month, mostly spent at home still in lockdown. A few offices and shops have already opened in London but all the public stuff, including my office at UCL will remain close until who knows when. So, in the meantime I put some work into prpearing some tutorials about how to use the different functions from my R package missingHE. These are built directly into the package in the form of vignettes which can be easily accessed from the R terminal once the package is installed locally.\nI have worked on three main tutorials dedicated to explain the basic functions of the package and to show how to customise the different models using different combinations of input choices. The three vignettes have each a specific target of users, starting from the beginners in using R to those who would like to have a more flexible specification of the models based on different modelling assumptions. I use the built-in dataset in the package, the MenSS study, to give practical examples of how the different changes to the models may affect the results in a standard analysis.\nThe three tutorials are:\n\nIntroduction to missingHE, which is intended for those who have little familiarity with R and just want an overview of the different functions of the package, what they do and how to extract the relevant information from the fitted models.\nFitting MNAR models in missingHE, which is intended for those who already know about the main functions of the package and would like to explore more deeply how to perform sensitivity analysis to missing not at random assumtpions using the arguments of each function.\nModel Customisation in missingHE, which is intended for those who have already grasped the basic idea behind the different functions and would like to customise their models and not just stick with the default settings. Examples include how to specify random effects, different prior distributions and so on.\n\nI believe these tutorials provide a reasonable summary about the key elements for anyone who would like to use the package but is a bit uncertain about what he or she can actually do with the functions and to which extent customisation is possible. For the moment the vignettes are only available from my GitHub version of the package (1.4.1) and can be accessed by installing the package using the command\n\ndevtools::install_github(\"AnGabrio/missingHE\", build_vignettes = TRUE)\n\nand then typing\n\nutils::browseVignettes(package = \"missingHE\")\n\nNote that you need to locally install the packages devtools and utils to access their functions. As soon as I have a bit of time I will update the version on CRAN to make them available from there as well. I spent a bit of time creating these tutorials and I hope people will find them useful to understand the package. In case anything is still unclear, feel free to contact me to ask questions.\nThat is pretty much it for the moment from me. I should add new tutorials for using JAGS and STAN on the website but time is always never enough. Naaah, I am just very lazy these days."
  },
  {
    "objectID": "posts/2020-04-20-my-blog-post/index.html",
    "href": "posts/2020-04-20-my-blog-post/index.html",
    "title": "New updates for missingHE",
    "section": "",
    "text": "In spite of how incredibly busy I am at the moment, which is also weird considering the whole lockdown situation still going on, I managed to upload a new version (1.4.0) of my R package missingHE with exciting updates!\nFor those who do not know, missingHE is specifically designed to implement Bayesian models for the analysis of trial-based economic evaluations and provides different methods to handle missingness in either or both the effectiveness and cost outcomes. The cool new things in this version are the following:\n\nFirst, random effects can now be specified for each model implemented in missingHE (I know, Bayesians should not talk about “random” or “fixed” effects as we know that there are no real “fixed” effects but the terms have become quite popular and many people would prefer this way). These include selection, hurdle and pattern mixture models. The package allows a flexible implementation of either random intercept only, random slope only and both random intercept and slope models based on the input given by the user. The random effects term is specified via the formula \\(y \\sim x + (x \\mid z)\\) where x is a covariate included also as a fixed effects in the model and z is the clustering variable over which the random effects for x are specified. It is possible to remove the random intercept if desired by adding 0 + inside the brackets (by default this is included).\nSecond, new types of posterior predictive checks can now be chosen using the function ppc for each type of model fitted using the function of the package. These include plotting the Bayesian posterior p-values (which should not be confused with the usual p-values as they are completely different) based on the posterior replications of the models and a given statistics computed from the observed data. The statistic can be provided by the user under the form of a univariate function (e.g. mean or sd) or a specific type of bivariate function (e.g. cor).\nThird, a new generic function called coef has been added which allows to extract the regression coefficients from each type of model, either in terms of fixed effects or random effects (if specified).\n\nI am quite proud of this new update as it is something I considered for a long time which is now available. If even one person find this useful, I think it will be worth all my effort. Very nice.\n\n\n\n\n\nOh, and yes you can also find the new version of missingHE on my GitHub page. I plan to upload a more serious tutorial on how to use all the functions of the package at some point (hopefully not too far from now).\nSo, now that all the fun part is done, I need to go back to doing meetings, reviews, writing papers, etc … It will be a quite busy period again but now I feel motivated. Let’s see for how long this will last."
  },
  {
    "objectID": "posts/2020-03-20-my-blog-post/index.html",
    "href": "posts/2020-03-20-my-blog-post/index.html",
    "title": "Living and working at home is nice, right?",
    "section": "",
    "text": "It has been roughtly a week and a half now since this whole shutdown started here in London and things are not going to be easy in the next few weeks. I am lucky, in that my job allows me to work remotely with limited inconveniences. Other people have to go outside for working and, if not risking thier life, at least put at risk the life of those who they care most. Last week was particularly bad in terms of supermarket products which were sold out for the most part. This week is a bit better as people may have realised that for the moment, if we just buy products as usual, we still have food and toilet paper for everyone.\nTo be honest, not much to update on my work which has slowed down due to this whole situation and also to me not feeling at my best. I hope I will have some time to look at the different projects I am involved with in the next few days. In the meantime, I worked a bit on my website with new JAGS and STAN tutorials and I have also uploaded on my GitHub page some materials (e.g. software code) related to some of the projects I did. For example, here the link to the JAGS and STAN code for the model I used to predict volleyball results\nNot sure what gif to use this time to conclude the post. So I guess I will just go for a random cat picture, which does not make any sense but which is always nice to look at."
  },
  {
    "objectID": "posts/2020-02-10-my-blog-post/index.html",
    "href": "posts/2020-02-10-my-blog-post/index.html",
    "title": "Let us do some work",
    "section": "",
    "text": "The moment for the second edition of the HEART’s one-day introductory course to health economics arrived at last! The course, led by Rachael Hunter and called “Understanding health economics in clinical trials”, took place on Tuesday 11 February and was prepared in collaboration between the HEART group and the Institute of Clinical Trials and Methodology (ICTM). I believe this second edition of the course was a success both in terms of the quality/quantity of the material covered during the six sessions throughout the day, as well as in terms of the positive feedback we received from the participants. Also, this time a new HEART member (Marie) joined the group and very nicely delivered the session about patient reported outcome measures (PROMs), engaging in nice discussions with the audience.\nFinally, a couple of personal notes.\n\nI recently attended a very interesting meeting about missing data methodology which was held by an international group of very talented senior and junior statisticians from different universities, including people like Ian White and James Carpenter from UCL and the LSHTM. It was really an amazing experience to meet so many people working in different stats area but with a common passion about missing data methods (also mine!). From what I understood this series of meetings (called “MiDIA”) have been held since years but do not have a very regular schedule due to people being busy I guess, which makes totally sense. Not sure when the next one will be held but now I am definitely looking forward to the next meetings!\n\n\n\n\n\n\n\nI would also like to highlight a recent tweet from UCL PRIMENT CTU, which advertises a new position as health economist in our HEART group for performing health economics using data from clinical trials. I would encourage anyone interested in some good applied health economic work to apply for this position. Deadline 15 March 2020.\n\n\n\n\n\n\n\nTo conclude, I would also like to say that I have done some updates to this website. From the inclusion of new tutorials on the use of JAGS and STAN on different statistical topics, to a restyle of the website. In particular I had fun by playing around with some Markdown code to add new features, e.g. customised alert notes and emoji, for example. Something like this:\n\n\n\n\n\n\n\n King’s note ! \n\n\n\nI  \\(\\LaTeX\\) very much.\n\n\nThis took me so much time but I am quite satisfied with the result if I may say so. You really never stop learning new things!"
  },
  {
    "objectID": "posts/2020-01-09-my-blog-post/index.html",
    "href": "posts/2020-01-09-my-blog-post/index.html",
    "title": "Let us do some work",
    "section": "",
    "text": "After the terrible start of this year, things are going ok now and I am quite busy with different projects that I left a bit behind. First, I can confirm that me and my colleagues from the HEART group are going to give an introductory course to health economic evaluations next month for different groups of people from academia and clinical trial units. The course has been generally structured based on our “pilot” we gave last year (which went really well by the way) and involves many different topics that will cover the entire day of February 11th. The attending list is already full and thw waiting list is also quite big; happy to see so much interest in economic evaluations.\nSecond, I will give a talk at the PRIMENT statistics and health economics and methodology seminar about an on-going project on missing data in trial-based analysis on Tuesday 28th, at UCL PRIMENT CTU. I am really happy to be back at these seminars which I feel I really nice and where you have the opportunity to interact with people from different backgrounds and job positions who may give some useful feedback on my work. Hopefully, people will find my research interesting!. I would also like to mention the fact that one of my HEART colleague, Marie, will give another talk at the same seminar just before me. Her topic is the economic analysis plan for a trial she has been involved with and I think she is really good, so may worth check her presentaiton out.\nThird, I have finalised a long-waited submission for a paper which has been discussed, written and re-written many times. I really hope we can get some useful feedback on it as I personally worked very hard to keep this work alive. Let see if my efforts have not been in vain and fingers crossed!\n\n\n\n\n\nFourth, as a side note, I have recently bought a new book on missing data called Semiparametric Thoery and Missing Data by Tsiatis, which looks very interesting. To be honest, the book is quite technical with many theoretical concpets and proofs which sometimes I find hard to follow. However, so far it gives a nice introduction to semiparametric models and I look forward to see how it approaches the missing data topic from a non likelihood-based approach. If you are into non/semiparametric statistics and want to find out more about this, I recommend the reading.\nFinally, more work is also coming up in the next weeks and some of this is not going to be very enjoyable, I think. Anyway, let us go through this busy period at our best and see how things will go."
  },
  {
    "objectID": "posts/2019-11-09-my-blog-post/index.html",
    "href": "posts/2019-11-09-my-blog-post/index.html",
    "title": "Too many things, again….",
    "section": "",
    "text": "I did not have much time to post anything this month until now as it has been a quite busy period. I have been involved in many different works and I have also involved other people in what I think could be some very interesting new projects. Not that I complain about having many different things to do (most of them are actually cool) but doing everything in a short period is not the best.\nA couple of things have come/are coming up. First, I have seriously started working on the coding of a decision model for some health economic evaluation project I have been involved in since last year. Everything seems ok after I spent lots of days and time fixing some small bugs in my code. I am about half way through the model and I hope I will be able to finish it before Christmas (I doubt it though).\nSecond, I have finished reviewing an interesting paper about some new methods for improving current practice for dealing with missing data, which I kinda enjoy reading (very good!).\nThird, I would like to quickly summarise my first experience at ISPOR Europe in Copenhagen. I was really excited to attend this conference which, as expected, revealed itself as huge with people coming from all over the world and with many interesting sessions and discussion topics. I had the chance to meet new and old people, such as professor Andrea Manca and the always very kind Chris Sampson for whom I was like a stalker asking for more and more information about himself and his work. I also met some of my old collegues from MapiGroup, now under ICON plc. It was very fun to hang out with these old friends and see what they have been up to during this time. Among them, I gladly caught up with my dear friend Ryan Pulleyblank, now doing a PhD at the University of Southern Denmark. My poster was a success with (unexpectedly) many people stopping by and asking for more information on my work. I was genuinely surprised by this as ISPOR is mostly a conference dedicated to companies rather than academic works and networking. To sum up, it was a very nice and fun experience and despite the level of statistical methodology was not particularly high I enjoyed my time there and I also had the chance to visit Copenhagen for the first time.\nFinally, as a side note, I have found the time to upload on my arXiv page a nice application of Bayesian hierarchical models for the prediction of volleyball matches which I have been working on the past summer, taking inspiration from the work of Gianluca about predicting football macthes. I hope my work can turn out in something cool as well.\n\n\n\n\n\nThis is all for the moment but soon I will be heading back to another quite busy period for me. I hope this will be the last for some time, especially given that Christmas is coming and I would like to have some free time to properly enjoy this period, which I really like, even more than Christmas itself."
  },
  {
    "objectID": "posts/2019-10-01-my-blog-post/index.html",
    "href": "posts/2019-10-01-my-blog-post/index.html",
    "title": "More good news…",
    "section": "",
    "text": "I have got two news coming up. First, the paper I wrote with Michael and Gianluca on Bayesian methods for longitudinal data in trial-based economic evaluations has finally been published as early view on JRSSA. As I said in some earlier posts, I am super happy about this collaboration and I hope I can continue working on similar projects in the future.\nSecond, I will soon give a talk about this work at the ICTMC conference in Brighton, next Monday. This will be the first time at this conference and unfortunately I will only be able to remain around for one day as I need to go back to London pretty soon. I hope I will be able to enjoy my day at the conference, even though I will miss the talks of Baptiste and Alexina which are scheduled for the last day of the conference. I hope I can at least have a quick chat with them the day I am around.\n\n\n\n\n\nI am also excited to visit Brighton, since many people keep telling me that I should go and visit this sort of british version of “Rimini”. To be honest, I do not expect to find a nice weather, given that in this period it is raining a lot in London, but I hope I will be lucky and get the only sunny day of the week.\nFinally, I have started a rubric called missing data on my website, where I try to describe some of the most popular methods to handle missing data and to provide some references for anyone who could be interested in this field. I am really fascinated by statistical methods for dealing with missingness, perhaps because it was the main focus of my PhD, but I am eager to review different methods and see if I can find something really interesting. Of course, to complete this it will take more time, which I hope I will be able to find in the next months."
  },
  {
    "objectID": "posts/2019-09-15-my-blog-post/index.html",
    "href": "posts/2019-09-15-my-blog-post/index.html",
    "title": "Discussing my thesis",
    "section": "",
    "text": "I have been kindly invited by the amazing person Chris Sampson to talk about the work I inlcuded in my PhD thesis for his monthly rubric entitled “Thesis Thursday” on the The Academic Health Economists blog.\nI happily accepted Chris’s invitation as I believe this initiative is really interesting and represents a nice way for newly graduated PhD students to advertise their work while also giving the chance to people interested in health economics to read about some academic work which is typically freely available to everyone.\nHere you can find the full interview, which is not very long and resolves around 5 questions that Chris asked me about my work. I already new this blog but I have never had a proper chance to read through its posts carefully, which is a shame.\n\n\n\n\n\nI shall promise myself to try to check it more often from now on, using this interview as a nice motivation to do so. In fact, there are not many blogs around health economics matters (here a non-comprehensive list), among which The Academic Health Economists and Gianluca’s blog are my favourites.\nI hope I will be able to find some time to write some nice posts about some health economic applications of my work in the next future as this is still the most interesting field for me at the moment."
  },
  {
    "objectID": "posts/2019-07-03-my-blog-post/index.html",
    "href": "posts/2019-07-03-my-blog-post/index.html",
    "title": "HESG Summer Meeting 2019",
    "section": "",
    "text": "I have just come back form my first Health Economists’ Study Group (HESG) meeting, which this year was held at the University of East Anglia in the beautiful city of Norwich, south east of England, and where I presented some preliminary results from one of my on-going works. I have to say, it was a remarkable experience which I really liked thanks to a wonderful and welcoming environment. I had the pleasure to talk to many people from different research areas involved in health economics (both from academia and industry) and to see many different projects and works.\nI particularly enjoy the structure of the meeting, which requires some chair and discussant who have to present and discuss the paper of the authors, who are only allowed to provide some clarification if needed. At first I thought this structure of the sessions was strange, but after attending many sessions and experiencing this for my own paper, I feel that it is a very good way to encourage discussion about works from different people rather than just focussing on your own presentation. Plus, the weather and always sunny, it felt like Italy for a few days.\n\n\n\nThe beautiful Norwich’s cathedral\n\n\nOther nice people and colleagues from HEART and other UCL department came to HESG with me, including Caroline and Ekaterina (aka Katia), you can see them in thumbnail of this post. I was also pleased to meet Baptiste from LSHTM, who shares with me the interest in missing data methods for cost-effectiveness analysis and who presented some very nice work on that. I had the chance to give some feedback to him and he did the same for me. It felt so nice when we started discussing about some aspects of our analyses and after some minutes we simply lost track of time and everyone else disappeared. I also had the opportunity to talk about my work with the discussant of my session, Catrin Plumpton from the Centre for Health Economics and Medicines Evaluation, who gave me some nice feedback which I really appreciated, especially given her mathematical background.\nAn important contribution to the success of the meeting was also given by the wonderful organisation of the event, including an accommodation located very closely to the main building of the meeting, plenty of food provided during each day, a nice bus tour of the city and a wonderful conference dinner. I must thank all the people, who organised the event who were very extremely nice to us and who were always ready to help us for whatever need we had, with a special mention for Emma Mcmanus who was amazing.\n\n\n\n\n\nIn summary, everything was good. Well, almost. Going back to the works presented, as usual, the only less positive note that I would like to make is the almost total absence of Bayesian applications. Some authors mentioned that they used some popular Bayesian program, such as WinBUGS, but this was mainly related to the usual meta-analysis stuff which is pretty standardised. I hope next time I will be able to see more people going Bayesian as this is what I am."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Assistant Professor in Statistics   Department of Methodology and Statistics  Faculty of Health Medicine and Life Sciences  Maastricht University",
    "section": "",
    "text": "I am an assistant professor in Statistics in the Department of methodology and statistics of the Faculty of Health Medicine and Life Sciences at Maastricht University in the Netherlands.\nMy main interests are in Bayesian statistical modelling for cost-effectiveness analysis and decision-making problems in the health systems. During my PhD I have specifically focused on the study and adoption of Bayesian methods to handle missing data in health economic evaluations and to assess the impact of their uncertainty on the output of the decision-making process. My research area involves different topics: from systematic literature reviews, case study applications, survival analysis, meta-analytic methods, multilevel models and trial-based clinical and economic analyses.\n\n\n\nI am very interested in the analysis of longitudinal data, with a focus on different types of statistical methods to deal with missingness. My preferred statistical programming software and the one I am most familiar with is R/RStudio by far, but I do also possess a good knowledge of other software such as STATA and MATLAB. I am quite expert in the use of free open-source Bayesian software programs, such as JAGS and Stan.\nI have collaborated with the Statistics for Health Economic Evaluation research group in the Department of Statistical Science at UCL, which is mainly focused on the development and application of Bayesian methods for health economic evaluations. The group works in collaboration with academics from different institutions and its activities are aimed at providing advice to statisticians, health economists and clinicians working in economic evaluations.\nI have also collaborated with the Health Economics Analysis and Research methodology Team in the Institute for Clinical Trials and Methodology at UCL, working primarily with the members of the Priment Clinical Trials Unit. The group focuses on the development of methodological tools for the analysis of the economic components in randomised control trials across a wide range of clinical areas and is formed by a group of interdisciplinary and varied experience.\n\n\n\n\n\n\n King’s note ! \n\n\n\nI am a huge fan of RStudio and its tools, such as Rmarkdown and blogdown packages and Quarto, which are aimed at the construction of documents that combine text, R code and the output from the execution of that code: from html and pdf files to multi-page web sites and e-books (yes this website is written in Markdown and Quarto!). Oh, and I loves using \\(\\LaTeX\\) !\n\n\n\n\nInterests\n\nMissing Data\nBayesian Statistics\nHealth Economics\nLongitudinal Data\nStatistical Methods for Health and Medical Data\n\n\nEducation\n\n PhD in Statistics, 2019\n\nUniversity College London (UK)\n\n MSc in Statistics and Econometrics, 2015\n\nUniversity of Essex (UK)\n\n MSc in Applied Economics, 2014\n\nUniversity of Pavia (Italy)\n\n BSc in Economics, 2012\n\nUniversity of Pavia (Italy)"
  },
  {
    "objectID": "index.html#biography",
    "href": "index.html#biography",
    "title": "Assistant Professor in Statistics   Department of Methodology and Statistics  Faculty of Health Medicine and Life Sciences  Maastricht University",
    "section": "",
    "text": "I am an assistant professor in Statistics in the Department of methodology and statistics of the Faculty of Health Medicine and Life Sciences at Maastricht University in the Netherlands.\nMy main interests are in Bayesian statistical modelling for cost-effectiveness analysis and decision-making problems in the health systems. During my PhD I have specifically focused on the study and adoption of Bayesian methods to handle missing data in health economic evaluations and to assess the impact of their uncertainty on the output of the decision-making process. My research area involves different topics: from systematic literature reviews, case study applications, survival analysis, meta-analytic methods, multilevel models and trial-based clinical and economic analyses."
  },
  {
    "objectID": "index.html#research-and-work",
    "href": "index.html#research-and-work",
    "title": "Assistant Professor in Statistics   Department of Methodology and Statistics  Faculty of Health Medicine and Life Sciences  Maastricht University",
    "section": "",
    "text": "I am very interested in the analysis of longitudinal data, with a focus on different types of statistical methods to deal with missingness. My preferred statistical programming software and the one I am most familiar with is R/RStudio by far, but I do also possess a good knowledge of other software such as STATA and MATLAB. I am quite expert in the use of free open-source Bayesian software programs, such as JAGS and Stan.\nI have collaborated with the Statistics for Health Economic Evaluation research group in the Department of Statistical Science at UCL, which is mainly focused on the development and application of Bayesian methods for health economic evaluations. The group works in collaboration with academics from different institutions and its activities are aimed at providing advice to statisticians, health economists and clinicians working in economic evaluations.\nI have also collaborated with the Health Economics Analysis and Research methodology Team in the Institute for Clinical Trials and Methodology at UCL, working primarily with the members of the Priment Clinical Trials Unit. The group focuses on the development of methodological tools for the analysis of the economic components in randomised control trials across a wide range of clinical areas and is formed by a group of interdisciplinary and varied experience.\n\n\n\n\n\n\n King’s note ! \n\n\n\nI am a huge fan of RStudio and its tools, such as Rmarkdown and blogdown packages and Quarto, which are aimed at the construction of documents that combine text, R code and the output from the execution of that code: from html and pdf files to multi-page web sites and e-books (yes this website is written in Markdown and Quarto!). Oh, and I loves using \\(\\LaTeX\\) !\n\n\n\n\nInterests\n\nMissing Data\nBayesian Statistics\nHealth Economics\nLongitudinal Data\nStatistical Methods for Health and Medical Data\n\n\nEducation\n\n PhD in Statistics, 2019\n\nUniversity College London (UK)\n\n MSc in Statistics and Econometrics, 2015\n\nUniversity of Essex (UK)\n\n MSc in Applied Economics, 2014\n\nUniversity of Pavia (Italy)\n\n BSc in Economics, 2012\n\nUniversity of Pavia (Italy)"
  },
  {
    "objectID": "posts/2019-08-03-my-blog-post/index.html",
    "href": "posts/2019-08-03-my-blog-post/index.html",
    "title": "The P value fallacy",
    "section": "",
    "text": "Today, I would like to briefly comment an interesting research article written by Goodman, who provided a clear and exemplary discussion about the typical incorrect interpretation of a standard frequentist analysis in the field of medical research. I will now briefly summarise the main argument of the paper and then add some personal comments.\nEssentially, the article describes the characteristics of the dominant school of medical statistics and highlights the logical fallacy at the heart of the typical frequentist analysis in clinical studies. This is based on a deductive inferential approach, which starts with a given hypothesis and makes conclusions under the assumption that the hypothesis is true. This is in contrast with a inductive approach, which uses the observed evidence to evaluate what hypothesis is most tenable. The two most popular methods of the frequentist paradigm are the P value proposed by Fisher and the hypothesis testing developed by Neyman and Pearson.\nThe P value is defined as the probability, under the assumption of no effect (null hypothesis), of obtaining a result equal to or more extreme than what was actually observed. Fisher proposed it as an informal index to be used as a measure of discrepancy between the data and the null hypothesis and therefore should not be interpreted as a formal inferential method. For example, since the P value can only be calculated on the assumption that the null hypothesis is true, it cannot be a direct measure of the probability that the null hypothesis is false. However, the main criticism to the P value is perhaps that it does not take into account the size of the observed effect, i.e. a small effect in a study with a large sample size can have the same P value as a large effect in a small study.\nHypothesis testing was proposed by Neyman and Pearson as an alternative approach to the P value, which assumes the existence of a null hypothesis (e.g. no effect) and an alternative hypothesis (e.g. nonzero effect). The outcome of the test is then simply to reject one hypothesis in favour of the other, solely based on the data. This exposes the researcher to two types of errors: type I error or false-positive (\\(\\alpha\\)) and type II error or false-negative (\\(\\beta\\)) result. Rather than focussing on single experiments, like the P value, hypothesis testing is effectively based on a deductive approach to minimise the errors over a large number of experiments. However, the price to pay to obtain this objectivity is the impossibility to make any inferential statement about a single experiment. The procedure only guarantees that in the long run, i.e. after considering many experiments, we shall not often be wrong.\nOver time a combination between the P value and hypothesis testing was developed under the assumption that the two approaches can be complementary. The idea was that the P value could be used to measure evidence in a single experiment while not violating the long run logic of hypothesis testing. The combined method is characterized by setting \\(\\alpha\\) and power \\(\\beta\\) before the experiment, then calculating a P value and rejecting the null hypothesis if the P value is less than the preset type I error rate. This means that the P value is considered a false-positive error rate specific to the data and also a measure of evidence against the null hypothesis. The P value fallacy is born from this statement, which assumes that an event can be seen simultaneously from a long run perspective (where the observed results are put together with other results that might have occurred in hypothetical repetitions of the experiment) and from a short run perspective (where the observed results are interpreted only with respect to the single experiment). However, these views are not reconcilable since a result cannot be at the same time an interchangeable (long-run) and unique (short-run) member of a group of results.\n\n\n\n\n\nI personally find this discussion fascinating and I believe that it is important to recognise the inconsistencies between the two alternative approaches to inference. The original authors of the two paradigms were well aware of the implications of their methods and never supported the combination of these. However, the combined approach has somehow become widely accepted in practice while its internal inconsistencies and conceptual limitations are hardly recognised.\nI feel that, since the two methods are perceived as “objective”, it is generally accepted that, if combined, they can produce reliable conclusions. This, however, is not necessarily true. Accepting at face value the significance result as a binary indicator of whether or not a relation is real is dangeroues and potentially misleading. This practice wants to show that conclusions are being drawn directly from the data, without any external influence, because direct inference from data to hypothesis is thought to result in mistaken conclusions only rarely and is therefore regarded as “scientific”.\nThis misguided approach has led to a much stronger emphasis towards the quantitative results alone (without any external input). In contrast, I believe that such perspective has the serious drawback of ignoring potentially useful information which is available (e.g. relevant medical knowledge or historical data) and which should be included in the analysis. Of course, I am aware of the potential issues that may arise from the selection and incorporation of external evidence, but I believe this should not be considered as “less reliable” or “more prone to mistakes” compared with the evidence from the available data. It is important that an agreement is reached about the selection of the type of evidence and methods to be used to perform the analysis solely based on their relevance with respect to the context analysed."
  },
  {
    "objectID": "posts/2019-09-25-my-blog-post/index.html",
    "href": "posts/2019-09-25-my-blog-post/index.html",
    "title": "MissingHE 1.2.1",
    "section": "",
    "text": "I have finally found some time to update the version for my R package missingHE, for which version 1.2.1 is now available on CRAN. I included two main features to the previous version of the package.\nFirst, I have added a new type of identifying restriction when fitting pattern mixture models through the function “pattern”. Before, only the complete case restriction was available, which identifies the distributions of the missing data with those from the completers. Now the alternative available case restriction is can also be selected, which relies on the distributions that can be identified among the non-completers to identify the distributions of the missing data. In this way, people can choose among at least two options for the type of restrictions and compare how this choice may affect the final estimates.\nSecond, I added a new accessory function called “ppc”, which allows to perform posterior predictive checks using the conditional parameters saved from the fitted model to generate replications of the data at each posterior iteration of the model. The function implements a relatively large number of checks, mostly taken from the R package bayesplot, which allow to assess the fit of the model to the observed data by type of outcome (effects and costs) and treatment group (control and intervention). For example, overalyed density plots can be generated to compare the empirical and replicated densities of the data to detect possible failures of the model.\n\n\n\nDensity plots for the observed and replicated data\n\n\nI feel this is very important as when fitting a Bayesian model it is crucial to assess whether the model seems to adequately capture the different characteristics of the observed data (e.g. skewness, structural values, etc.). A wide range of predictive checks are available, including histograms (see thumbnail pciture), scatterplots, error intervals, empirical cumulative distribution functions, statistics of interest and many others. In addition, these checks can be performed for each type of missingness model and parametric distribution chosen within missingHE.\n\n\n\n\n\nOf course, it is important to remember that, when dealing with missing data the fit of the model can only be checked with respect to the observed values and therefore this check is only partial since the fit to the unobserved values can never be checked. This is also why it is not meaningful to assess the fit of a model fitted under a missing not at random assumption because this is based on information which is not directly available from the data at hand and thus impossible to check."
  },
  {
    "objectID": "posts/2019-10-28-my-blog-post/index.html",
    "href": "posts/2019-10-28-my-blog-post/index.html",
    "title": "Copenhagen, I am coming …",
    "section": "",
    "text": "Finally the time of ISPOR Europe 2019 has arrived and I will depart in a few days for Copenhagen, where the conference is held this year. I am actually looking forward to this as I am curious to see what type of conference ISPOR is, that is, whether I will be able to find some interesting works and have some “applied statistics”-related discussions or the attention is more placed on “economics and clinical” matters. From what I heard by other people who routinely attend the conference, there should be a bit of both sides, even though I really hope I will be able to see some intersting methods and engage in discussion with some authors.\nI know the conference is mainly related to address the needs of pharmaceutical and consultancy companies, but I hope I will be able to see some familiar faces there. Well, to be honest I know that some people I already know are going, which is good considering that their work is really cool. As for me, I will present the same work that I showed at ICTMC 2019 (some slides available here), but this time in the format of a poster, of which I am kind of very proud in terms of the final output, if I may say so.\nApart from this nice event, there are many things coming up when I will be back from the conference, which I really need to start working on. Mostly, these are related to some routine work for some trial analyses at PRIMENT, which by the way is advertising a new health economist job vacancy for those who might be interested. Other tasks include writing down and code a decision model on which I have been working since ages, papers review, other collaborations with different people, starting my co-supervision for a new PhD student at stats and, after I can find some free time, do some research work on my beloved missing data. Am I ready? not sure about that …"
  },
  {
    "objectID": "posts/2019-12-09-my-blog-post/index.html",
    "href": "posts/2019-12-09-my-blog-post/index.html",
    "title": "Not a very good start…",
    "section": "",
    "text": "After some nice holiday break, I came back to work ready for an exciting 2020 … or so I thought. Unfortunately, I have recently been caught by a terrible flu which forced me to postpone my flight back to London of a week. The worst part is that I was basically a dead corpse moving around with high fever and an awful condition for more than 4 days. It was quite a bad experience which I rarely had in my life. I am just glad I survived this.\n\n\n\n\n\nGoing back to more interesting news. Before my cursed period, I was smart enough to work on different things and I am happy to announce a new update for my missingHE package, which is available both on my GitHub page and on the CRAN repository. Its new version is 1.3.2 and has the nice addition of making available more choices for the parametric distributions that can be selected in all main functions of the package to handle missing data in trial-based economic evaluations. In particular, it is now possible to choose among new probability distributions for the health outcomes, including continuous (Gamma, Weibull, Exponential, Logistic), discrete (Poisson, Negative Binomial) and binary (Bernoulli) distributions. These may be useful when the analysis is not based on utilities scores but some other types of effects, such as survival time, number of events or binary outcomes. I have also included some examples for each type of outcome in the MenSS dataset (available directly once installed the package on your machine) so that people can play around with the new distributions.\nAnother good news is that the last paper written with Michael about missing data handling in economic evaluations will soon be published in the February issue of JRSSA, which will make the final and official version of the article that can be cited, I think.\nFinally, an announcement about the one-day course I am holding together with my mates from the HEART group about an introduction to economic evaluations to people who are not familiar with health economics. The course will take place next month, I believe on Feb 11th, in central London (soon an update about the exact location) and, as the previous edition, I am happy to see that all spots have been taken and everything is sold out (well, to be precise the course is free …). Need to meet up with the others to make the last changes and prepare the slides but I am quite excited about this, given also the good response we got last time.\nNow I am (hopefully) ready to start the new year and there are many things already piling up on my list of things to do in the next days. Let’s try again 2020."
  },
  {
    "objectID": "posts/2020-02-01-my-blog-post/index.html",
    "href": "posts/2020-02-01-my-blog-post/index.html",
    "title": "Finally here …",
    "section": "",
    "text": "The new year is finally taking off for me and I have a couple of updates. First, I would like to remind everyone about the exciting new course “understanding health economics in clinical trials” that me and the rest of our research team HEART have put together to support the dissemination of health economics among all people involved in the design and analysis of clinical trials. I look forward to deliver this one-day short course together with my colleagues from the UCL PCPH department which will be structured into different sessions during the day of Feb 11th at the UCL CCTU - 2nd Floor, 90 High Holborn, London. The course is specifically intended for those who would like to know more about health economics, which has become an important component in the design, analysis and most crucially, for the funding approval of clinical trials. The course will focus on the following aspects:\n\nA short intorduction to the basic concepts of health economics and why these can be relevent to different people\nA review of different types of intruments and tools used to collect health economic data in clinical trials\nA quick look at decision models with some examples\nA summary of the typical results from health economic analyses and how to interpret them\n\nThe course is still in its pilot form and therefore it is free of charge. If there are still places available, you are very welcome to join and give us your feedback!.\n\n\n\n\n\nSecond, I am happy to announce that my recent paper about the use of Bayesian Hierarchical Models for the Prediction of Volleyball Results has finally been published on the Journal of Applied Statistics. I am really proud of this paper as it is my first solo paper published and because I have always been very invested in the general topic of predicting sport results using probability models. To be able to publish something about this based on my own efforts is very rewarding in terms of the (small) contribution to research that I hope I was able to provide.\n\n\n\n\n\nFinally, I have submitted an abstract to the 2020 European Health Economics Association Conference, which this year will be held in Oslo, Norway. I have now to patiently wait for the review of the abstracts and see if my work made it, either as an oral presentation or as a poster. Fingers crossed!."
  },
  {
    "objectID": "posts/2020-03-01-my-blog-post/index.html",
    "href": "posts/2020-03-01-my-blog-post/index.html",
    "title": "Lockdown",
    "section": "",
    "text": "I have to admit, although I expected some fear to spread because of the virus which is currently and quickly infecting the world, I was surprised by the frenzy surrounding us, especially in my homecountry (Italy) and particularly in my parents’ region which is at the moment under lockdown. I will also probably cancel my planned trip home for Eastern and perhaps also after that since the situations is still unclear and I may be unable to come back to the UK in the short time. This is pretty scary to all the people living in those territories, who are now forbidden to have any sort of public meeting and are strongly recommended to stay at home. I am afraid this will not be enough to stop the virus from spreading but of course it is useful as it is the only way we have if we want to contain it. The hope is that by summer time the heath will reduce the ability of the virus to spread and give us some time to come up with a possible vaccine in the next months.\nThere have already been attempts to estimate the fatality ratio of the virus using statistical methods. Here I post the tweet from Andrew Gelman\n\nwhich refers to epidemiologists who tried to use Stan for achieving this objective, although an additional reference to another work based on the use of differential equation analysis is also made. However, results are still preliminary and subject to limitations for the type of data and assumptions used. From a statistical perspective I am sure this new epidemic will be interesting to study and I guess lots of funding will be devoted to analyse the upcoming data to get a better idea of the actual threat it represents for the people. I am not an epidemiologist, so I do not have a big statistical interest in this, although I am pretty much worried as any other person. Hopefully, this will become clearer as time passes and let us just hope the number of deaths will not be very high.\nSorry about talking about this here, but from time to time I would also like to highlight was is currently happening around me. As for my research, nothing has changed much for me at the moment and life as usual continues with another busy upcoming period with lots of boring meetings, reports and standard analyses to do, but hopefully I can also save some time to do some methodological work. I am also waiting for the decision about my abstract which I submitted to the EuHEA conference 2020 and which is supposed to be held in Oslo this July. I really hope I have a chance to presenting my work there as I have never been to this specific health economic conference. Fingers crossed! Of course, nobody knows what will happen from here till July and much is to be discussed also with respect to how the spreading of the virus may affect everyone’s schedule in the next months."
  },
  {
    "objectID": "posts/2020-04-01-my-blog-post/index.html",
    "href": "posts/2020-04-01-my-blog-post/index.html",
    "title": "So much time but also not really",
    "section": "",
    "text": "The lockdown proceeds also here in the UK, as in the rest of the world, and at the moment we have no clear idea how long it will last. Not much we can do apart from staying at home all the time and practicing social distancing. I am still ok with living at home 24/7 but this has affected my productivity, especially in terms of collaborating with other people.\nAlthough I have a lot of time to dedicate to some works, I am making a slow progress and the time of the day seems to fly in an instant with so many things to do. This months is particularly busy as I am trying to submit a revision for a paper which hanted me for quite a lot of time now and which I must finish by the first days of May. I am also working on side projects but these have been slowed down due to the current situation. I hope I can find the time (and the will) to do some more updates to my website by adding more tutorials and similar stuff. I do have some nice ideas about possible projects and collaboration but I need to wait until after this weird period.\nI am also planning to prepare a new version for my R package missingHE to add some nice additional features for post-processing the results of Bayesian models and to implement new types of models. These, however, will take time, which at the moment is one thing that I do have but that I also do not have.\n\n\n\n\n\nAnyway, not much of an update this one, but I hope things will move quicker in the next couple of months or so."
  },
  {
    "objectID": "posts/2020-05-18-my-blog-post/index.html",
    "href": "posts/2020-05-18-my-blog-post/index.html",
    "title": "Sorry, an error occurred",
    "section": "",
    "text": "It has been a while from my last update on this website, but this has been an incredibly busy period with lots of routine work that I had to do. Now the situation has clamed down a bit, and I have also some news to report. So, here I am finally.\nIn the past few days, I had an extremely interesting email correspondence with one guy (don’t want to say the name for privacy) interested in using my R package missingHE to do some trial-based CEA. Awesome, I thought. He was looking for some advice for how to customise some models in the package and how to get the results he wanted. Unfortunately for my pride, but fortunately for my package, we discovered a small bug in the code when trying to specify a hurdle model (two-part model) to handle the zero costs when not including any covariate inside the logit model to estimate the probability of having a zero cost. Essentially, under these specific circumstances, the function did not correctly backtransformed the estimate of the mean costs on the appropriate scale and the results provided were incorrect. Sorry about that!\n\n\n\n\n\nTo be honest, you can get away by simply including some baseline covariate into the logit model for the structural zero costs, in which case the estimates produced by the function are correct. I have immediately updated the package version to correct this bug on my GitHub page, where you can find the most up to date version (1.4.1). The version on CRAN will be updated at the next iteration as I have recently uploaded the 1.4.0 version in May. In the meantime, if you want to avoid having that issue above, you can download and install the updated version from my GitHub page.\nI think this is also a good chance to tell that I have updated an old paper on my on my Arxiv account. Both the content and title of the paper have changed considerably, but overall I feel that the overall message and quality of the article has improved. It is still an on-going version, but I am quite satisfied with its current status given all the effort I put into it. Have a lool in case you are interested. New title : “Joint longitudinal models for dealing with missing at random data in trial-based economic evaluations”.\nI believe that is all for this update. Not much going on due to the whole lockdown situation here in the UK but hopefully things are improving a little in a two-months time we will be able to at least go to the office. Let’s see."
  },
  {
    "objectID": "posts/2020-07-07-my-blog-post/index.html",
    "href": "posts/2020-07-07-my-blog-post/index.html",
    "title": "Why be Bayesian",
    "section": "",
    "text": "Many times I have been asked by co-workers and people around me who are a bit familiar with statistics why I choose to be Bayesian and whether I feel confident in using this approach for my data analysis rather than the most widely accepted frequentist methods, at least in my research area. Well, I am sure there are many valid arguments I could use to reply to this question but if I have to summarise my answer in two words I would say: why not?\nNow, a bit more into the details for those who were not extremely annoyed by my previous sentence. So, I truly believe that the Bayesian approach can be considered as a complement rather than a substitute to the frequentist paradigm. The main reason is relate to its much stronger links with probability theory compared with the classical approach in that not only are sampling distributions required for summaries of data, but also a wide range of distributions are used to represent prior opinion about proportions, event rates, and other unknown quantities. In a nutshell, the key difference between the two approaches is how they confront the concept of probability of a certain event. In fact, although there is general consensus about the rules of probability, that there is no universal concept of probability, and two quite different definitions come from the frequentist and Bayesian approach:\nRather than debating on philosophical debates about the foundations of statistics I prefer to focus on those aspects which I believe make the Bayesian approach, if not more intuitive than the frequentist counterpart, at least more attractive. Be worn I am not trying to start a war as I think both approaches could be used without the need to completely discard the other. The simple fact of being able to choose between two methods, rather than restricting themselves to a single option, seems a good enough reason for me to advocate the use of both approaches. I terms of your own knowledge, experience and skills, You do not gain anything by saying “I will never be Bayesian” or “I will never be a frequentist”. On the contrary, by opening your mind and explore the use of one or the other method you will be able to have more options at your disposal that you can use to tackle the different problems you will face in your analyses.\nFor the purpose of this post I just want to highlight some aspects which make the Bayesian approach particularly useful and, in some cases, even arguably preferable than the frequentist approach. Note that I am well aware there could be cases where the opposite holds and this is precisely why I believe it is important that statisticians should become familiar with both methods. By doing so they will be able to overcome the limitations/concerns associated with one method for a specific problem at hand using the instruments made available from the other method. Since I am a Bayesian, here I want to report the reasons and situations in which the Bayesian approach could provide a powerful tool.\nLet us start with a quick recap of the basic principle behind Bayesian methods. Bayesian statistical analysis relies on Bayes’s Theorem, which tells us how to update prior beliefs about parameters and hypotheses in light of data, to yield posterior beliefs. The theorem itself is utterly uncontroversial and follows directly from the conventional definition of conditional probability. If \\(\\theta\\) is some object of interest, but subject to uncertainty, e.g. a parameter, a hypothesis, a model, a data point, then Bayes Theorem tells us how to rationally revise prior beliefs about \\(\\theta\\), \\(p(\\theta)\\), in light of the data \\(y\\), to yield posterior beliefs \\(p(y \\mid \\theta)\\). In this way Bayes Theorem provides a solution to the general problem of induction, while in the specific case of statistical inference, Bayes Theorem provides a solution to problem of how to learn from data. Thus, in a general sense, Bayesian statistical analysis is remarkably simple and even elegant, relying on this same simple recipe in each and every application.\nAs I see it, there are a few major reasons why statisticians should consider learning about the Bayesian approach to statistical inference, and in the social sciences in particular:\nThe result of a Bayesian analysis is a posterior probability statement, ‘posterior’ in the literal sense, in that such a statement characterizes beliefs after looking at data. Examples include: the posterior probability that a regression coefficient is positive, negative or lies in a particular interval; the posterior probability that a subject belongs to a particular latent class; the posterior probabilities that a particular statistical model is true model among a family of statistical models.\nNote that the posterior probability statements produced by a Bayesian analysis are probability statements over the quantities or objects of direct substantive interest to the researcher (e.g. parameters, hypotheses, models, predictions from models). Bayesian procedures condition on the data at hand to produce posterior probability statements about parameters and hypotheses. Frequentist procedures do just the reverse: one conditions on a null hypothesis to assess the plausibility of the data one observes (and more ‘extreme’ data sets that one did not observe but we might have had we done additional sampling), with another step of reasoning required to either reject or fail to reject the null hypothesis. Thus, compared to frequentist procedures, Bayesian procedures are simple and straightforward, at least conceptually.\nThe prior density also provides a way for model expansion when we work with data sets that pool data over multiple units and/or time periods. Data sets of this sort abound in the social sciences. Individuals live in different locations, with environmental factors that are constant for anyone within that location, but vary across locations. key question in research of this type is how the causal structure that operates at one level of analysis (e.g. individuals) varies across a ‘higher’ level of analysis (e.g. localities or time periods). The Bayesian approach to statistical inference is extremely well-suited to answering this question. Recall that in the Bayesian approach parameters are always random variables, typically (and most basically) in the sense that the researcher is unsure as to their value, but can characterize that uncertainty in the form of a prior density \\(p(\\theta)\\). We can replace the prior with a stochastic model formalizing the researcher’s assumptions about the way that parameters \\(\\theta\\) might vary across groups \\(j = 1,..., J\\) , perhaps as a function of observable characteristics of the groups; e.g., \\(\\theta_j \\sim f (z_j, \\gamma )\\), where now \\(\\gamma\\) is a set of unknown hyperparameters. That is, the model is now comprised of a nested hierarchy of stochastic relations: the data from unit \\(j\\), \\(y_j\\), are modeled as a function of covariates and parameters \\(\\theta_j\\) , while cross-unit heterogeneity in the \\(\\theta_j\\) is modeled as function of unit-specific covariates \\(z_j\\) and hyperparameters \\(\\gamma\\). Models of this sort are known to Bayesians as hierarchical models, but go by many different names in different parts of the social sciences depending on the specific form of the model and the estimation strategy being used (e.g. ‘random’ or ‘varying’ coefficients models, ‘multilevel’ or ‘mixed’ models). Compared with the frequentist counterpart, thanks to the use of Markov chain Monte Carlo (MCMC) methods, Bayesian computation for these models has also become rather simple. Indeed, MCMC algorithms have proven themselves amazingly powerful and flexible, and have brought wide classes of models and data sets out of the ‘too hard’ basket. Other modelling examples include data sets with lots of missing data, or models with lots of parameters, model with latent variables, mixture models, and flexible semi-and non-parametric models.\nFrequentist inference asks assuming hypothesis \\(H_0\\) is true, how often would we obtain a result at least as extreme as the result actually obtained?’, where ‘extreme’ is relative to the hypothesis being tested. If results such as the one obtained are sufficiently rare under hypothesis \\(H_0\\) (e.g. generate a sufficiently small p value), then we conclude that \\(H_0\\) is incorrect, rejecting it in favor of some alternative hypothesis. Indeed, we teach our students to say that when the preceding conditions hold, we have a statistically significant result. My experience is that in substituting this phrase for the much longer textbook definition, people quickly forget the frequentist underpinnings of what it is they are really asserting, and, hence seldom question whether the appeal to the long-run, repeated sampling properties of a statistical procedure is logical or realistic. In the Bayesian approach we condition on the data at hand to assess the plausibility of a hypothesis (via Bayes Rule), while the frequentist approach conditions on a hypothesis to assess the plausibility of the data (or more extreme data sets), with another step of reasoning required to either reject or fail to reject hypotheses. The frequentist p-value is the relative frequency of obtaining a result at least as extreme as the result actually obtained, assuming hypothesis \\(H_0\\) to be true, where the sampling distribution of the result tells us how to assess relative frequencies of possible different results, under \\(H_0\\). But what about cases where repeated sampling makes no sense, even as a thought experiment?\nRecall that in the frequentist approach, parameters are fixed characteristics of populations, so \\(\\mu\\) either lies in the interval or it doesn’t. The correct interpretation of a frequentist confidence interval concerns the repeated sampling characteristics of a sample statistic. In the case of a \\(95\\%\\) confidence interval, the correct frequentist interpretation is that \\(95\\%\\) of the \\(95\\%\\) confidence intervals one would draw in repeated samples will include \\(\\mu\\). Now, is the \\(95\\%\\) confidence interval that one constructs from the data set at hand one of the lucky \\(95\\%\\) that actually contains \\(\\mu\\), or not? No ones knows.\nFinally, aside from acknowledging the subjectivity inherent to the general scientific exercise, the Bayesian approach rests on a subjective notion of probability, but demands that subjective beliefs conform to the laws of probability. Put differently, in the Bayesian approach, the subjectivity of scientists is acknowledged, but simultaneously insists that subjectivity be rational, in the sense that when confronted with evidence, subjective beliefs are updated rationally, in accord with the axioms of probability. Again, it is in this sense that Bayesian procedures offer a more direct path to inference; as I put it earlier, the Bayesian approach lets researchers mean what they say and say what they mean. For instance, the statement, having looked at the data, I am \\(95\\%\\) sure that \\(\\mu\\) is included in an interval is a natural product of a Bayesian analysis, a characterization of the researcher’s beliefs about a parameter in formal, probabilistic terms, rather than a statement about the repeated sampling properties of a statistical procedure."
  },
  {
    "objectID": "posts/2020-07-07-my-blog-post/index.html#conclusions",
    "href": "posts/2020-07-07-my-blog-post/index.html#conclusions",
    "title": "Why be Bayesian",
    "section": "Conclusions",
    "text": "Conclusions\nThe mathematics and computation underlying Bayesian analysis has been dramatically simplified via a suite of MCMC algorithms. The combination of the popularization of MCMC and vast increases in the computing power available to social scientists means that Bayesian analysis is now well and truly part of the mainstream of quantitative social science. Despite these important pragmatic reasons for adopting the Bayesian approach, it is important to remember that MCMC algorithms are Bayesian algorithms: they are tools that simplify the computation of posterior densities. So, before we can fully and sensibly exploit the power of MCMC algorithms, it is important that we understand the foundations of Bayesian inference.\nThis time I went overboard with the discussion but I thought it could be interesting to clarify here the key points, in my opinion, which make the Bayesian approach not only valid and efficient, but even a powerful tool that, once grasped the underlying philosophy, can be used to overcome the difficulties of standard methods, especially when dealing with complex analyses.\nSo what are you waiting for? do not sit in your frequentist comfort zone but expand your statistical knowledge! Evolve!"
  },
  {
    "objectID": "posts/2020-09-10-my-blog-post/index.html",
    "href": "posts/2020-09-10-my-blog-post/index.html",
    "title": "Starting a new adventure!",
    "section": "",
    "text": "Hello dear readers, I have some exciting news about myself and my future which I am eager to communicate on this blog. I know that it is not exactly the most interesting news for everybody but I have recently joined a new research team in the Department of methodology and statistics at Maastricht University, in the Netherlands.\nI must say, it was not easy for me to leave UCL, where I have studied and obtained my PhD degree and spent quite a few years of my life. I am glad that during this time I met so many fantastic people and colleagues from whom I have leared so much and which I would like to thank from the bottom of my heart. I must say thank you to all my previous PhD supervisors, namely Gianluca Baio, Alexina J. Mason and Rachael Hunter, who were incredibly supportive throughout my entire PhD and with whom I also collaborated during my experience as a research fellow at the department of statistical sciences and PCPH. Particularly during my first post-doc position, I came to know so many colleagues and share so many ideas, which became an essential part of my academic activity and personal growth. Among others, special thanks are also due to my colleagues and friends from the HEART team at PCPH with whom I am still collaborating on some interesting research projects.\nAlthough both my personal and working experience at UCL were amazing, after almost 4 years of PhD and 2 years as a reasearch fellow, I felt that it was time for a change in my life. London can be a quite stressfull city to live in and I wanted to see if I could take some new opportunity to enhance my personal growth by taking a new perspective. This is why I decided to accept a new position as assistant professor in statistics at UM, which I have officially joined a few days ago. During my interview for the position, I was really intrigued by the prospect of this job in terms of both increasing my teaching experience at the university level as well as obtaining more independence in regard to the research topics I would like to explore. Don’t get me wrong, I will still work on stats methods for CEA as this will still be the main focus of my research activity for the next years to come. I love it so much! However, it will also be exciting to see how I can use the experience and knowledge acquired at UCL to contribute (and hopefully improve) the current approach in CEA in the Netherlands.\nI already know that, especially at the beginning, it will not be easy. From studying a new language (yes it something that I would like to do!), to getting acquainted with the new place and rules, the new teaching duties, new colleagues, etc… There is much to learn and to do and I am really excited to see where this new adventure will lead me in a few months/years from now. In the meantime I have to do my best to adapt this big change in my life. My new colleagues at the stats department have been very nice to welcome me during these difficult times where interaction with people must be limited to avoid the spread of the virus. I hope this situation will change in the next years and that we can gradually go back to “normality”, if you want to call it that way.\nTo conclude, I would still thank all the people who have supported me during these exciting but also quite challenging times, first among all my parents to whom my unconditional love goes. I will try to post my future updates in a regular way on my website but, especially at the beginning, I will probably need some time to adapt to the new changes and I will be quite busy. I still hope to continue my on-going collaborations with all my previous colleagues from UCL and the UK while also being able to meet new people and start new relationships here in Maastricht and the Netherlands.\n\n\n\n\n\nThank you and see you soon !\nDank u en tot ziens!"
  },
  {
    "objectID": "posts/2020-11-05-my-blog-post/index.html",
    "href": "posts/2020-11-05-my-blog-post/index.html",
    "title": "A couple of updates",
    "section": "",
    "text": "Finally some exciting updates! I really need some good news after all that happened this year. So, first of all I have recently found out that one of the paper I co-authored got published early this year but I actually forgot to check it. The work is an interesting long-term CEA for a new drug in prostate cancer patients, extrapolating the data from an RCT named STAMPEDE. I was only partially involved with this work, which has been mostly done by the talented and always cheerful Caroline Clarke with whom I collaborated during my post-doc experience at UCL. My contribution mostly resolved about double checking the R code for the model which, I have to say, was pretty sophisticated and not always clear given that was originally done by someone else (I we know how understanding someone else’s code can be an hard task). Overall, I am really happy for this paper which I hope may be of interest to someone specialised in that type of analysis and population.\nNext, I am also proud of having contributed to the third round for the course “Understanding Health Economics in Clinical Trials”, which I delivered together with my ex-colleagues and co-members of the HEART team from UCL. This is the third time we have delivered the course in the past two years and I was really impressed by how far we have come since our first attempt. The structure of the course is now very nice and most of the people attending out last edition (in online format of course, thanks 2020!) provided very nice feedback. This, I believe, was the last time we offered the course for free and from the next time we will start charging a small fee to the participants. We have some minor adjustments to make in a couple of sessions, but overall I feel pretty good about it. I would like to thank all my HEART buddies, with a special mention to Ekaterina Bordea, with whom I share the delivery of the final session of the course. Although we were a bit strict on time, and I ended up taking most of it (sorry Ekaterina!), we both got very positive comments from the attendants. I hope I will still be involved with these guys for the future editions of the course as it was really fun.\nAslo, a quick update about my acadmic work. I have written an abstract for a paper together with some very nice people involved in missing data analysis in CEA, including the amazing Baptiste Leurent and Catrin Plumpton. We submitted the abstract to HESG 2021 which has been accepted as a presentation. We still need to write up the actual paper which we need to submit by the end of this month but I am confident we will make it. This work is again about missing data in CEA but based on a different perspective compared with the standard imputation approach and instead explores the use of mixed models as a possible alternative under some assumptions. I really enjoyed working on this, especially with Baptiste with whom I started working on this quite a few months ago. I am not sure I will be able to be present at HESG as I have some heavy teaching duties in Maastircht in that period but I look forward receiving some feedback for our idea.\nI have also found some time to upload on ArXiv a drafted version for a paper I strated working on about one year ago at UCL. The work is a sort of experiment for me where I tried to apply some Bayesian methods for jointly modelling patient-level partitioned survival cost-utility data. The idea is pretty nice, I think, but I will probably need to polish off a few things before finalising the paper. I was also excited to implement something in STAN which I have started using more frequently in the last months. Although in my experience other Bayesian software seem to be pretty good for different analyses, I think STAN has a great potential in the future, especially thanks to all the support and community posting online solutions and code for different types of problems and analyses.\nFinally, I have been involved in some stats teaching for a master course in epidemiology at my affiliated faculty FHML at UM. Everyting was done online, but overall I was happy about how I delivered my sessions of the course and with the feedback I received from the sudents. This was my first teaching experience at UM and, considering the special circumstances most of us are in this year, I think I managed pretty good. Next month I will involved in the marking of the exams for this course while from Jan 2021 I will be quite busy with lots of courses and a quite frightening time schedule. Good luck to me!\n\n\n\n\n\nSo, lots of news but this is also becuase I did not find much time in the past weeks to update my blog. I hope to be more consistent in the future but you never know, especially this year. One thing I am looking for is to explore and visit Maastricht for which I haven’t had a chance since I started my contract last September. Everyone says that the Christmas market is particularly beautiful but this year there won’t be one because of \\(\\ldots\\) well 2020!"
  },
  {
    "objectID": "posts/2021-01-15-my-blog-post/index.html",
    "href": "posts/2021-01-15-my-blog-post/index.html",
    "title": "What is a Bayesian credible interval?",
    "section": "",
    "text": "Happy new year everybody! Yeah, I know it already almost February but I have been incredibly busy the past few weeks after the Xmas break. From getting familiar with the courses I am teaching this term to providing consultancy advice on statistical problems for students from FHML at UM. This last task has been particularly challenging as I did not have a clear idea of what was the statistical background of the students I has to work with and, in fact, the level of statistical knowledge varied considerably depending on the specific cases considered and I had to adjust my way of interacting with the clients on a case-by-case basis. Overall, I have learned a lot in these past weeks, from doing lots of online teaching and consultations, and I believe I am now well prepared to deal with more students, which is something I am looking for now that I have this experience. So, come at me students!\nNow, leaving aside this little off-topic with respect to the content of this post, let us go back to the key aspect which I glossed over last month. The question that was asked by Dr. Morris was how do you interpret a Bayesian credible interval?. In his comment he argued that the interpretation of such interval was not clear when compared, for example, to the frequentist interpretation of confidence intervals, i.e. the range of likely values for the population parameter of interest based on a \\((1-\\alpha)\\%100\\) confidence level.\n\n\n\nHow CIs are computed\n\n\nThe definition relies on the typical frequentist concept of treating sample data as random quantities that can be hypothetically drawn many times from the population and for each of these data a different confidence interval can be computed based on the sample statistics. This procedure ensures that out of all possible samples that I can draw from the population (and therefore out of all CIs I can compute), about \\(95\\%\\) of these intervals will contain the true (unknown) population parameters of interest which are considered fixed quantities. This, however, means that if I consider the specific CI that I computed over my collected sample data, I have no guarantee that it contains the true parameter value. Indeed, it is always possible that I got particularly unlucky with my sample and my CI belongs to that \\(5\\%\\) (out of the many possible that I could compute based on many drawn samples) that will not contain the true parameter value. This means that, although very powerful on a theoretical level, the frequentist argument may not be that useful when I have to focus on the available data that I have, rather than relying on long-term justifications which may not be applicable to my specific situation.\nSo, the natural question in situations in which this approach does not answer ver well at my research question is, is there another way?. The answer is of course yes and one of these alternative approaches is the Bayesian perspective. In contrast to the frequentist argument, the Bayesian approach treats the statistical problem under examination under a different perspective. First, the (unknown) parameters of interest are not fixed but are random quantities generated according to some probability distribution (the prior distribution). Since this distribution is assigned to the parameter before looking at the data, it corresponds to a mathematical representation of the degree of belief that one has about the possible values of the parameter of interest which must be formulated prior to observing the available data (it may be based on previous research or completely subjective opinions, for example). At this point, the Bayesian procedure simply consists in using the available data, which are treated as fixed, to update our initial belief about the possible parameter values through the Bayesian theorem, i.e. \\(p(\\theta|y)=\\frac{p(y\\mid \\theta)p(\\theta)}{\\int p(y|\\theta)p(\\theta)d\\theta}\\).\nThis updated state of belief for the parameter values is itself a probability distribution, known as the posterior distribution \\(p(\\theta \\mid y)\\). This distribution represents how, following a rational and coherent procedure intrinsic to the Bayesian theorem, the initial belief about \\(\\theta\\) (expressed by the prior \\(p(\\theta)\\)) is updated based on the available data (expressed by the likelihood \\(p(y\\mid \\theta)\\)). The posterior is always a compromise between prior and likelihood but since typically the contribution to the likelihood is provided by \\(n\\) data points, the weight of the likelihood is often much larger (equal to \\(n\\)) compared with that of the prior (equal to \\(1\\)). Thus, the more data become available the less is the weight of the prior on the posterior and when \\(n \\rightarrow \\infty\\) the posterior is completely dominated by the likelihood. In addition, since the posterior is a probability distribution, it is always possible to directly summarise the uncertainty around certain posterior quantities of interest (e.g.~mean or mode). Thus, we can calculate the range of values within which our estimate lies with a given probability of say \\(95%\\), corresponding to our credible interval.\nSo the key difference between the frequentist and Bayesian approach is from a theoretical perspective of how probability is conceived: a long-run frequency vs degree of belief definition. This has also implications on how uncertainty around our estimates of interest is expressed: a general rule that applies across all possible cases but that does not guarantee the the validity of the conclusions in each specific case vs probabilistic approach whose results specifically applies to the case under consideration but that does not guarantee that its conclusions are valid across all possible cases.\nA hardcore frequenstist may also point out that Bayesian inference requires prior distributions that are reasonable as otherwise they may drive your results away from the truth. However, my reply would be yes, it is always important that priors are defined based on some available and plausible information (or simply use very loosely defined priors in case such knowledge is not available), but the question is how you define the truth. Is it a single value? In my opinion it is more reasonable to think at the truth as a distribution in that models are only used as a way to simply the reality and not to precisely represent it. Not rejecting a true null hypothesis may be ok but what then? should I continue testing for many different values for what I believe is the true population parameter? Instead, I would prefer to make assumptions about what are possible values for the parameters (therefore accounting for uncertainty around which values are more/less likely a priori) and then use the data I collect to either support/contrast those assumptions.\nThe appealing of the Bayesian approach is precisely to be very practical: combine the current data and an initial assumption about parameter values that generated the data to update my initial belief (in a rational and coherent way) and quantify how the uncertainty around the likely values for the parameter has changed after observing the data. This straightforward interpretation of how to deal with expressing the uncertainty for any unobserved quantity (parameter, prediction, missing value) makes the Bayesian approach optimal for handling decision-making problems where the quantification of the uncertainty around the conclusions is a fundamental element in the decision process."
  },
  {
    "objectID": "posts/2021-03-25-my-blog-post/index.html",
    "href": "posts/2021-03-25-my-blog-post/index.html",
    "title": "Sunny days which I cannot fully enjoy",
    "section": "",
    "text": "Here we are again. Except now it feels like a very nice spring time here in Maastricht with beautiful sunny days an warm weather. The picture does not really represent the environment in this region of the Netherlands (called Limburg), but I thought it was a very nice picture to put as thumbnail.\nI am sorry for this very short and boring posts but this period was a bit busy with lots of work. Hopefully, I will be able to enjoy a short break for a couple of weeks until the next round is up. Anyway, no big updates from me as in this period I have considerably enhanced my teaching and tutoring skills for different groups of student in a undergraduate epidemiological and statistical course here at UM. At first it was a fun exercise and the topics is always nice to review. However, after a couple of months of seminars and lectures I needed some time off from teaching as I was depleting my daily batteries so bad that I only had the energy to survive during the last period. The next course I will teach will start in middle April, so I hope I will have enough time to recover and be ready for the new round.\nDue to my involvement with the teaching I did not have much time for pretty much anything else. I continued doing some consultancy work for the students from other departments who required some assistance in their statistical analysis. Nothing too complex but sometimes it is necessary to go in details with the students in order to understand what they really want as I pretty sure most of the times this is unclear to them as well. At the moment I have a constant number of consultancy sessions per week and so far I had generally positive experiences. In one of these I also had the luck to be involved as a co-author in one health economic analysis done by one medical PhD student from UM. I assisted the student throughout a series of meetings where we went through some quite advanced statistical modelling approach, including multiple imputation, mixed models and bootstrapping. Luckily I was able to teach these methods in R as otherwise things could have become quite long and slow using other software such as SPSS. I believe the analysis is now towards its conclusion and I think the student demonstrated some very nice abilities and skills for doing some advanced coding and interpret the results from the analysis.\nIn the meantime I am still involved in a couple of other reserach projects which however are slowing down due to my lack of time. The ideas I wanted to explore and discuss with other colleagues are cool and potentially interesting but the number of hours in a day are not enough.\n\n\n\n\n\nOne happy note comes from one of the paper I had previously submitted to a journal which has been now officially approved for publication. Here you can find the ArXiv version of the paper which focuses on the implementation of Bayesian methods for the analysis of trial-based partitioned surivival cost-utility data as possible way to deal with some complexities of these types of analyses. This was I side project I decided to explore while I was still in London and then I finished it while I had some free time after my enrollment here at UM More info about this in the next weeks/months.\nFinally, I have received the prestigious title of honorary lecturer at the stats department at UCL, which is essentially just an official requirement that I need to possess in order to continue working with my colleagues from London and to continue supervising a PhD student for a speicfic project (of course about missing data!). Anyway, now I feel like a very important person (not really but hey that’s a cool title to add to the CV!)."
  },
  {
    "objectID": "posts/2021-05-15-my-blog-post/index.html",
    "href": "posts/2021-05-15-my-blog-post/index.html",
    "title": "The good, the bad and the ugly",
    "section": "",
    "text": "A quick update from me about a couple of things, some good, some bad and other \\(\\ldots\\) ugly.\n\nLet’s start with the good. I have found the time to work on some old projects which are finally progressing in their late stages. The most important news is that my article on Bayesian methods for partitioned survival cost-utility analyses has been published in pre-print form and is available for free here. Soon the offical version will become available as open access which is really nice if anyone would be interested in my methods. As for the other projects, at the moment I cannot say too much but they are all very interesting and involve the collaboration with some cool people I had the privilege to work with during my past experience at UCL. Hopefully, in the next couple of months some concrete proof of these collaborations will become publicly available and I am excited to talk about these projects which I think have a huge potential. I also have a couple of ideas for some future projects and I hope during the summer to have some extra time to dedicate to them. I would particularly like to start coding up some Bayesian statistical methods into a new R package to make these more accessible to everybody and I have some coll ideas in mind. It is only matter to find the time to put them into practice!\nNow the bad. The weather in the last period has not been great here in the Netherlands with lots of raining days which, coupled with some extensive teaching duties, did not really help to put me into a good mood for doing some research. I managed to work on some on-going projects but I could not find the time to start new projects or going back to some old ideas which I still have in mind. I am also haunted by thesis consultations with some master students which take lots of time and in this period I cannot find the energy to do anything else apart from routine activities. In addition, the situation of the vaccines does not seem to improve very quickly as my age group will likely receive the vaccine only towards the end of June which means that it may not be possible for me to go back to Italy to see my parents and friends during the summer as I might be asked to be back for the second dose. I hope this situation will be solved before the start of the new academic year. I really miss traveling!\nFinally, the ugly. I have some harsh deadlines which are coming up with respect to some joint projects on some applied works. Nothing particularly complex but they tend to be time consuming and require my attendance to many meetings which I do not find extremely interesting, let’s just say that. The next couple of weeks will be crucial and I really need to find some time to be ready and focus on finishing off these projects which I have dragged from a long time. I have also received confirmation that my invited talk R-HTA 2021 workshop this July will be totally online which makes me a bit sad as I find it really difficult to discuss my research without a physical audience in front of me. Next month I am also volunteering for presenting my work at my department and I need to come up with some good ideas for summarise my entire research in a way which does not completely bore my new colleagues. Seems like a mission impossible if you ask me but I will give it a shot as soon as I have some time.\n\n\n\n\n\n\nSo, many good things are in development but there are also bad things which I must first attend as well as some ugly things from which I cannot really escape. Wish me good luck!"
  },
  {
    "objectID": "posts/2021-07-02-my-blog-post/index.html",
    "href": "posts/2021-07-02-my-blog-post/index.html",
    "title": "A nice workshop",
    "section": "",
    "text": "I have recently been invited to give a talk about my R package missingHE at the R for HTA summer workshop 2021, and I would like to spend a couple of words to describe what was my experience at this event which for me was a first-time. Here you can see one of my tweets about the event (I will try to use Twitter more often in the future, promise!)\n\nIn general, I found it to be a very interesting conference with a crazy amount of people coming from all over the world who were interested, in one way or another, in the use and application of R for health economic assessment. To be honest, the fact that the workshop was fully online, for obvious reasons, may have encouraged lots of people to join compared to what would have been a standard in person attendance. Even so, I am quite impressed by the many people engaging in discussion and the very interesting topics raised. I felt that my presenation was very appreciated and I had a ton of fun discussing some missing data things with some of my colleagues at the workshop, including Manuel Gomes and Gianluca Baio who I know very well from my past at UCL. However, missing data analysis can be really hard to grasp from a mere presentation and I hope I simply gave some intuitions to some of the people in the audience about the importance that missing data assumptions cover in any analysis.\nAside from my personal stuff, which I really love despite not being very appealing to everybody, I have to pay my compliments to the organisers of the the event which ran smoothly and with some very interesting back and forth discussions between people coming from different places and positions but all with the common interest in HTA analyses. In particular, the discussion panel following my presentation was very interesting as it opened up a “hot” argument in regard to the use of R for HTA in the industry. All points discussed were valid and I felt that two of the main concerns which hold people in the HTA industry from using R in their analyses are:\n\nClients do not like going through the R code in order to understand the model structure and results. They have been used to a standardised way to report the model which is often based on an Excel spreadsheet in which they can play around with the cells and see the results for themselves. Since they are unfamiliar with R, they do not want to spend extra time trying to figure out what the model is like based on the R code. However, they are well aware that Excel is not able to perform any advanced statistical modelling and that all models are nowadays implemented in software such as R.\nThere is a general concern about the “quality” of the R packages available on CRAN or on the individual GitHub repositories of different developers. Many people made the comparison with software such as STATA or SAS which cannot be freely updated by the users but where there is a reference body who is in charge of making all the testing and checks before a new version or update is released to the public. On the contrary, R packages do not have such checks as the only requirement for a package to be uploaded on CRAN is that is does not crash when called.\n\nI agree that both points are valid and in fact this is something which should be seriously taken into account in order to encourage people to use R more often. If I may say my humble opinion, what I would argue is that the presence of a “unique” and “validated” reference for the checking and updating of the software routines is not necessarily an advantage in all situations. For example, if someone develops a new model which has not been yet implemented in any current functions, then it will be very difficult that a new version of the software is released simply to include this new model but it will be more likely that a certain amount of time will pass (so that more updates can be included all together) before the new commands become available to everybody. In this case, a software like R gives the chance to any developer to implement their method within a new package which can be uploaded and made available to everybody in a very short time. Of course, it also important that proper testing is carried out to ensure to minimise the amount of bugs or issues that may arise from using this new functions. Although these problems occur more frequently within a software framework such as R, the free-user nature of the software allows everybody encountering an issue in the use of the package to contact the developer, point out the problem and ask for a solution which may become available to everybody in a matter or days or even hours! I personally think that going back to a unique controller for checking the quality of all packages goes against the spirit behind the use of R as a free-user software that anyone can use to create, update and extend packages so to make them available to everybody without the need to wait for an external and impartial controller to do the checks. People will always find some issues or bugs for a new package and, in time, the more people use it the more the corresponding functions will be tested and will ensure a high quality resource for any newcomer.\nAs for the “problem” with the clients, I think this is a very delicate issue as, like in any private sector, it is important to try to match the needs of the client as much as possible. At the moment I think the standard approach is to implement the model in R and then “copy” the results into an Excel spreadsheet giving some powers to the client in terms of changing some inputs and see how this affects the results. This is of course very time consuming and also frustrating at times. A possible solution would be to use the web version of R called R Shiny which is a sort of user-friendly interface which allows people to play around with a model developed in R in a fashion which resembles the familiar Excel output clients are used to see. It is not perfect as, most of the times, the amount of modifications which are allowed to the clients is quite small and it is not really possible to perform any serious debugging of the code unless looking directly at the R code itself which raises the same problem as above. Personally, I think that it will be a matter of time until Excel outputs will gradually go away from the international landscape of HTA. This has already happened in most of academia, particularly in the UK, where statistical analyses are performed using statistical software. Soon the need to implement and improve more complex models in order to comply with the regulations and guidelines of decision-makers will make the simple idea of using Excel as something obsolete and both consultants and clients will need to adapt to the new standard, which is likely to be a statistical software such as R.\nMaybe it is still not the time, but we are getting there I am confident in this!"
  },
  {
    "objectID": "posts/2021-09-20-my-blog-post/index.html",
    "href": "posts/2021-09-20-my-blog-post/index.html",
    "title": "Back to work",
    "section": "",
    "text": "Well those holidays were pretty short in my opinion!\nAnyway, time to get back to work and, in fact, it has been already a while since the normal routine started. I try to keep my posts updated as much as I can but with the approaching of the next academic year things are a bit hectic. New and old courses, tutorials, preparation of new teaching material, coordination with other lecturers, consultation with students and researchers, etc… I just need to get back into the mix but sometimes it feels a bit overwhelming. Not that I complain but I just need a bit more time to find the right balance between all these “required” activities and my research which has been left a bit behind if I want to be honest. Hopefully, in a couple of months things will slow down and I will have some free to time to continue doing some nice research work. I really miss that now !\nEnough about my lamentations, let’s see if there is someting interesting that I can talk about in this, otherwise, plain post. Well I always wanted to give my opinion on a very interesting, although now a bit dated (2001), health economics article by the emeritus professor Antony O’Hagan, to my knowledge still working at the University of Sheffield. This I think the first general health economics publicaton addressing the topic of the need to develop a consistent and flexible statistical framework for the analysis of health economic individual-level data. The article is entitled “A framework for cost-effectiveness analysis from clinical trial data” and is a marvellous example of what many people have replicated and expanded over the following years (including myself), namely the call for a comprehensive analysis framework which can be generally applied for the analysis of cost-effectiveness data which takes into account the typical statistical issues that characterise these types of data.\nIndeed, for many years health economists did not rely on a solid general approach for the modelling and analysis of the data collected from clinical trials as in most cases focus of health economics literature was on decision modelling or aggregated data analysis, with little methodological emphasis on how practitioners involved in clinical studies should handle the specific problems that affect these data, i.e. from missingness, bivariate outcome, covariate adjustment, etc… Antony’s paper was for sure the first reading I was exposed which provided an idea on how things could be done in a more consistent way through the introduction of a Bayesian analysis framework and a clear justification for the adoption of this statistical approach in the field of health economics. This was already explored in the past by other authors (e.g. Claxton for sure was one of the first ones recognising the advantages of using a Bayesian approach) but no clear guidelines or implementation modelling strategy was available for standard analysts to look at.\nIn my opinion this is a very underrated publication and someone may disagree with me when I say that I find this to be a milestone in the statistical analysis literature of health economic data. I rarely see this paper cited in modern articles or publications, although many of these propose a similar concept to the one Antony gave in his paper, i.e. the call for a statistically-sound and reliable analysis framework that analysts may replicate in their analyses to obtain more consistency in the methodology used as well as taking into account the possible problems affecting the data that may lead to biased results. Of course, today things have changed and new publications provide more advanced and alternative approaches to deal with these problems, but the general concept behind this stil remains the same and I think we all owe, at least partially, Antony for what he tried to achieve with his paper. I am not sure how much his work influenced current health economics practice but it has for sure affected me and my research considerably.\nI would encourage any health eocnomist to read this paper as it gives a nice picture of why a statistical framework for the analysis of cost-effectiveness data is something that is desirable and that people should try to implement in thier analyses. Key aspects include the possibility to:\n\nSpecify the model for effect and cost data in a flexible way by expressing the mean population parameters for the two outcomes as functions various model parameters\nThe advantages provided by the Bayesian framework to implement this framework while also naturally accommodating the propagation and quantification of uncertainty surranding the parameters of interest\nThe convenience of using this framework to handle statistical issues such as correlation between outcomes, covariate adjustment, and provision of standard output such as cost-effectiveness planes and curves\n\nThere is also a clear “defense” of the use of prior information in regard to the modelling of these parameters within a Bayesian framework. Long story short, appropriate use of informative priors does not bias the inferences in the sense of favouring one treatment over another in a “subjective” way. Prior information is essentially an extra tool available to analysts which may also be the only way to incorporate some external information (i.e. expert opinion) into the model which would otherwise be discarded therefore resulting in an effective loss of information. Of course, it is important that the way this information is elicited into the model is reasonable and that it actually reflects the information available from external sources. However, there are plenty of ways to perform these assessments to check this in a rigorous way. I find it annoying that people would discard the use of prior information simply because they are not “sure” it is correct to use it. As any source of information, external information is information and as such it would inefficient, whenever this is available, to ignore it. Sometimes ignoring this information may lead to biased conclusions as well, e.g. in the case of missing data where observed data information may not be enough to obtain reliable results.\nIn such cases, wouldn’t be more reasonable, although I agree more difficult, to think about what type of other information is available to us and how this can be incorporated into our analysis in order to better assess the robustness of our results compared to, say, simply cover our eyes and pretend no information exists?"
  },
  {
    "objectID": "posts/2021-11-20-my-blog-post/index.html",
    "href": "posts/2021-11-20-my-blog-post/index.html",
    "title": "And now what?",
    "section": "",
    "text": "As the spectre of a new lockdown approaches the Netherlands, I find myself wondering what are the prospects for the upcoming months. Well, hopefully, things will be better than last year given that the amount of vaccinations is pretty high and there should be no big problems in terms of mobility for those who were vaccinated. This is good as I intend to pass Xmas together with my family and friends back in Italy, unlike last year. However, the fact that the spread of the virus continuos in spite of the high number of vaccinated people poses a big question of what will happen in terms of work. Does it remain fully online or a mix between in-person and online research/teaching will be required from now on? going back to fully in-person work seems pretty unlikely, at least for now and for the next few months. I must say that, although I feel quite ok in working from home in terms of research, I think students really need an in-person discussion and feedback in order to improve the efficacy of what we want them to learn. I experienced this myself by comparing the teaching I did last year with respect to this year, and I noticed how students get so much more from an in-person lecture/tutorial compared to an online one. The context and environment is not the same and I am pretty sure attention is quickly lost when people can have access to easy distraction elements, especially when they attend from the comfort of their own house. With this I simply hope that at least the teaching part of my job will remain on-site as I believe this is the most effective way to teach students and capture their attention, of course if this does not pose any real threat to their health. Personally, I also feel much better knowing that I need to go to an office at least once a week where I have the chance to chat with colleagues and fuly focus on my work and duties but I would also understand who prefers to work from home because of family or other more important issues.\nAt the moment I have a bit of a break from my teaching and I am focussing on my research (finally!). I need to take advantage of this moment as much as possible as form January things will become again a bit hazy due to the many courses I will be involved with. I have started and continued projects with different people both here in Maastricht but also with some old colleagues I met in London with whom I still have contact. Hopefully, I will be able to finalise and submit at least a couple of papers before the end of the year and enjoy my break afterwards. I will also need to look up for some grant applications for the upcoming year as this is an important (although often annoying) part of my job. The unfortunate thing is that, in the Netherlands, there is no national grant specifically dedicated to statistics but applications should be made with respect to some specific research field such as medicine, engineering, neuroscience, etc… I am not a big fan of this type of applications as the lack of statistical background in the people sitting in the evaluating committees is often the reason why applications too focussed on statistical aspects end up being rejected. I suspect this is what happened to my application at the VENI research scheme last year, despite the fact I received a good evaluation of my CV. It also makes sense as if people are not going to understand how your work can be applied in practice to enhance people’s lives, then there is no way they are going to decide in favour of your proposal compared to some applied research work in an area they are familiar with, for example. I need to figure out a smart way to combine my interest in developing new stats methods with some interesting applied work, at least from the perspective of the examiners. This is not easy as competition is also strong for these types of grants and I need to ask for some help from more experienced colleagues to learn how to write things in a more appealing and interesting way for someone who does not know much about statistics.\nFinally, I have also started looking for some future conferences to attend (hopefully in person) in the upcoming year. I found a couple related to HE that I would eager to attend.\n\nThe first is a Dutch-specific HE conference called lola HESG, which this year will be held here in Maastricht. From my understanding this was established in a similar way to what HESG has been organised in the UK, i.e. as a way to share knowledge and build connections among people involved in the analysis of HE data and problems. The format is also very alike as authors are asked to present and discuss the work of colleagues, rather than their own, and to discuss this with someone else. Based on my experience at HESG this format can be really helpful, especially for people just starting doing reasearch to find some possible collaborations or new ideas to work on. Given that it is here, I would have basically zero costs to sustain and therefore I hope I will have the chance to join this conference and meet some new faces.\nThe second is the EuHEA conference 2022, which will be held in Oslo. I have never been to this type of conference but I know that it mostly deals with HE problems and appications for UK and EU countries. I would like to attend this one too as it will provide the chance to receive some nice feedback from expert authors in the area of the statistical analysis of HE data. Although I am quite experienced with it, there is always room for improvement and personal development and I believe this could be the perfect occasion to have an update about what other people in this sector are up to (I have been left a bit behind since last year given that I missed most of the online conferences). This issue is that this is typically not a cheap conference and I find myself in troubles in the event the department will not be able to fund it (given that I might also attend the other conference in the same year).\n\nSo, overall lots to look forward to in the upcoming months where I hope I can start working again on my research projects that were put aside. There are still lots of things which are uncertain but I need to start planning my work since now to avoid potential on the go. And in the event such problems arise, as always, we will do the only thing we can do. We will adapt!"
  },
  {
    "objectID": "posts/2022-02-05-my-blog-post/index.html",
    "href": "posts/2022-02-05-my-blog-post/index.html",
    "title": "Studying Item Response Theory",
    "section": "",
    "text": "Hi everybody, I am happy to be back on my blog to talk about something new and interesting, at least to me it is! Recently I have been so busy with routine works that I had little time to focus on my personal projects, which is something I definitely need to change in the upcoming weeks. Taking by remorse, I decided to force myself to start looking at some new and interesting research lines which could be linked to my current expertise.\nWell, it was since last year that I wanted to dedicate more time to study and learn about the theoretical statistical framework of Item Response Theory, which I find really fascinating. So, last week I started giving myself a bit of time to review some of major handbooks and manuals on the topic, which I only knew in a partial way. Specifically, given my involvement in trial-based analayses, I was quite familiar with concepts such as validated multi-item questionnaires, latent constructs, Likert scales and so on, but I never delved into the whole IRT topic due to time constraints and other priorities until now. I am still reviewing lots of literature as I feel there are concepts/notions which I do not grasp very well yet, but recently I have been reading the Handbook of modern IRT by Wim J. van der Linden, as a general manual providing the basics of the theory and most popular modelling approaches.\nVery briefly, what I learned so far is that IRT theory was born as a way to theoretically and, most importantly, mathematically link the probability for a respondent to give a specific answer to an item within a questionnaire to an underlying latent construct or ability. There is a variety of model classes, each associated to a specific type of item question (e.g. binary, ordinal, categorical) and, within these classes, a there is a range of different model specifications each associated with different assumptions to provide an estimate of some latent abilities of interests while also trying to control for the influence of some other nuisance parameters (e.g. item difficulty) through a rigorous specification of the mathematical functions linking these two types of parameters. I have still lot to cover, but the premises so far are very exciting to me as this can be genuinely seen as a latent analysis problem where an unobserved parameter is estimated by assuming an inherent inductive approach in which observed data (i.e. individual item-answers) and are used to learn something about the latent parameter. Does this sound familiar to you? well to me damn yes, this is the basis of the Bayesian analysis! In fact, Bayesian analysis of IRT data is very popular as their inherent hierarchical structure perfectly matches the flexible Bayesian modelling framework where parameters are random variables to which distributions are assigned to represent their level of uncertainty. This is done prior to observing the data, which can be extremely helpful to incorporate external assumptions about these parameters in the model, as well as after observing the data, after updating the prior in a rational and coherent way (i.e. through Bayes’ theorem). This natural advantage of the Bayesian approach and the notorious difficulties that standard frequentist approaches encounter when running very complex and hierarchical models make the adoption of the Bayesian philosophy not only intuitive but also very practical.\nI think that there is so much potential to work on this topic and to see if there is room to expand the current methodology in some new directions. Of course, I am aware that many different Bayesian models have been applied to these types of data but I feel that there could be space for opening up some new research opportunities that have been only rarely touched before. For example, when collecting questionnaire data, especially self-reported ones, missing values are common and almost inevitable. This further complicates the analysis since assumptions about these unobserved answers must be made when running any type of model. In most cases I see that standard and simplifying assumptions, such as missing completely at random (MCAR), are made to avoid any terrible increase in model specification. Although certainly possible in some cases, such assumptions are not very likely to hold in realistic scenarios (e.g. it is unlikely that people who did not answer at some items are not systematically different from those who completed the questionnaire). So, what if I would like to explicitly state the missing data assumptions within the IRT framework? for example including a selection or pattern mixture model to assess the impact of informative missingness assumptions on the results? This for sure becomes a nightmare from a methodological perspective but I really think it is our duty to evolve this field to improve the methods.\nPossible new research work? grant proposal? could be. I just need to find the time to look at this with calm and critical thinking. Is there anyone out who might be interested in a joint work on this? I am always available!"
  },
  {
    "objectID": "posts/2022-04-10-my-blog-post/index.html",
    "href": "posts/2022-04-10-my-blog-post/index.html",
    "title": "New paper out",
    "section": "",
    "text": "Hello there! (do you get the movie reference?)\nJust a quick update from this month as I have recently received confirmation that a new paper I was involved with as first author has finally been published on Health Economics!!! Hurray! This is a work I am quite proud of as I was the guy who had the initial idea behind the concept of the paper and I was lucky enough to find some very helpful colleagues from the UK, most notably the very nice Baptiste Leurent and Catrin Plumpton who helped me out with laying down my ideas and give it an interesting appeal for the general health economist’s audience.\nThe main objective of the paper is nothing revolutionary or extremely innovative but rather on the importance of spreading awareness about the use of a specific type of modelling approach that has been rarely adopted in the context of health economic evaluation based on trial data, namely linear mixed effects models (LMMs). Indeed, most of the references to this approach in the health economic literature is related to the use of LMM to deal with clustering at the level of centres/countries which, do not get me wrong, is totally correct. However, I have noticed over time that LMMs have almost never been adopted in trial-based CEAs for the standard analysis of utility and cost data collected at different time points. This I think is a big gap in the literature as many analysts may simply ignore the potential use of LMMs for analysing these data given their lack of implementation in the literature.\nAfter reaching out to my colleguaes to have their opinion on the matter, we all agreed that it could be nice to lay down the coding and rationale for using LMMs to perform standard cost-effectiveness analyses based on trial data. Of course, as any method, there are advantages and drawbacks when using LMMs compared to the standard OLS models fitted at the level of aggregated variables (e.g. QALYs) instead of modelling utilities at each time. I will not go into much detail about these pros and cons (perhaps have a look at the paper if you are intrigued) but I will highlight two main points:\n\nWhen dealing with missing outcome values (i.e. always) modelling the disaggregated longitudinal data has the practical advantage the information about the reason for missingness can be more intuitively incorporated into the analysis and additionally allows to avoid some potential loss of power or even bias compared to standard complete case analysis approaches. The nice thing is that LMMs are also valid under standard Missing At Random (MAR) assumptions with the additional benefit that estimates can be derived without the need of using multiple imputation procedures unless some auxiliary variables are also considered into the analysis that are related to missingness. Thus, when then performing bootstrapping, no practical issues arises in terms of choosing the way to combine multiply imputed data and bootstrap replications which instead commonly occurs in standard analyses.\nLMMs allows for a quite flexible specification of the covariance structure of the data over time which can be tested from the observed data. Retrieval of the effects if interest (e.g. mean QALYs or total costs per arm) can be easily obtained by linearly combining the parameter estimates from the fixed effects part of the model depending on the parameterisation chosen. This can also be done in a straightforward way using common software packages (e.g. R or STATA) and does not require more than one or two lines of code.\n\nOf course, there are also downsides to be considered. For example, the use of multiple imputation to account for auxiliary variables or bivariate modelling to deal with the correlation between utilities and costs is not so easy to do. This is perhaps an idea for future development of the methods in the context of CEA. Well, my general solution is quite simple, just go Bayesian and get rid of all your headaches! However, I am not sure all practitioners will agree with me on that. Anyway, I invite you to have a look at the paper if you are involved in CEAs as it may provide some interesting thoughts for you to explore in relation to the methodology to use when analysing trial data and the potential implications of ignoring missing values in a context where the longitudinal nature of the data should be taken into account.\nHave a nice read!"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "R markdown for teaching\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nteaching\n\n\n\n\n\n\n\n\n\nMay 5, 2022\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nNew paper out\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\n\n\n\n\n\n\n\nApr 10, 2022\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nAttending conferences and invited talks\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nconference\n\n\n\n\n\n\n\n\n\nMar 15, 2022\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nStudying Item Response Theory\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nstatistics\n\n\nIRT\n\n\n\n\n\n\n\n\n\nFeb 5, 2022\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nAnd now what?\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nJan 5, 2022\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nAnd now what?\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nconference\n\n\n\n\n\n\n\n\n\nNov 20, 2021\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nBaseline adjustment in trial based CEA\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nhealth economics\n\n\n\n\n\n\n\n\n\nOct 10, 2021\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nBack to work\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nhealth economics\n\n\n\n\n\n\n\n\n\nSep 20, 2021\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nHolidays, finally\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nphilosophy\n\n\n\n\n\n\n\n\n\nAug 2, 2021\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nA nice workshop\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nconference\n\n\n\n\n\n\n\n\n\nJul 2, 2021\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nToo hot\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nconference\n\n\n\n\n\n\n\n\n\nJun 15, 2021\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nThe good, the bad and the ugly\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nconference\n\n\n\n\n\n\n\n\n\nMay 15, 2021\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nGoing back to teaching, hurray!\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\nteaching\n\n\nconference\n\n\n\n\n\n\n\n\n\nApr 15, 2021\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nSunny days which I cannot fully enjoy\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\n\n\n\n\n\n\n\nMar 25, 2021\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nDoing some teaching…\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nstatistics\n\n\npublication\n\n\nteaching\n\n\n\n\n\n\n\n\n\nFeb 15, 2021\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is a Bayesian credible interval?\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nJan 15, 2021\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nIt is Xmas again Yeah\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\npublication\n\n\n\n\n\n\n\n\n\nDec 20, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nA couple of updates\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nHealth Economics\n\n\npublication\n\n\n\n\n\n\n\n\n\nNov 5, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nWhy health economists do not care about statistical significance?\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nHealth Economics\n\n\n\n\n\n\n\n\n\nOct 10, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nStarting a new adventure!\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\n\n\n\n\n\n\n\nSep 10, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Bayesian inference?\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nAug 7, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nWhy be Bayesian\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nJul 7, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nNew tutorials for missingHE\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nmissingHE\n\n\n\n\n\n\n\n\n\nJun 5, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nSorry, an error occurred\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nmissingHE\n\n\n\n\n\n\n\n\n\nMay 18, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nNew updates for missingHE\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nmissingHE\n\n\n\n\n\n\n\n\n\nApr 20, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nSo much time but also not really\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nmissingHE\n\n\n\n\n\n\n\n\n\nApr 1, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nLiving and working at home is nice, right?\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\n\n\n\n\n\n\n\nMar 20, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nLockdown\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nconference\n\n\n\n\n\n\n\n\n\nMar 1, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nLet us do some work\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nworkshop\n\n\n\n\n\n\n\n\n\nFeb 10, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nFinally here …\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nworkshop\n\n\nconference\n\n\npublication\n\n\n\n\n\n\n\n\n\nFeb 1, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nLet us do some work\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nworkshop\n\n\n\n\n\n\n\n\n\nJan 9, 2020\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nNot a very good start…\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nmissingHE\n\n\n\n\n\n\n\n\n\nDec 9, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nToo many things, again….\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nconference\n\n\n\n\n\n\n\n\n\nNov 9, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nCopenhagen, I am coming …\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nconference\n\n\n\n\n\n\n\n\n\nOct 28, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nMore good news…\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nconference\n\n\n\n\n\n\n\n\n\nOct 1, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nMissingHE 1.2.1\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nmissingHE\n\n\n\n\n\n\n\n\n\nSep 25, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nDiscussing my thesis\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\ninterview\n\n\n\n\n\n\n\n\n\nSep 15, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nThe P value fallacy\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nAug 3, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\n\n\n\n\n\n\nHESG Summer Meeting 2019\n\n\n\n\n\n\nQuarto\n\n\nR\n\n\nAcademia\n\n\nHealth Economics\n\n\n\n\n\n\n\n\n\nJul 3, 2019\n\n\nAndrea Gabrio\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/bookHTA/bookHTA.html",
    "href": "research/bookHTA/bookHTA.html",
    "title": "Bayesian Hierarchical Models for the Prediction of Volleyball Results",
    "section": "",
    "text": "Introduction\nThe type of data used in economic evaluations typically come from a range of sources, whose evidence is combined to inform HTA decision-making. Traditionally, relative effectiveness data are derived from randomised controlled clinical trials (RCTs), while healthcare resource utilisation, costs and preference-based quality of life data may come from the same study that estimated the clinical effectiveness or not. A number of HTA agencies have developed their own methodological guidelines to support the generation of the evidence required to inform their decisions. In this context, the primary role of economic evaluation for HTA is not the estimation of the quantities of interest (e.g. the computation of point or interval estimation, or hypothesis testing), but to aid decision making. The implication of this is that the standard frequentist analyses that rely on power calculations and \\(P\\)-values to estimate statistical and clinical significance, typically used in RCTs, are not well-suited for addressing these HTA requirements.\nIt has been argued that, to be consistent with its intended role in HTA, economic evaluation should embrace a decision-theoretic paradigm and develop ideally within a Bayesian statistical framework to inform two decisions\n\nwhether the treatments under evaluation are cost-effective given the available evidence and\nwhether the level of uncertainty surrounding the decision is acceptable (i.e. the potential benefits are worth the costs of making the wrong decision).\n\nThis corresponds to quantify the impact of the uncertainty in the evidence on the entire decision-making process (e.g. to what extent the uncertainty in the estimation of the effectiveness of a new intervention affects the decision about whether it is paid for by the public provider).\n\n\nBayesian methods in HTA\nThere are several reasons that make the use of Bayesian methods in economic evaluations particularly appealing. First, Bayesian modelling is naturally embedded in the wider scheme of decision theory; by taking a probabilistic approach, based on decision rules and available information, it is possible to explicitly account for relevant sources of uncertainty in the decision process and obtain an optimal course of action. Second, Bayesian methods allow extreme flexibility in modelling using computational algorithms such as Markov Chain Monte Carlo (MCMC) methods; this allows to handle in a relatively easy way the generally sophisticated structure of the relationships and complexities that characterise effectiveness, quality of life and cost data. Third, through the use of prior distributions, the Bayesian approach naturally allows the incorporation of evidence from different sources in the analysis (e.g. expert opinion or multiple studies), which may improve the estimation of the quantities of interest; the process is generally referred to as evidence synthesis and finds its most common application in the use of meta-analytic tools. This may be extremely important when, as it often happens, there is only some partial (imperfect) information to identify the model parameters. In this case analysts are required to develop chain-of-evidence models. When required by the limitations in the evidence base, subjective prior distributions can be specified based on the synthesis and elicitation of expert opinion to identify the model, and their impact on the results can be assessed by presenting or combining the results across a range of plausible alternatives. Finally, under a Bayesian approach, it is straightforward to conduct sensitivity analysis to properly account for the impact of uncertainty in all inputs of the decision process; this is a required component in the approval or reimbursement of a new intervention for many decision-making bodies, such as NICE in the UK.\nThe general process of conducting a Bayesian analysis (with a view of using the results of the model to perform an economic evaluation) can be broken down in several steps, which are graphically summarized in Figure 1.\n\n\n\n\n\n\nFigure 1: Diagram representation of the process for health economic evaluation.\n\n\n\nThe starting point is the identification of the decision problem, which defines the objective of the economic evaluation (e.g. the interventions being compared, the target population, the relevant time horizon). In line with the decision problem, a statistical model is constructed to describe the (by necessity, limited) knowledge of the underlying clinical pathways. This implies, for example, the definition of suitable models to describe variability in potentially observed data (e.g. the number of patients recovering from the disease because of a given treatment), as well as the epistemic uncertainty in the population parameters (e.g. the underlying probability that a random individual in the target population is cured, if given the treatment under study). At this point, all the relevant data are identified, collected and quantitatively sytnthesised to derive the estimates of the input parameters of interest for the model.\nThese parameter estimates (and associated uncertainties) are then fed to the economic model, with the objective of obtaining some relevant summaries indicating the benefits and costs for each intervention under evaluation. Uncertainty analysis represents some sort of detour from the straight path going from the statistical model to the decision analysis: if the output of the statistical model allowed us to know with perfect certainty the true value of the model parameters, then it would be possible to simply run the decision analysis and make the decision. Of course, even if the statistical model were the true representation of the underlying data generating process (which it most certainly is not), because the data may be limited in terms of length of follow up, or sample size, the uncertainty in the value of the model parameters would still remain. This parameter (and structural) uncertainty is propagated throughout the whole process to evaluate its impact on the decision-making. In some cases, although there might be substantial uncertainty in the model inputs, this may not turn out to modify substantially the output of the decision analysis, i.e. the new treatment would be deemed as optimal irrespectively. In other cases, however, even a small amount of uncertainty in the inputs could be associated with very serious consequences. In such circumstances, the decision-maker may conclude that the available evidence is not sufficient to decide on which intervention to select and require more information before a decision can be made.\nThe results of the above analysis can be used to inform policy makers about two related decisions:\n\nwhether the new intervention is to be considered (on average) value for money, given the evidence base available at the time of decision, and\nwhether the consequences (in terms of net health loss) of making the wrong decision would warrant further research to reduce this decision uncertaint.\n\nWhile the type and specification of the statistical and economic models vary with the nature of the underlying data (e.g. individual (ILD) level versus aggregated (ALD) data, the decision and uncertainty analyses have a more standardised set up.\n\n\nConclusions\nHTA has been slow to adopt Bayesian methods; this could be due to a reluctance to use prior opinions, unfamiliarity, mathematical complexity, lack of software, or conservatism of the healthcare establishment and, in particular, the regulatory authorities. However, the use of Bayesian approach has been increasingly advocated as an efficient tool to integrate statistical evidence synthesis and parameter estimation with probabilistic decision analysis in an unified framework for HTA. This enables a transparent evidence-based decision modelling, reflecting the uncertainty and the structural relationships in all the available data.\nWith respect to trial-based analyses, the flexibility and modularity of the Bayesian modelling structure are well-suited to jointly account for the typical complexities that affect ILD. In addition, prior distributions can be used as convenient means to incorporate external information into the model when the evidence from the data is limited or absent (e.g. for missing values). In the context of evidence synthesis, the Bayesian approach is particularly appealing in that it allows for all the uncertainty and correlation induced by the often heterogeneous nature of the evidence (either ALD only or both ALD and ILD) to be synthesised in a way that can be easily integrated within a decision modelling framework.\nThe availability and spread of Bayesian software among practitioners since the late 1990s, such as OpenBUGS or JAGS, has greatly improved the applicability and reduced the computational costs of these models. Thus, analysts are provided with a powerful framework, which has been termed comprehensive decision modelling, for simultaneously estimating posterior distributions for parameters based on specified prior knowledge and data evidence, and for translating this into the ultimate measures used in the decision analysis to inform cost-effectiveness conclusions."
  },
  {
    "objectID": "research/jointHTA/jointHTA.html",
    "href": "research/jointHTA/jointHTA.html",
    "title": "Joint Longitudinal Models for Dealing With Missing at Random Data in Trial-Based Economic Evaluations",
    "section": "",
    "text": "Introduction\nIn trial-based economic evaluation, some individuals are typically associated with missing data at some time point, so that their corresponding aggregated outcomes (e.g. quality-adjusted life-years) cannot be evaluated. Restricting the analysis to the complete cases is inefficient and can result in biased estimates, while imputation methods are often implemented under a missing at random (MAR) assumption. We propose the use of joint longitudinal models to extend standard approaches by taking into account the longitudinal structure to improve the estimation of the targeted quantities under MAR.\n\n\nStandard approach in trial-based CEA\nAccording to recent reviews, standard practice in trial-based CEAs handles missingness at the level of the aggregated outcomes and baseline variables. Indeed, estimates of interest are obtained by directly modeling the aggregated outcomes rather than the utility and cost data at each time. This requires the analyst to process the data collected on individual \\(i\\) at time \\(j\\) in treatment \\(t\\), to derive the aggregated measures over the study duration.\nFigure 1 shows a typical data set of trial-based CEA, formed by the sets of utility and cost variables collected at baseline \\(j = 0\\) and some follow-ups \\(j = 1,\\ldots,J\\). The graph represents the standard procedure for processing the data and identifying the variables used in the analysis.\n\n\n\n\n\n\nFigure 1: Schematic representation of the standard procedure for processing trial-based CEA data\n\n\n\nA general limitation of any aggregated method is to ignore the longitudinal nature of the data and discard all follow-up values for partially observed individuals. Conversely, methods that handle missingness at each time point account for the longitudinal structure, incorporate all available evidence, and potentially make the missingness assumptions (e.g. missing at random or MAR) more reasonable.\n\n\nMethods\nWe propose the use of joint longitudinal models to extend standard approaches by taking into account the longitudinal structure to improve the estimation of the targeted quantities under MAR. We compare the results from methods that handle missingness at an aggregated (case deletion, baseline imputation, and joint aggregated models) and disaggregated (joint longitudinal models) level under MAR. The methods are compared using a simulation study and applied to data from 2 real case studies.\n\n\nConclusions\nJoint longitudinal models provide an alternative and potentially less biased approach for handling missing data with respect to current practice under a missing at random assumption. Methods that ignore some of the available information may be associated with biased results and mislead the decision-making process. This is a potentially serious issue for those who use these evaluations in their decision making, thus possibly leading to incorrect policy decisions about the cost-effectiveness of new treatment options."
  },
  {
    "objectID": "research/missingHE/missingHE.html",
    "href": "research/missingHE/missingHE.html",
    "title": "missingHE",
    "section": "",
    "text": "missingHE is a R package, available on CRAN which is aimed at providing some useful tools to analysts in order to handle missing outcome data under a full Bayesian framework in economic evaluations. The package relies on the R package R2jags to implement Bayesian methods via the statistical software JAGS to obtain inferences using Markov Chain Monte Carlo (MCMC) methods. Different types of missing data models are implemented in the package, including selection models, pattern mixture models and hurdle models. A range of parametric distributions can be specified when modelling the typical outcomes in an trial-based economic evaluations, namely the effectiveness and cost variables, while simultaneously incorporating different assumptions about the missingness mechanism, which allows to easily perform sensitivity analysis to a range of alternative missing data assumptions according to the modelling choices selected by the user.\nmissingHE also provides functions, taken and adapted from other R packages, to assess the results of each type of model, including summaries of the posterior distributions of each model parameter, range and imputations of the missing values, different types of model diagnostics to assess convergence of the algorithm, posterior predictive checks, model assessment measures based on the fit to the observed data, and a general summary of the economic evaluations, including the results from probabilistic sensitivity analyses which are automatically performed within a Bayesian modelling framework.\nFor example, the function plot can produce graphs, such as those shown in Figure 1, which compare the observed and imputed values for both cost and benefit measures in each treatment group to detect possible concerns about the plausibility of the imputations.\n\n\n\n\n\n\nFigure 1: Plot of observed (black dots) and imputed (red dots and lines) effectiveness and cost data by treatment group.\n\n\n\nMore information, including new updates, about missingHE can be found on my dedicated GitHub repository or via the most up to date version of the package on CRAN."
  },
  {
    "objectID": "research/partsurvHTA/partsurvHTA.html",
    "href": "research/partsurvHTA/partsurvHTA.html",
    "title": "A Bayesian Framework for Patient-Level Partitioned Survival Cost-Utility Analysis",
    "section": "",
    "text": "Modelling Framework\nwe extend the current methods for modelling trial-based partitioned survival cost-utility data, taking advantage of the flexibility of the Bayesian approach, and specify a joint probabilistic model for the health economic outcomes. We propose a general framework that is able to account for the multiple types of complexities affecting individual level data (correlation, missingness, skewness and structural values), while also explicitly modelling the dependence relationships between different types of quality of life and cost components.\nConsider a clinical trial in which patient-level information on a set of suitably defined effectiveness and cost variables is collected at \\(J\\) time points on \\(N\\) individuals, who have been allocated to \\(T\\) intervention groups. Assume that the primary endpoint of the trial is OS, while secondary endpoints include PFS, a self-reported health-related quality of life questionnaire (e.g. EQ-5D) and health records on different types of services (e.g. drug frequency and dosage, hospital visits, etc.). Following standard health economic notation, we denote with \\(\\boldsymbol e_{it}\\) and \\(\\boldsymbol c_{it}\\) the two sets of health economic outcomes (effectiveness and costs) collected for the \\(i\\)-th individual in treatment \\(t\\) of the trial. For simplicity, we define \\(\\boldsymbol e_{it}\\) and \\(\\boldsymbol c_{it}\\) based on the variables used in the analysis.\nThe effectiveness outcomes are represented by pre-progression (\\(e^{PFS}\\_{it}=\\text{QAS}^{\\text{PFS}}\\)) and post-progression (\\(e^{PPS}\\_{it}=\\text{QAS}^{\\text{PPS}}\\)) QAS data calculated using survival and utility data collected up to and beyond progression. We denote the full set of effectiveness variables as \\(\\boldsymbol e_{it}=(e^{\\text{PFS}}\\_{it},e^{\\text{PPS}}\\_{it})\\), formed by the pre and post-progression components. The cost outcomes are represented by a set of \\(K\\) variables (\\(c\\_{it}=c^k\\_{it}\\), for \\(k=1,\\ldots,K\\)) calculated based on \\(K\\) different types of health services and associated unit prices. We denote the full set of cost variables as \\(\\boldsymbol c\\_{it}=(c^1\\_{it},\\ldots,c^K\\_{it})\\), formed by the \\(K\\) different cost components.\nThe objective of the economic evaluation is to perform a patient-level partitioned survival cost-utility analysis by specifying a joint model \\(p\\boldsymbol e\\_{it}, \\boldsymbol c\\_{it} \\mid \\boldsymbol \\theta)\\), where \\(\\boldsymbol \\theta\\) denotes the full set of model parameters. Among these parameters, interest is in the marginal mean effectiveness and costs \\(\\boldsymbol \\mu=(\\mu\\_{et},\\mu\\_{ct})\\) which are used to inform the decision-making process. Different approaches can be used to specify \\(p\\boldsymbol e\\_{it}, \\boldsymbol c\\_{it} \\mid \\boldsymbol \\theta)\\). Here, we express the joint distribution as\n\\[\np(\\boldsymbol e_{it}, \\boldsymbol c_{it} \\mid \\boldsymbol \\theta) = p(\\boldsymbol e_{it} \\mid \\boldsymbol \\theta_e)p(\\boldsymbol c_{it} \\mid \\boldsymbol  e_{it} , \\boldsymbol  \\theta_c),\n\\tag{1}\\]\nwhere \\(p(\\boldsymbol e_{it} \\mid \\boldsymbol  \\theta_e)\\) is the marginal distribution of the effectiveness and \\(p(\\boldsymbol  c_{it} \\mid \\boldsymbol  e_{it} \\boldsymbol  \\theta_c)\\) is the conditional distribution of the costs given the effectiveness, respectively indexed by \\(\\boldsymbol  \\theta_e\\) and \\(\\boldsymbol  \\theta_c\\), with \\(\\boldsymbol  \\theta=(\\boldsymbol  \\theta_e,\\boldsymbol  \\theta_c)\\). We specify the model in Equation 1 in terms of a marginal distribution for the effectiveness and a conditional distribution for the costs. A key advantage of using a conditional factorisation, compared to a multivariate marginal approach, is that univariate models for each variable can be flexibly specified to tackle the idiosyncrasies of the data (e.g. non-normality ans spikes) while also capturing the potential correlation between the variables. We now describe how the two factors on the right-hand side of the Equation can be specified.\nFigure 1 provides a visual representation of the proposed modelling framework.\n\n\n\n\n\n\nFigure 1: Visual representation of the proposed modelling framework\n\n\n\nThe effectiveness and cost distributions are represented in terms of combined “modules”- the red and blue boxes - in which the random quantities are linked through logical relationships. Notably, this is general enough to be extended to any suitable distributional assumption, as well as to handle covariates in each module.\n\n\nConclusions\nAlthough our approach may not be applicable to all cases, the data analysed are very much representative of the “typical” data used in partitioned survival cost-utility analysis alongside clinical trials. Thus, it is highly likely that the same features apply to other real cases. This is a very important, if somewhat overlooked problem, as methods that do not take into account the complexities affecting patient-level data, while being easier to implement and well established among practitioners, may ultimately mislead cost-effectiveness conclusions and bias the decision-making process."
  },
  {
    "objectID": "research/reviewQES/reviewQES.html",
    "href": "research/reviewQES/reviewQES.html",
    "title": "Missingness Methods in trial-based Cost-Effectiveness Analysis",
    "section": "",
    "text": "We performed a systematic literature review that assesses the quality of the information reported and type of methods used to handle missing outcome data in trial-based economic evaluations. The purpose of this review is to critically appraise the current literature in within-trial CEAs with respect to the quality of the information reported and the methods used to deal with missingness for both effectiveness and costs. The review complements previous work, covering 2003-2009 (88 articles) with a new systematic review, covering 2009-2015 (81 articles) and focuses on two perspectives.\nFirst, we provide guidelines on how the information about missingness and related methods should be presented to improve the reporting and handling of missing data. We propose to address this issue by means of a Quality Evaluation Scheme (QES), providing a structured approach that can be used to guide the collection of information, formulation of the assumptions, choice of methods, and considerations of possible limitations for the given missingness problem. Second, we review the description of the missing data, the statistical methods used to deal with them and the quality of the judgement underpinning the choice of these methods."
  },
  {
    "objectID": "research/reviewQES/reviewQES.html#descriptive-review",
    "href": "research/reviewQES/reviewQES.html#descriptive-review",
    "title": "Missingness Methods in trial-based Cost-Effectiveness Analysis",
    "section": "Descriptive Review",
    "text": "Descriptive Review\n\n\n\n\n\n\nFigure 2: Missingness methods by outcome and period.\n\n\n\nFrom the comparison of the base-case methods used for the costs and effects between 2009 and 2015, the Figure above shows a marked reduction in the number of methods not clearly described for the effects, compared to those for the costs. A possible reason for this is that, while clinical effectiveness measures are often collected through self-reported questionnaires, which are naturally prone to missingness, cost measures rely more on clinical patient files which may ensure a higher completeness rate. It was not possible to confirm this interpretation in the reviewed studies due to the high proportions of articles not clearly reporting the missing rates in both 2003-2009 and 2009-2015 periods, for effects (\\(\\approx 45\\%\\) and \\(\\approx 38\\%\\)) and costs ( \\(\\approx 50\\%\\) and \\(\\approx 62\\%\\)). In addition, clinical outcomes are almost invariably the main objective of RCTs and are usually subject to more advanced and standardised analyses. Arguably, costs are often considered as an add-on to the standard trial: for instance, sample size calculations are almost always performed with the effectiveness measure as the only outcome of interest. Consequently, missing data methods are less frequently well thought through for the analysis of the costs. However, this situation is likely to change as cost data from different perspectives (e.g. caregivers, patients, society, etc.) are being increasingly used in trials, leading to the more frequent adoption of self-report cost data which may start to exhibit similar missingness characteristics to effect data.\nThe review identified only a few articles using more than one alternative method. In addition, these analyses are typically conducted without any clear justification about their underlying missing data assumptions and may therefore not provide a concrete assessment of the impact of missingness uncertainty. This situation indicates a gap in the literature associated with an under-implementation of sensitivity analysis, which may significantly affect the whole decision-making process outcome, under the perspective of a body who is responsible for providing recommendations about the implementation of alternative interventions for health care matters.\nLimiting the assessment of missingness assumptions to a single case is unlikely to provide a reliable picture of the underlying mechanism. This, in turn, may have a significant impact on the CEA and mislead its conclusions, suggesting the implementation of non-cost-effective treatments. Robustness analyses assess the sensitivity of the results to alternative missing data methods but do not justify the choice of these methods and their underlying assumptions about missingness which may therefore be inappropriate in the specific context analysed. By contrast, sensitivity analyses, which rely on external information to explore plausible alternative methods and missingness assumptions, represent an important and more appropriate tool to provide realistic assessments of the impact of missing data uncertainty on the final conclusions."
  },
  {
    "objectID": "research/reviewQES/reviewQES.html#quality-assessment",
    "href": "research/reviewQES/reviewQES.html#quality-assessment",
    "title": "Missingness Methods in trial-based Cost-Effectiveness Analysis",
    "section": "Quality assessment",
    "text": "Quality assessment\nGenerally speaking, most of the reviewed papers achieved an unsatisfactory quality score under the QES. Indeed, the benchmark area on the top-right corner of the graphs is barely reached by less than \\(7\\%\\) of the articles, both for cost and effect data.\nOverall, the proportions of the studies associated with the lowest category (E) prevails in the majority of the years, with a similar pattern over time between missing costs and effects. All the articles that are associated with the top category (A) belong to the period 2013-2015, with the highest proportions of articles falling in this category being observed in 2015 for both outcomes. The opportunity of reaching such a target might be precluded by the choice of the method adopted, which may not be able to support less restrictive assumptions about missingness, even when this would be desirable. As a result, when simple methods cannot be fully justified it is necessary to replace them with more flexible ones that can relax assumptions and incorporate more alternatives. In settings such as those involving MNAR, sensitivity analysis might represent the only possible approach to account for the uncertainty due to the missingness in a principled way. However, due to the lack of studies either performing a sensitivity analysis or providing high quality scores on the assumptions, missingness is not adequately addressed in most studies. This could have the serious consequence of imposing too restrictive assumptions about missingness and affect the outcome of decision making."
  }
]